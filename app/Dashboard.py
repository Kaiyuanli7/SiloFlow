# --------------------------------------------------
# FastAPI-compatible helper to create & store forecast (no Streamlit)
def generate_and_store_forecast_api(model_name: str, horizon: int, evaluations: dict) -> dict:
    """Generate future_df for *model_name* and return results/errors as dict. No Streamlit dependencies."""
    import traceback
    result = {
        "success": False,
        "error": None,
        "future_df": None,
        "X_future": None,
        "parquet_path": None,
    }
    try:
        res_eval = evaluations.get(model_name)
        if res_eval is None:
            result["error"] = f"No evaluation found for model: {model_name}"
            return result

        base_df = res_eval.get("df_base")
        categories_map = res_eval.get("categories_map", {})
        mdl_result = load_trained_model(model_name)
        if isinstance(mdl_result, dict) and "error" in mdl_result:
            result["error"] = f"Model load failed for {model_name}: {mdl_result['error']}"
            return result
        mdl = mdl_result

        if not isinstance(base_df, pd.DataFrame):
            result["error"] = f"base_df is not a DataFrame for model: {model_name}"
            return result
        _d(f"[EXPLORE-API] base_df shape: {base_df.shape}, columns: {list(base_df.columns)}")
        _d(f"[EXPLORE-API] categories_map: {categories_map}")
        _d(f"[EXPLORE-API] Model type: {type(mdl)}")
    except Exception as exc:
        result["error"] = f"Critical error during forecast setup: {exc}"
        _d(f"[CRITICAL-API] Exception in setup: {exc}")
        _d(traceback.format_exc())
        return result

    try:
        if isinstance(mdl, (MultiOutputRegressor, MultiLGBMRegressor)) and horizon <= HORIZON_DAYS:
            sensors_key = [c for c in [
                "granary_id", "heap_id", "grid_x", "grid_y", "grid_z"
            ] if c in base_df.columns]
            _d(f"[EXPLORE-API] sensors_key: {sensors_key}")
            last_rows = (
                base_df.sort_values("detection_time")
                .groupby(sensors_key, dropna=False)
                .tail(1)
                .copy()
            )
            _d(f"[EXPLORE-API] last_rows shape: {last_rows.shape}")
            X_snap, _ = features.select_feature_target_multi(
                last_rows, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE, allow_na=True
            )
            _d(f"[EXPLORE-API] X_snap shape: {X_snap.shape}")
            model_feats = get_feature_cols(mdl, X_snap)
            _d(f"[EXPLORE-API] model_feats: {model_feats}")
            X_snap_aligned = X_snap.reindex(columns=model_feats, fill_value=0)
            _d(f"[EXPLORE-API] X_snap_aligned shape: {X_snap_aligned.shape}")
            preds_mat = model_utils.predict(mdl, X_snap_aligned)
            _d(f"[EXPLORE-API] preds_mat shape: {getattr(preds_mat, 'shape', None)}")
            n_out = preds_mat.shape[1] if getattr(preds_mat, "ndim", 1) == 2 else 1
            all_future_frames: list[pd.DataFrame] = []
            last_dt = pd.to_datetime(last_rows["detection_time"]).max()
            for h in range(1, horizon + 1):
                day_frame = last_rows.copy()
                day_frame["detection_time"] = last_dt + timedelta(days=h)
                day_frame["forecast_day"] = h
                idx = min(h - 1, n_out - 1)
                if getattr(preds_mat, "ndim", 1) == 2:
                    pred_val = preds_mat[:, idx]
                else:
                    pred_val = preds_mat
                day_frame["predicted_temp"] = pred_val
                day_frame["temperature_grain"] = pred_val
                day_frame[TARGET_TEMP_COL] = pred_val
                day_frame["is_forecast"] = True
                all_future_frames.append(day_frame)
            future_df = pd.concat(all_future_frames, ignore_index=True)
            future_df.loc[future_df["is_forecast"], TARGET_TEMP_COL] = np.nan
            X_day_aligned = X_snap_aligned.copy()
        else:
            hist_df = base_df.copy()
            all_future_frames: list[pd.DataFrame] = []
            for d in range(1, horizon + 1):
                day_df = make_future(hist_df, horizon_days=1)
                day_df = _inject_future_lag(day_df, hist_df)
                day_df["forecast_day"] = d
                day_df = features.add_time_since_last_measurement(day_df)
                day_df = features.add_multi_lag_parallel(day_df, lags=(1,2,3,4,5,6,7,14,30))
                day_df = features.add_rolling_stats_parallel(day_df, window_days=7)
                for col, cats in categories_map.items():
                    if col in day_df.columns:
                        day_df[col] = pd.Categorical(day_df[col], categories=cats)
                X_day, _ = features.select_feature_target_multi(
                    day_df, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE, allow_na=True
                )
                _d(f"[EXPLORE-API] Day {d} X_day shape: {X_day.shape}")
                model_feats = get_feature_cols(mdl, X_day)
                _d(f"[EXPLORE-API] Day {d} model_feats: {model_feats}")
                X_day_aligned = X_day.reindex(columns=model_feats, fill_value=0)
                _d(f"[EXPLORE-API] Day {d} X_day_aligned shape: {X_day_aligned.shape}")
                preds = model_utils.predict(mdl, X_day_aligned)
                _d(f"[EXPLORE-API] Day {d} preds shape: {getattr(preds, 'shape', None)}")
                if hasattr(preds, 'ndim') and preds.ndim == 2:
                    preds_step = preds[:, 0]
                else:
                    preds_step = preds
                day_df["predicted_temp"] = preds_step
                day_df["temperature_grain"] = preds_step
                day_df[TARGET_TEMP_COL] = preds_step
                day_df["is_forecast"] = True
                hist_df = pd.concat([hist_df, day_df], ignore_index=True, sort=False)
                all_future_frames.append(day_df)
            future_df = pd.concat(all_future_frames, ignore_index=True)
            future_df.loc[future_df["is_forecast"], TARGET_TEMP_COL] = np.nan
        result["future_df"] = future_df
        result["X_future"] = X_day_aligned
        result["success"] = True
    except Exception as exc:
        result["error"] = f"Error during forecast generation: {exc}"
        _d(f"[ERROR-API] Exception in forecast generation: {exc}")
        _d(traceback.format_exc())
        return result

    # Persist predictions to Parquet
    try:
        core_cols = [
            c for c in [
                "granary_id",
                "heap_id",
                "grid_x",
                "grid_y",
                "grid_z",
                "detection_time",
                "forecast_day",
                "predicted_temp",
            ]
            if c in future_df.columns
        ]
        out_df = future_df[core_cols].copy()
        _d(f"[EXPLORE-API] out_df shape: {out_df.shape}, columns: {list(out_df.columns)}")
        out_dir = pathlib.Path("data/forecasts")
        out_dir.mkdir(parents=True, exist_ok=True)
        forecast_name = f"{pathlib.Path(model_name).stem}_forecast_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        forecast_path = out_dir / forecast_name
        from granarypredict.ingestion import save_granary_data
        parquet_path = save_granary_data(
            df=out_df,
            filepath=forecast_path,
            format='parquet',
            compression='snappy'
        )
        result["parquet_path"] = str(parquet_path)
        _d(f"[FORECAST-API] Parquet written to {parquet_path}")
    except Exception as exc:
        result["error"] = f"Error writing forecast CSV: {exc}"
        _d(f"[ERROR-API] Could not write forecast CSV: {exc}")
        _d(traceback.format_exc())
        result["success"] = False
    return result
import pathlib
# --- API imports ---
from fastapi import FastAPI, Query, HTTPException
from fastapi.responses import JSONResponse
import uvicorn

import pickle
from datetime import timedelta, datetime
from typing import Optional, List
import multiprocessing
import traceback

import numpy as np
import pandas as pd
import plotly.graph_objects as go
import streamlit as st
from itertools import islice

from granarypredict import cleaning, features, model as model_utils
from granarypredict.config import ALERT_TEMP_THRESHOLD, MODELS_DIR
from granarypredict import ingestion
from granarypredict.data_utils import comprehensive_sort, assign_group_id
# from granarypredict.data_organizer import organize_mixed_csv  # deprecated
from granarypredict.multi_lgbm import MultiLGBMRegressor  # NEW
from granarypredict.optuna_cache import load_optimal_params, save_optimal_params, clear_cache, list_cached_params  # NEW

# Streamlit reload may have stale module; fetch grain thresholds safely
try:
    from granarypredict.config import GRAIN_ALERT_THRESHOLDS  # type: ignore
except ImportError:
    GRAIN_ALERT_THRESHOLDS = {}

from sklearn.metrics import r2_score
from sklearn.model_selection import GroupShuffleSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor  # NEW
from sklearn.model_selection import GroupKFold

# ---------------------------------------------------------------------
# ðŸˆ¯ï¸  Simple i18n helper  (EN / ä¸­æ–‡)  -----------------------------------
# ---------------------------------------------------------------------


_TRANSLATIONS_ZH: dict[str, str] = {
    # Core UI elements
## FastAPI code removed. Streamlit GUI only.
    "Thermal Physics Features:": "çƒ­ç‰©ç†ç‰¹æ€§ï¼š",
    "Thermal Inertia": "çƒ­æƒ¯æ€§",
    "Stability Index": "ç¨³å®šæŒ‡æ•°",
    "Change Resistance": "å˜åŒ–é˜»åŠ›",
    "Equilibrium Temperature": "å¹³è¡¡æ¸©åº¦",
    "Mean Reversion": "å‡å€¼å›žå½’",
    "Historical Stability": "åŽ†å²ç¨³å®šæ€§",
    "Dampening Factor": "é˜»å°¼å› å­",
    "Progressive Penalties": "ç´¯è¿›æƒ©ç½š",
    "How This Improves Predictions:": "è¿™å¦‚ä½•æ”¹å–„é¢„æµ‹ï¼š",
    
    # Parallel processing messages
    "Parallel processing available: {}, max_workers: {}, CPU cores: {}": "å¹¶è¡Œå¤„ç†å¯ç”¨ï¼š{}ï¼Œæœ€å¤§å·¥ä½œè¿›ç¨‹ï¼š{}ï¼ŒCPU æ ¸å¿ƒï¼š{}",
    "Parallel processing: {} workers on {} cores": "å¹¶è¡Œå¤„ç†ï¼š{} ä¸ªå·¥ä½œè¿›ç¨‹åœ¨ {} ä¸ªæ ¸å¿ƒä¸Š",
    "Parallel Processing": "å¹¶è¡Œå¤„ç†",
    "Workers": "å·¥ä½œè¿›ç¨‹",
    "Expected Speedup": "é¢„æœŸåŠ é€Ÿ",
    "3-5x faster feature engineering": "ç‰¹å¾å·¥ç¨‹é€Ÿåº¦æé«˜ 3-5 å€",
    "2-4x faster hyperparameter optimization": "è¶…å‚æ•°ä¼˜åŒ–é€Ÿåº¦æé«˜ 2-4 å€",
    "Active": "æ¿€æ´»",
    "Disabled": "ç¦ç”¨",
    "Enable All Cores": "å¯ç”¨æ‰€æœ‰æ ¸å¿ƒ",
    "Disable Parallel": "ç¦ç”¨å¹¶è¡Œ",
    "Enable Parallel Processing": "å¯ç”¨å¹¶è¡Œå¤„ç†",
    "Could not retrieve parallel processing information: {}": "æ— æ³•æ£€ç´¢å¹¶è¡Œå¤„ç†ä¿¡æ¯ï¼š{}",
    
    # Parameter cache messages
    "Cached parameter sets": "ç¼“å­˜çš„å‚æ•°é›†",
    "Best MAE": "æœ€ä½³ MAE",
    "Trials": "è¯•éªŒ",
    "Data shape": "æ•°æ®å½¢çŠ¶",
    "Cached": "ç¼“å­˜æ—¶é—´",
    "Clear this": "æ¸…é™¤æ­¤é¡¹",
    "No cached parameters available.": "æ²¡æœ‰å¯ç”¨çš„ç¼“å­˜å‚æ•°ã€‚",
    "Train a model with Optuna optimization to create cache entries.": "ä½¿ç”¨ Optuna ä¼˜åŒ–è®­ç»ƒæ¨¡åž‹ä»¥åˆ›å»ºç¼“å­˜æ¡ç›®ã€‚",
    "Clearing parameter cache...": "æ­£åœ¨æ¸…é™¤å‚æ•°ç¼“å­˜...",
    "Parameter cache cleared!": "å‚æ•°ç¼“å­˜å·²æ¸…é™¤ï¼",
    "Clearing cache for {}...": "æ­£åœ¨æ¸…é™¤ {} çš„ç¼“å­˜...",
    "Refreshing cache view...": "æ­£åœ¨åˆ·æ–°ç¼“å­˜è§†å›¾...",
    "Using default parameters - enable Optuna or use parameter cache for optimized settings": "ä½¿ç”¨é»˜è®¤å‚æ•° - å¯ç”¨ Optuna æˆ–ä½¿ç”¨å‚æ•°ç¼“å­˜ä»¥èŽ·å¾—ä¼˜åŒ–è®¾ç½®",
    "Force re-optimization enabled but Optuna is disabled - using default parameters": "å¼ºåˆ¶é‡æ–°ä¼˜åŒ–å·²å¯ç”¨ä½† Optuna å·²ç¦ç”¨ - ä½¿ç”¨é»˜è®¤å‚æ•°",
    
    # Tab labels
    "Summary": "æ‘˜è¦",
    "Predictions": "é¢„æµ‹æ˜Žç»†",
    "3D Grid": "ä¸‰ç»´ç½‘æ ¼",
    "Time Series": "æ—¶é—´åºåˆ—",
    "Anchor 7-day": "é”šå®š 7 å¤©",
    "Uncertainty": "ä¸ç¡®å®šæ€§",
    "Extremes": "æžå€¼",
    "Debug": "è°ƒè¯•",
    "Evaluation": "è¯„ä¼°",
    "Forecast": "é¢„æµ‹",
    
    # Data section
    "Verbose debug mode": "è¯¦ç»†è°ƒè¯•æ¨¡å¼",
    "Upload your own data file": "ä¸Šä¼ æ‚¨çš„æ•°æ®æ–‡ä»¶",
    "Or pick a bundled sample dataset:": "æˆ–é€‰æ‹©ä¸€ä¸ªæ†ç»‘ç¤ºä¾‹æ•°æ®é›†ï¼š",
    "Sample dataset": "ç¤ºä¾‹æ•°æ®é›†",
    "-- Select sample --": "-- é€‰æ‹©ç¤ºä¾‹ --",
    "Loading sample dataset: {}": "æ­£åœ¨åŠ è½½ç¤ºä¾‹æ•°æ®é›†ï¼š{}",
    "Sample dataset '{}' selected.": "å·²é€‰æ‹©ç¤ºä¾‹æ•°æ®é›†'{}'ã€‚",
    "Raw Data": "åŽŸå§‹æ•°æ®",
    "Sorted Data": "å·²æŽ’åºæ•°æ®",
    
    # Location filter
    "Location Filter": "ä½ç½®ç­›é€‰å™¨",
    "Warehouse": "ä»“åº“",
    "Silo": "ç­’ä»“",
    "All": "å…¨éƒ¨",
    
    # Training section
    "Train on uploaded CSV": "ä½¿ç”¨ä¸Šä¼ çš„ CSV è¿›è¡Œè®­ç»ƒ",
    "Model file": "æ¨¡åž‹æ–‡ä»¶",
    "Apply to all models": "åº”ç”¨åˆ°æ‰€æœ‰æ¨¡åž‹",
    "Evaluate": "è¯„ä¼°",
    "Eval & Forecast": "è¯„ä¼°å¹¶é¢„æµ‹",
    
    # Status indicators
    "ACTIVE": "æ¿€æ´»",
    "DISABLED": "ç¦ç”¨",
    "High": "é«˜",
    "Medium": "ä¸­ç­‰",
    "Low": "ä½Ž",
    "Sequential mode: Recommended for small trial counts or debugging": "é¡ºåºæ¨¡å¼ï¼šæŽ¨èç”¨äºŽå°è¯•éªŒæ¬¡æ•°æˆ–è°ƒè¯•",
    
    # Messages and notifications
    "No forecast generated yet for this model.": "è¯¥æ¨¡åž‹å°šæœªç”Ÿæˆé¢„æµ‹ã€‚",
    "Uploaded file appears empty or unreadable. Please verify the CSV.": "ä¸Šä¼ çš„æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–ã€‚è¯·æ£€æŸ¥ CSVã€‚",
    "Model not found â€“ please train or select another.": "æœªæ‰¾åˆ°æ¨¡åž‹ â€“ è¯·è®­ç»ƒæˆ–é€‰æ‹©å…¶ä»–æ¨¡åž‹ã€‚",
    "No spatial temperature data present.": "æ²¡æœ‰ç©ºé—´æ¸©åº¦æ•°æ®ã€‚",
    "Detected mixed dataset â€“ organising into per-silo filesâ€¦": "æ£€æµ‹åˆ°æ··åˆæ•°æ®é›† â€“ æ­£åœ¨æŒ‰ç­’ä»“æ•´ç†æ–‡ä»¶â€¦",
    "Training model â€“ please wait...": "æ­£åœ¨è®­ç»ƒæ¨¡åž‹ â€“ è¯·ç¨å€™...",
    "No saved models yet.": "å°šæœªä¿å­˜ä»»ä½•æ¨¡åž‹ã€‚",
    "Please upload a CSV first to evaluate.": "è¯·å…ˆä¸Šä¼  CSV ä»¥è¿›è¡Œè¯„ä¼°ã€‚",
    "Evaluating model(s) â€“ please wait...": "æ­£åœ¨è¯„ä¼°æ¨¡åž‹ â€“ è¯·ç¨å€™...",
    "Generating forecastâ€¦": "æ­£åœ¨ç”Ÿæˆé¢„æµ‹â€¦",
    "Forecast generated â€“ switch tabs to view.": "é¢„æµ‹å·²ç”Ÿæˆ â€“ åˆ‡æ¢é€‰é¡¹å¡æŸ¥çœ‹ã€‚",
    "Model evaluation completed successfully.": "æ¨¡åž‹è¯„ä¼°æˆåŠŸå®Œæˆã€‚",
    "Forecast generation completed successfully.": "é¢„æµ‹ç”ŸæˆæˆåŠŸå®Œæˆã€‚",
    "training completed (validation split contained no ground-truth targets).": "è®­ç»ƒå®Œæˆï¼ˆéªŒè¯åˆ†å‰²ä¸åŒ…å«çœŸå®žç›®æ ‡ï¼‰ã€‚",
    "training completed. MAE: {}, RMSE: {}": "è®­ç»ƒå®Œæˆã€‚MAEï¼š{}ï¼ŒRMSEï¼š{}",
    "Model training completed successfully": "æ¨¡åž‹è®­ç»ƒæˆåŠŸå®Œæˆ",
    "Training completed successfully. MAE: {}, RMSE: {}": "è®­ç»ƒæˆåŠŸå®Œæˆã€‚MAEï¼š{}ï¼ŒRMSEï¼š{}",
    
    # Optuna optimization
    "Starting Optuna hyperparameter optimization: {} trials": "å¼€å§‹ Optuna è¶…å‚æ•°ä¼˜åŒ–ï¼š{} æ¬¡è¯•éªŒ",
    "Using {} parallel processes for enhanced optimization speed": "ä½¿ç”¨ {} ä¸ªå¹¶è¡Œè¿›ç¨‹ä»¥æé«˜ä¼˜åŒ–é€Ÿåº¦",
    "Live trial results will be displayed below as optimization progresses...": "ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å®žæ—¶è¯•éªŒç»“æžœå°†æ˜¾ç¤ºåœ¨ä¸‹æ–¹...",
    "Live Optuna Trial Results": "å®žæ—¶ Optuna è¯•éªŒç»“æžœ",
    "Best Trial": "æœ€ä½³è¯•éªŒ",
    "Trial": "è¯•éªŒ",
    "Learning Rate": "å­¦ä¹ çŽ‡",
    "Max Depth": "æœ€å¤§æ·±åº¦",
    "Num Leaves": "å¶èŠ‚ç‚¹æ•°",
    "Optuna Optimization Complete": "Optuna ä¼˜åŒ–å®Œæˆ",
    "Total Trials": "æ€»è¯•éªŒæ•°",
    "Improvement": "æ”¹è¿›",
    "View All Trial Results": "æŸ¥çœ‹æ‰€æœ‰è¯•éªŒç»“æžœ",
    "Subsample": "å­é‡‡æ ·",
    "Colsample": "åˆ—é‡‡æ ·",
    "Best Parameters Found": "æ‰¾åˆ°çš„æœ€ä½³å‚æ•°",
    "Optuna optimization completed. Best MAE: {}{}": "Optuna ä¼˜åŒ–å®Œæˆã€‚æœ€ä½³ MAEï¼š{}{}",
    "Parallel optimization completed using {} processes": "ä½¿ç”¨ {} ä¸ªè¿›ç¨‹çš„å¹¶è¡Œä¼˜åŒ–å®Œæˆ",
    "Optuna optimization failed: {}": "Optuna ä¼˜åŒ–å¤±è´¥ï¼š{}",
    "Using cached optimal parameters!": "ä½¿ç”¨ç¼“å­˜çš„æœ€ä¼˜å‚æ•°ï¼",
    "Best Parameters Found: Learning Rate: {}, Max Depth: {}, Num Leaves: {}": "æ‰¾åˆ°çš„æœ€ä½³å‚æ•°ï¼šå­¦ä¹ çŽ‡ï¼š{}ï¼Œæœ€å¤§æ·±åº¦ï¼š{}ï¼Œå¶èŠ‚ç‚¹æ•°ï¼š{}",
    "Optimal parameters saved to cache!": "æœ€ä¼˜å‚æ•°å·²ä¿å­˜åˆ°ç¼“å­˜ï¼",
    "Failed to save parameters to cache": "ä¿å­˜å‚æ•°åˆ°ç¼“å­˜å¤±è´¥",
    
    # Model leaderboard
    "Model Leaderboard": "æ¨¡åž‹æŽ’è¡Œæ¦œ",
    "No evaluations yet.": "å°šæ— è¯„ä¼°ç»“æžœã€‚",
    "model": "æ¨¡åž‹",
    "confidence": "ç½®ä¿¡åº¦",
    "accuracy": "å‡†ç¡®çŽ‡",
    "rmse": "RMSE",
    "mae": "MAE",
    "rank": "æŽ’å",
    
    # Debug
    "Debug Log (full)": "è°ƒè¯•æ—¥å¿—ï¼ˆå®Œæ•´ï¼‰",
    
    # Evaluation metrics
    "Forecast Summary (per day)": "é¢„æµ‹æ‘˜è¦ï¼ˆæ¯æ—¥ï¼‰",
    "Top Predictive Features": "æœ€å…·é¢„æµ‹åŠ›çš„ç‰¹å¾",
    "Daily Extremes (h+1)": "æ¯æ—¥æžå€¼ (h+1)",
    "No horizon-1 predictions available to compute extremes.": "æ²¡æœ‰å¯ç”¨äºŽè®¡ç®—æžå€¼çš„ h+1 é¢„æµ‹ã€‚",
    "Feature Matrices (first 100 rows)": "ç‰¹å¾çŸ©é˜µï¼ˆå‰ 100 è¡Œï¼‰",
    "Training â€“ X_train": "è®­ç»ƒ â€“ X_train",
    "Evaluation â€“ X_eval": "è¯„ä¼° â€“ X_eval",
    "Model Feature Columns (order)": "æ¨¡åž‹ç‰¹å¾åˆ—ï¼ˆé¡ºåºï¼‰",
    "High temperature forecast detected for at least one grain type â€“ monitor closely!": "æ£€æµ‹åˆ°æŸäº›ç²®é£Ÿç±»åž‹çš„é«˜æ¸©é¢„æµ‹ â€“ è¯·å¯†åˆ‡ç›‘æŽ§ï¼",
    "All predicted temperatures within safe limits for their grain types": "æ‰€æœ‰é¢„æµ‹æ¸©åº¦å‡åœ¨å…¶ç²®é£Ÿç±»åž‹çš„å®‰å…¨èŒƒå›´å†…",
    "LightGBM uses early stopping; optimal number of trees will be selected automatically.": "LightGBM ä½¿ç”¨æå‰åœæ­¢ï¼›å°†è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ ‘æ•°é‡ã€‚",
    
    # Metrics labels
    "Conf (%)": "ç½®ä¿¡åº¦ (%)",
    "Acc (%)": "å‡†ç¡®çŽ‡ (%)",
    "MAE h+1": "MAE h+1",
    "MAE h+2": "MAE h+2",
    "MAE h+3": "MAE h+3",
    "RMSE h+1": "RMSE h+1",
    "RMSE h+2": "RMSE h+2",
    "RMSE h+3": "RMSE h+3",
    "MAPE h+1": "MAPE h+1",
    "Confidence (%)": "ç½®ä¿¡åº¦ (%)",
    "Accuracy (%)": "å‡†ç¡®çŽ‡ (%)",
    "RMSE": "RMSE",
    "MAE": "MAE",
    "MAPE (%)": "MAPE (%)",
    "Forecast horizon (h + ? days)": "é¢„æµ‹èŒƒå›´ (h + ? å¤©)",
    "MAE per horizon": "å„é¢„æµ‹æœŸ MAE",
    "RMSE per horizon": "å„é¢„æµ‹æœŸ RMSE",
    "MAPE per horizon": "å„é¢„æµ‹æœŸ MAPE",
    
    # Explanations
    "Row-wise horizon metrics (above) average the error of each h-day-ahead prediction across all evaluation rows.\nFor a real-world, bulletin-style view of performance, switch to the 'Anchor 7-day' tab, where metrics are computed by freezing predictions on an anchor day and comparing them with observations that occur h days later.": "ä¸Šæ–¹æŒ‰é¢„æµ‹æœŸæ±‡æ€»çš„æŒ‡æ ‡æ˜¯å¯¹è¯„ä¼°é›†æ¯ä¸€è¡Œçš„ h æ—¥é¢„æµ‹è¯¯å·®å–å¹³å‡ã€‚\nè‹¥æƒ³æŸ¥çœ‹æ›´è´´è¿‘å®žé™…ä¸šåŠ¡çš„è¡¨çŽ°ï¼Œè¯·åˆ‡æ¢åˆ° \"Anchor 7-day\" é¡µç­¾ï¼šåœ¨é‚£é‡Œï¼ŒæŒ‡æ ‡åœ¨é”šå®šæ—¥å†»ç»“é¢„æµ‹ï¼Œå†ä¸Ž h å¤©åŽçš„çœŸå®žæ¸©åº¦æ¯”è¾ƒ",
    "Anchor metrics emulate operational use: predictions are frozen on the selected anchor day (forecast_day = 1) and each horizon h is scored against the real temperature measured h days later.\nThis gives the most realistic estimate of future-forecast performance.": "Anchor æŒ‡æ ‡æ¨¡æ‹Ÿå®žé™…æ“ä½œæµç¨‹ï¼šåœ¨é”šå®šæ—¥ï¼ˆforecast_day = 1ï¼‰å†»ç»“é¢„æµ‹ï¼Œå¹¶åœ¨ h å¤©åŽç”¨çœŸå®žè§‚æµ‹æ¸©åº¦æ‰“åˆ†ã€‚\nè¿™èƒ½æä¾›å¯¹æœªæ¥é¢„æµ‹æ€§èƒ½çš„æœ€çœŸå®žä¼°è®¡ã€‚",
    
    # Uncertainty analysis
    "Uncertainty Analysis & Prediction Confidence": "ä¸ç¡®å®šæ€§åˆ†æžå’Œé¢„æµ‹ç½®ä¿¡åº¦",
    "Uncertainty Estimation": "ä¸ç¡®å®šæ€§ä¼°è®¡",
    "Prediction Confidence Intervals": "é¢„æµ‹ç½®ä¿¡åŒºé—´",
    "Avg Uncertainty": "å¹³å‡ä¸ç¡®å®šæ€§",
    "Max Uncertainty": "æœ€å¤§ä¸ç¡®å®šæ€§",
    "Min Uncertainty": "æœ€å°ä¸ç¡®å®šæ€§",
    "Reliability": "å¯é æ€§",
    "Uncertainty by Forecast Horizon": "æŒ‰é¢„æµ‹æœŸçš„ä¸ç¡®å®šæ€§",
    "Horizon": "é¢„æµ‹æœŸ",
    "Days Ahead": "é¢„æµ‹å¤©æ•°",
    "Avg Uncertainty (Â°C)": "å¹³å‡ä¸ç¡®å®šæ€§ (Â°C)",
    "Confidence Level": "ç½®ä¿¡æ°´å¹³",
    "Available Confidence Intervals": "å¯ç”¨ç½®ä¿¡åŒºé—´",
    "Type": "ç±»åž‹",
    "Available": "å¯ç”¨",
    "Lower Bound": "ä¸‹ç•Œ",
    "Upper Bound": "ä¸Šç•Œ",
    "How to Interpret Uncertainty": "å¦‚ä½•è§£é‡Šä¸ç¡®å®šæ€§",
    "Uncertainty intervals will be available after making predictions": "è¿›è¡Œé¢„æµ‹åŽå°†æä¾›ä¸ç¡®å®šæ€§åŒºé—´",
    "Could not load model for uncertainty analysis: {}": "æ— æ³•åŠ è½½æ¨¡åž‹è¿›è¡Œä¸ç¡®å®šæ€§åˆ†æžï¼š{}",
    "Please ensure the model is properly trained and saved.": "è¯·ç¡®ä¿æ¨¡åž‹å·²æ­£ç¡®è®­ç»ƒå’Œä¿å­˜ã€‚",
    
    # Forecasting
    "No forecast generated for this model yet.": "å°šæœªä¸ºè¯¥æ¨¡åž‹ç”Ÿæˆé¢„æµ‹ã€‚",
    "Forecast Summary (predicted)": "é¢„æµ‹æ‘˜è¦ï¼ˆé¢„æµ‹ï¼‰",
    "Daily Predicted Extremes": "æ¯æ—¥é¢„æµ‹æžå€¼",
    "No predictions found to compute extremes.": "æœªæ‰¾åˆ°ç”¨äºŽè®¡ç®—æžå€¼çš„é¢„æµ‹ã€‚",
    "Future Feature Matrix (first 100 rows)": "æœªæ¥ç‰¹å¾çŸ©é˜µï¼ˆå‰ 100 è¡Œï¼‰",
    "|Mean(X_eval) âˆ’ Mean(X_future)| (Top 20)": "|Mean(X_eval) âˆ’ Mean(X_future)|ï¼ˆå‰ 20ï¼‰",
    "X_future matrix not available yet.": "X_future çŸ©é˜µå°šä¸å¯ç”¨ã€‚",
    "Please evaluate the model first.": "è¯·å…ˆè¯„ä¼°æ¨¡åž‹ã€‚",
    "Unable to access base data or model for forecasting.": "æ— æ³•è®¿é—®åŸºç¡€æ•°æ®æˆ–æ¨¡åž‹è¿›è¡Œé¢„æµ‹ã€‚",
    "Download predictions CSV": "ä¸‹è½½é¢„æµ‹ CSV",
    "Select day": "é€‰æ‹©å¤©æ•°",
    
    # Anchor 7-day tab
    "7-Day Forecast from Anchor Day (forecast_day=1)": "é”šå®šæ—¥çš„ 7 å¤©é¢„æµ‹ï¼ˆforecast_day=1ï¼‰",
    "forecast_day column missing â€“ cannot compute anchor forecast.": "forecast_day åˆ—ç¼ºå¤± â€“ æ— æ³•è®¡ç®—é”šå®šé¢„æµ‹ã€‚",
    "No anchor dates available.": "æ²¡æœ‰å¯ç”¨çš„é”šå®šæ—¥æœŸã€‚",
    "Select anchor date": "é€‰æ‹©é”šå®šæ—¥æœŸ",
    "No rows for selected anchor date {}.": "é€‰å®šé”šå®šæ—¥æœŸ {} æ²¡æœ‰è¡Œã€‚",
    "horizon_day": "é¢„æµ‹æœŸå¤©æ•°",
    "forecast_date": "é¢„æµ‹æ—¥æœŸ",
    "predicted_mean": "é¢„æµ‹å¹³å‡å€¼",
    "actual_mean": "å®žé™…å¹³å‡å€¼",
    "max_abs_err": "æœ€å¤§ç»å¯¹è¯¯å·®",
    "Anchor-day 7-Day Forecast vs Actual": "é”šå®šæ—¥ 7 å¤©é¢„æµ‹ä¸Žå®žé™…å¯¹æ¯”",
    "Date": "æ—¥æœŸ",
    "Temperature (Â°C)": "æ¸©åº¦ (Â°C)",
    "Predicted": "é¢„æµ‹",
    "Actual": "å®žé™…",
    "Sensor-level discrepancies (> 0.5 Â°C)": "ä¼ æ„Ÿå™¨çº§åˆ«å·®å¼‚ï¼ˆ> 0.5 Â°Cï¼‰",
    "h+{}: No matching sensor readings available.": "h+{}ï¼šæ²¡æœ‰åŒ¹é…çš„ä¼ æ„Ÿå™¨è¯»æ•°ã€‚",
    "h+{}: No sensor differences > {} Â°C.": "h+{}ï¼šæ²¡æœ‰ä¼ æ„Ÿå™¨å·®å¼‚ > {} Â°Cã€‚",
    "Aggregate MAE Metrics Across All Anchors": "æ‰€æœ‰é”šå®šç‚¹çš„èšåˆ MAE æŒ‡æ ‡",
    "Avg MAE (all anchors Ã— 7 days)": "å¹³å‡ MAEï¼ˆæ‰€æœ‰é”šå®šç‚¹ Ã— 7 å¤©ï¼‰",
    "Max MAE (all anchors Ã— 7 days)": "æœ€å¤§ MAEï¼ˆæ‰€æœ‰é”šå®šç‚¹ Ã— 7 å¤©ï¼‰",
    "MAE by Forecast Horizon (All Anchors)": "æŒ‰é¢„æµ‹æœŸçš„ MAEï¼ˆæ‰€æœ‰é”šå®šç‚¹ï¼‰",
    "h+{} MAE": "h+{} MAE",
    "h+{} Max |Error|": "h+{} æœ€å¤§ |è¯¯å·®|",
    
    # Extremes
    "Average Daily Absolute Error (h+1)": "æ¯æ—¥å¹³å‡ç»å¯¹è¯¯å·® (h+1)",
    "Over-Prediction (h+1)": "è¿‡é¢„æµ‹ (h+1)",
    "Under-Prediction (h+1)": "æ¬ é¢„æµ‹ (h+1)",
    "Select date": "é€‰æ‹©æ—¥æœŸ",
    "date": "æ—¥æœŸ",
    "type": "ç±»åž‹",
    "predicted": "é¢„æµ‹",
    "actual": "å®žé™…",
    "error": "è¯¯å·®",
    "avg_daily_abs_error": "æ¯æ—¥å¹³å‡ç»å¯¹è¯¯å·®",
    "Over": "è¿‡åº¦",
    "Under": "ä¸è¶³",
    "Max": "æœ€å¤§",
    "Min": "æœ€å°",
    "Avg |Error|": "å¹³å‡ |è¯¯å·®|",
    "Prediction column '{}' not found â€“ unable to draw time-series for h+{}.": "æœªæ‰¾åˆ°é¢„æµ‹åˆ— '{}' â€“ æ— æ³•ä¸º h+{} ç»˜åˆ¶æ—¶é—´åºåˆ—ã€‚",
    "Actual Avg": "å®žé™…å¹³å‡å€¼",
    "Predicted h+{} (eval)": "é¢„æµ‹ h+{}ï¼ˆè¯„ä¼°ï¼‰",
    "Predicted h+{} (future)": "é¢„æµ‹ h+{}ï¼ˆæœªæ¥ï¼‰",
    "Average Grain Temperature â€“ h+{}": "å¹³å‡ç²®é£Ÿæ¸©åº¦ â€“ h+{}",
    "Xaxis": "X è½´",
    "Yaxis": "Y è½´",
    
    # Plot labels
    "Temp (Â°C)": "æ¸©åº¦ (Â°C)",
    "Pred (Â°C)": "é¢„æµ‹ (Â°C)",
    "Î” (Â°C)": "Î” (Â°C)",
    "grid_x": "ç½‘æ ¼ X",
    "grid_z": "ç½‘æ ¼ Z",
    "grid_y": "ç½‘æ ¼ Y",
    "Prediction Uncertainty by Forecast Horizon": "æŒ‰é¢„æµ‹æœŸçš„é¢„æµ‹ä¸ç¡®å®šæ€§",
    "Forecast Horizon": "é¢„æµ‹æœŸ",
    "Average Uncertainty (Â°C)": "å¹³å‡ä¸ç¡®å®šæ€§ (Â°C)",
    
    # Conservative system detailed messages
    "Conservative Temperature System": "ä¿å®ˆæ¸©åº¦ç³»ç»Ÿ",
    "Models temperature resistance to change": "æ¨¡åž‹æ¸©åº¦å˜åŒ–é˜»åŠ›",
    "Measures temperature consistency": "æµ‹é‡æ¸©åº¦ä¸€è‡´æ€§",
    "Quantifies fluctuation dampening": "é‡åŒ–æ³¢åŠ¨é˜»å°¼",
    "Natural settling point per sensor": "æ¯ä¸ªä¼ æ„Ÿå™¨çš„è‡ªç„¶å¹³è¡¡ç‚¹",
    "Tendency to return to equilibrium": "å›žå½’å¹³è¡¡çš„è¶‹åŠ¿",
    "Long-term stability patterns": "é•¿æœŸç¨³å®šæ¨¡å¼",
    "Dynamic change dampening": "åŠ¨æ€å˜åŒ–é˜»å°¼",
    "Increasing constraints for longer horizons": "å¯¹è¾ƒé•¿é¢„æµ‹æœŸçš„é€’å¢žçº¦æŸ",
    "Reduces aggressive changes": "å‡å°‘æ¿€è¿›å˜åŒ–",
    "Improves 7-day accuracy": "æé«˜ 7 å¤©å‡†ç¡®æ€§",
    "Sensor-specific learning": "ä¼ æ„Ÿå™¨ç‰¹å®šå­¦ä¹ ",
    "Physical realism": "ç‰©ç†çŽ°å®žæ€§",
    "Predictions respect thermal inertia": "é¢„æµ‹å°Šé‡çƒ­æƒ¯æ€§",
    "Less cumulative error buildup": "æ›´å°‘çš„ç´¯ç§¯è¯¯å·®å †ç§¯",
    "Each probe learns its stability characteristics": "æ¯ä¸ªæŽ¢é’ˆå­¦ä¹ å…¶ç¨³å®šç‰¹æ€§",
    "Temperature evolution follows grain physics": "æ¸©åº¦æ¼”å˜éµå¾ªç²®é£Ÿç‰©ç†å­¦",
    "Conservative mode disabled - predictions may be aggressive": "ä¿å®ˆæ¨¡å¼ç¦ç”¨ - é¢„æµ‹å¯èƒ½æ¿€è¿›",
    "Enable conservative mode in model training to get more stable predictions": "åœ¨æ¨¡åž‹è®­ç»ƒä¸­å¯ç”¨ä¿å®ˆæ¨¡å¼ä»¥èŽ·å¾—æ›´ç¨³å®šçš„é¢„æµ‹",
    
    # More uncertainty messages
    "Low uncertainty": "ä½Žä¸ç¡®å®šæ€§",
    "High confidence predictions": "é«˜ç½®ä¿¡åº¦é¢„æµ‹",
    "Medium uncertainty": "ä¸­ç­‰ä¸ç¡®å®šæ€§",
    "Moderate confidence": "ä¸­ç­‰ç½®ä¿¡åº¦",
    "High uncertainty": "é«˜ä¸ç¡®å®šæ€§",
    "Lower confidence, use with caution": "ä½Žç½®ä¿¡åº¦ï¼Œè°¨æ…Žä½¿ç”¨",
    "~2/3 of actual values should fall within this range": "çº¦ 2/3 çš„å®žé™…å€¼åº”åœ¨æ­¤èŒƒå›´å†…",
    "~19/20 of actual values should fall within this range": "çº¦ 19/20 çš„å®žé™…å€¼åº”åœ¨æ­¤èŒƒå›´å†…",
    
    # Additional status messages
    "System Configuration:": "ç³»ç»Ÿé…ç½®ï¼š",
    "Stability Features": "ç¨³å®šæ€§ç‰¹å¾",
    "8 thermal physics features": "8 ä¸ªçƒ­ç‰©ç†ç‰¹å¾",
    "Stability Boost": "ç¨³å®šæ€§å¢žå¼º",
    "3.0x feature importance": "3.0 å€ç‰¹å¾é‡è¦æ€§",
    "Uncertainty Samples": "ä¸ç¡®å®šæ€§æ ·æœ¬",
    "50 bootstrap samples": "50 ä¸ªè‡ªåŠ©é‡‡æ ·",
    "Directional Features": "æ–¹å‘æ€§ç‰¹å¾",
    "2.0x importance boost": "2.0 å€é‡è¦æ€§å¢žå¼º",
    "Horizon Balancing": "é¢„æµ‹æœŸå¹³è¡¡",
    "Enabled": "å·²å¯ç”¨",
    "Conservative Loss": "ä¿å®ˆæŸå¤±",
    "Thermal inertia penalties": "çƒ­æƒ¯æ€§æƒ©ç½š",
    "Expected Benefits:": "é¢„æœŸæ”¶ç›Šï¼š",
    "More stable predictions": "æ›´ç¨³å®šçš„é¢„æµ‹",
    "Respects grain thermal inertia": "å°Šé‡ç²®é£Ÿçƒ­æƒ¯æ€§",
    "Reduced aggressive changes": "å‡å°‘æ¿€è¿›å˜åŒ–",
    "Conservative temperature evolution": "ä¿å®ˆçš„æ¸©åº¦æ¼”å˜",
    "Better 7-day accuracy": "æ›´å¥½çš„ 7 å¤©å‡†ç¡®æ€§",
    "Less cumulative error buildup": "æ›´å°‘çš„ç´¯ç§¯è¯¯å·®å †ç§¯",
    "Uncertainty quantification": "ä¸ç¡®å®šæ€§é‡åŒ–",
    "Confidence intervals for all predictions": "æ‰€æœ‰é¢„æµ‹çš„ç½®ä¿¡åŒºé—´",
    "Conservative system initialized - thermal stability features active": "ä¿å®ˆç³»ç»Ÿå·²åˆå§‹åŒ– - çƒ­ç¨³å®šæ€§ç‰¹å¾æ¿€æ´»",
    
    # System status messages
    "System": "ç³»ç»Ÿ",
    "ACTIVE": "æ¿€æ´»",
    "DISABLED": "ç¦ç”¨",
    "Conservative System Status": "ä¿å®ˆç³»ç»ŸçŠ¶æ€",
    "Conservative Temperature Prediction": "ä¿å®ˆæ¸©åº¦é¢„æµ‹",
    "Quantile Regression Active": "åˆ†ä½æ•°å›žå½’æ¿€æ´»",
    "Using quantile objective with uncertainty quantification for enhanced 7-day forecasting accuracy.": "ä½¿ç”¨åˆ†ä½æ•°ç›®æ ‡å’Œä¸ç¡®å®šæ€§é‡åŒ–ä»¥å¢žå¼º 7 å¤©é¢„æµ‹å‡†ç¡®æ€§ã€‚",
    
    # Missing translations for recent updates
    "Optuna configured for {} parallel processes": "Optuna é…ç½®ä¸º {} ä¸ªå¹¶è¡Œè¿›ç¨‹",
    "Approximately {}x faster optimization": "ä¼˜åŒ–é€Ÿåº¦çº¦æé«˜ {} å€",
    "Recommended for": "æŽ¨èç”¨äºŽ",
    "trials with 4+ CPU cores available": "éœ€è¦ 4+ CPU æ ¸å¿ƒçš„è¯•éªŒ",
    "sets available": "å¥—å¯ç”¨",
    "Later horizons (H+7) will receive 4x more weight than earlier horizons (H+1)": "åŽæœŸé¢„æµ‹æœŸï¼ˆH+7ï¼‰å°†æ¯”å‰æœŸé¢„æµ‹æœŸï¼ˆH+1ï¼‰èŽ·å¾— 4 å€æƒé‡",
    "Earlier horizons (H+1) will receive 4x more weight than later horizons (H+7)": "å‰æœŸé¢„æµ‹æœŸï¼ˆH+1ï¼‰å°†æ¯”åŽæœŸé¢„æµ‹æœŸï¼ˆH+7ï¼‰èŽ·å¾— 4 å€æƒé‡",
    "All horizons will receive equal weight (1.0x each)": "æ‰€æœ‰é¢„æµ‹æœŸå°†èŽ·å¾—ç›¸ç­‰æƒé‡ï¼ˆå„ 1.0 å€ï¼‰",
    
    # Comprehensive missing translations
    "Parameter Cache": "å‚æ•°ç¼“å­˜",
    "Use parameter cache": "ä½¿ç”¨å‚æ•°ç¼“å­˜",
    "Automatically save/load optimal parameters to skip redundant Optuna optimization": "è‡ªåŠ¨ä¿å­˜/åŠ è½½æœ€ä¼˜å‚æ•°ä»¥è·³è¿‡å†—ä½™çš„ Optuna ä¼˜åŒ–",
    "Force re-optimization": "å¼ºåˆ¶é‡æ–°ä¼˜åŒ–",
    "Run Optuna even if cached parameters exist": "å³ä½¿å­˜åœ¨ç¼“å­˜å‚æ•°ä¹Ÿè¿è¡Œ Optuna",
    "Clear cache": "æ¸…é™¤ç¼“å­˜",
    "Clear all cached parameters": "æ¸…é™¤æ‰€æœ‰ç¼“å­˜å‚æ•°",
    "Clearing parameter cache...": "æ­£åœ¨æ¸…é™¤å‚æ•°ç¼“å­˜...",
    "Parameter cache cleared!": "å‚æ•°ç¼“å­˜å·²æ¸…é™¤ï¼",
    "Cached parameters": "ç¼“å­˜å‚æ•°",
    "Future-safe mode enabled - environmental variables excluded": "æœªæ¥å®‰å…¨æ¨¡å¼å·²å¯ç”¨ - æŽ’é™¤çŽ¯å¢ƒå˜é‡",
    "All variables included, including environmental data": "åŒ…å«æ‰€æœ‰å˜é‡ï¼ŒåŒ…æ‹¬çŽ¯å¢ƒæ•°æ®",
    "Anchor-day early stopping": "é”šæ—¥æ—©åœæ³•",
    "Use 7-day consecutive forecasting accuracy for early stopping with optimized interval checking": "ä½¿ç”¨7å¤©è¿žç»­é¢„æµ‹å‡†ç¡®æ€§è¿›è¡Œæ—©åœï¼Œå¹¶é‡‡ç”¨ä¼˜åŒ–çš„é—´éš”æ£€æŸ¥",
    "Anchor-day early stopping enabled for enhanced 7-day accuracy": "é”šæ—¥æ—©åœæ³•å·²å¯ç”¨ï¼Œä»¥å¢žå¼º7å¤©å‡†ç¡®æ€§",
    "Using standard early stopping method": "ä½¿ç”¨æ ‡å‡†æ—©åœæ–¹æ³•",
    "Horizon Balancing Configuration": "é¢„æµ‹æœŸå¹³è¡¡é…ç½®",
    "Balance horizon training": "å¹³è¡¡é¢„æµ‹æœŸè®­ç»ƒ",
    "Ensures equal priority for all forecast horizons (H+1 through H+7) during model training": "ç¡®ä¿æ¨¡åž‹è®­ç»ƒæœŸé—´æ‰€æœ‰é¢„æµ‹æœŸï¼ˆH+1åˆ°H+7ï¼‰å…·æœ‰ç›¸ç­‰çš„ä¼˜å…ˆçº§",
    "Horizon balancing enabled - correcting forecast horizon bias": "é¢„æµ‹æœŸå¹³è¡¡å·²å¯ç”¨ - çº æ­£é¢„æµ‹æœŸåå·®",
    "Using standard horizon weighting approach": "ä½¿ç”¨æ ‡å‡†é¢„æµ‹æœŸæƒé‡æ–¹æ³•",
    "Horizon weighting strategy": "é¢„æµ‹æœŸæƒé‡ç­–ç•¥",
    "equal": "ç›¸ç­‰",
    "increasing": "é€’å¢ž",
    "decreasing": "é€’å‡",
    "equal: All horizons get equal priority (recommended) | increasing: Later horizons get more weight | decreasing: Earlier horizons get more weight": "ç›¸ç­‰ï¼šæ‰€æœ‰é¢„æµ‹æœŸèŽ·å¾—ç›¸ç­‰ä¼˜å…ˆçº§ï¼ˆæŽ¨èï¼‰| é€’å¢žï¼šåŽæœŸé¢„æµ‹æœŸèŽ·å¾—æ›´å¤šæƒé‡ | é€’å‡ï¼šå‰æœŸé¢„æµ‹æœŸèŽ·å¾—æ›´å¤šæƒé‡",
    "Performance Optimization": "æ€§èƒ½ä¼˜åŒ–",
    "Parallel Processing": "å¹¶è¡Œå¤„ç†",
    "Active": "æ¿€æ´»",
    "Workers": "å·¥ä½œå™¨",
    "Expected Speedup": "é¢„æœŸåŠ é€Ÿ",
    "3-5x faster feature engineering": "ç‰¹å¾å·¥ç¨‹å¿«3-5å€",
    "2-4x faster hyperparameter optimization": "è¶…å‚æ•°ä¼˜åŒ–å¿«2-4å€",
    "Enable All Cores": "å¯ç”¨æ‰€æœ‰æ ¸å¿ƒ",
    "Disable Parallel": "ç¦ç”¨å¹¶è¡Œ",
    "Disabled": "å·²ç¦ç”¨",
    "Enable Parallel Processing": "å¯ç”¨å¹¶è¡Œå¤„ç†",
    "Could not retrieve parallel processing information: {}": "æ— æ³•èŽ·å–å¹¶è¡Œå¤„ç†ä¿¡æ¯ï¼š{}",
    "Train on uploaded CSV": "åœ¨ä¸Šä¼ çš„CSVä¸Šè®­ç»ƒ",
    "Initiating model training process...": "æ­£åœ¨å¯åŠ¨æ¨¡åž‹è®­ç»ƒè¿‡ç¨‹...",
    "Training model â€“ please wait...": "æ­£åœ¨è®­ç»ƒæ¨¡åž‹ - è¯·ç¨å€™...",
    "Using cached optimal parameters!": "ä½¿ç”¨ç¼“å­˜çš„æœ€ä¼˜å‚æ•°ï¼",
    "Best MAE": "æœ€ä½³ MAE",
    "Cached": "å·²ç¼“å­˜",
    "Starting Optuna hyperparameter optimization: {} trials": "å¼€å§‹ Optuna è¶…å‚æ•°ä¼˜åŒ–ï¼š{} æ¬¡è¯•éªŒ",
    "Using {} parallel processes for enhanced optimization speed": "ä½¿ç”¨ {} ä¸ªå¹¶è¡Œè¿›ç¨‹ä»¥æé«˜ä¼˜åŒ–é€Ÿåº¦",
    "Live trial results will be displayed below as optimization progresses...": "ä¼˜åŒ–è¿›è¡Œæ—¶ä¸‹æ–¹å°†æ˜¾ç¤ºå®žæ—¶è¯•éªŒç»“æžœ...",
    
    # Conservative system translations
    "Conservative System Status": "ä¿å®ˆç³»ç»ŸçŠ¶æ€",
    "Conservative Temperature Prediction": "ä¿å®ˆæ¸©åº¦é¢„æµ‹",
    "System Configuration": "ç³»ç»Ÿé…ç½®",
    "Stability Features": "ç¨³å®šæ€§ç‰¹å¾",
    "8 thermal physics features": "8 ä¸ªçƒ­ç‰©ç†ç‰¹å¾",
    "Stability Boost": "ç¨³å®šæ€§å¢žå¼º",
    "3.0x feature importance": "3.0 å€ç‰¹å¾é‡è¦æ€§",
    "Uncertainty Samples": "ä¸ç¡®å®šæ€§æ ·æœ¬",
    "50 bootstrap samples": "50 ä¸ªè‡ªä¸¾æ ·æœ¬",
    "Directional Features": "æ–¹å‘æ€§ç‰¹å¾",
    "2.0x importance boost": "2.0 å€é‡è¦æ€§å¢žå¼º",
    "Horizon Balancing": "é¢„æµ‹æœŸå¹³è¡¡",
    "Enabled": "å·²å¯ç”¨",
    "Conservative Loss": "ä¿å®ˆæŸå¤±",
    "Thermal inertia penalties": "çƒ­æƒ¯æ€§æƒ©ç½š",
    "Expected Benefits": "é¢„æœŸæ•ˆç›Š",
    "More stable predictions": "æ›´ç¨³å®šçš„é¢„æµ‹",
    "Respects grain thermal inertia": "å°Šé‡ç²®é£Ÿçƒ­æƒ¯æ€§",
    "Reduced aggressive changes": "å‡å°‘æ¿€è¿›å˜åŒ–",
    "Conservative temperature evolution": "ä¿å®ˆæ¸©åº¦æ¼”åŒ–",
    "Better 7-day accuracy": "æ›´å¥½çš„7å¤©å‡†ç¡®æ€§",
    "Less cumulative error buildup": "æ›´å°‘çš„ç´¯ç§¯è¯¯å·®ç§¯ç´¯",
    "Uncertainty quantification": "ä¸ç¡®å®šæ€§é‡åŒ–",
    "Confidence intervals for all predictions": "æ‰€æœ‰é¢„æµ‹çš„ç½®ä¿¡åŒºé—´",
    
    # Additional cache management translations
    "Cached parameter sets": "ç¼“å­˜å‚æ•°é›†",
    "Data shape": "æ•°æ®å½¢çŠ¶",
    "Trials": "è¯•éªŒ",
    "Clear this": "æ¸…é™¤æ­¤é¡¹",
    "Clearing cache for {}...": "æ­£åœ¨æ¸…é™¤ {} çš„ç¼“å­˜...",
    "Clear all cache": "æ¸…é™¤æ‰€æœ‰ç¼“å­˜",
}


def _t(msg: str) -> str:
    """Translate *msg* to Chinese if the user selected that language."""
    lang = st.session_state.get("lang", "en")
    if lang == "zh":
        return _TRANSLATIONS_ZH.get(msg, msg)
    return msg

# ---------------------------------------------------------------------
# Debug helper â€“ collects messages in session state
# ---------------------------------------------------------------------

# Defined early so it's available everywhere

def _d(msg, use_toast=True):
    if not st.session_state.get("debug_mode"):
        return
    import streamlit as _st
    if use_toast:
        _st.toast(str(msg))
    log = _st.session_state.setdefault("debug_msgs", [])
    log.append(str(msg))

# Add constant after imports
ENV_COLUMNS = [
    "temperature_inside",
    "temperature_outside",
    "humidity_warehouse",
    "humidity_outside",
]

# Additional columns unavailable for real future dates that should be
# removed when training a "future-safe" model.
FUTURE_SAFE_EXTRA = [
    "max_temp",      # historic max air temp inside silo
    "min_temp",      # historic min inside temp
    "line_no",       # production line identifier (constant, but 0-filled in future)
    "layer_no",      # vertical layer identifier (constant, but 0-filled in future)
]

# Preset horizons (days) for quick selector controls
PRESET_HORIZONS = [7, 14, 30, 90, 180, 365]

# -----------------------------------------------------------------------------
# ðŸ”§ GLOBAL FORECAST HORIZON (days)
# -----------------------------------------------------------------------------
# Change **one** number here to adjust how many days ahead the model should
# learn and predict throughout the entire dashboard.  All downstream helper
# functions reference HORIZON_TUPLE instead of hard-coding (1, 2, 3).

HORIZON_DAYS: int = 7
# Tuple (1, 2, â€¦, HORIZON_DAYS)
HORIZON_TUPLE: tuple[int, ...] = tuple(range(1, HORIZON_DAYS + 1))

# Target column representing daily average grain temperature for evaluation/forecast
TARGET_TEMP_COL = "temperature_grain"  # per-sensor target for model & metrics

# Utility to detect if a model is "future-safe" by filename convention (contains 'fs_')
def is_future_safe_model(model_name: str) -> bool:
    return "fs_" in model_name.lower()

st.set_page_config(page_title="SiloFlow", layout="wide")

# Directory that holds bundled sample CSVs shipped with the repo
PRELOADED_DATA_DIR = pathlib.Path("data/preloaded")

# Directory that holds bundled pre-trained models
PRELOADED_MODEL_DIR = MODELS_DIR / "preloaded"

# Ensure directory exists so globbing is safe
PRELOADED_MODEL_DIR.mkdir(parents=True, exist_ok=True)

def load_uploaded_file(uploaded_file) -> pd.DataFrame:
    """Load uploaded file with automatic format detection (CSV or Parquet)."""
    try:
        # Check file extension to determine format
        file_extension = pathlib.Path(uploaded_file.name).suffix.lower()
        
        if file_extension == '.parquet':
            # Load Parquet file
            _d(f"[LOAD] Reading Parquet file: {uploaded_file.name}")
            df = pd.read_parquet(uploaded_file)
        elif file_extension in ['.csv', '.gz', '.gzip', '.bz2', '.zip', '.xz']:
            # Load CSV file (including compressed)
            _d(f"[LOAD] Reading data file: {uploaded_file.name}")
            df = pd.read_csv(uploaded_file)
        else:
            # Try to read as CSV (fallback)
            _d(f"[LOAD] Attempting to read as CSV: {uploaded_file.name}")
            df = pd.read_csv(uploaded_file)
        
        _d(f"[LOAD] Loaded {len(df)} rows, {len(df.columns)} columns")
        return df
        
    except Exception as e:
        st.error(f"Error loading file: {e}")
        return pd.DataFrame()


def load_trained_model(path: Optional[str | pathlib.Path] = None):
    """Attempt to load a model from user-saved or preloaded directories with comprehensive error handling."""
    # Default fallback
    if path is None:
        path = MODELS_DIR / "rf_model.joblib"

    path = pathlib.Path(path)

    # If given just a filename, search in both dirs
    if not path.is_absolute() and not path.exists():
        user_path = MODELS_DIR / path
        preload_path = PRELOADED_MODEL_DIR / path
        if user_path.exists():
            path = user_path
        elif preload_path.exists():
            path = preload_path

    if path.exists():
        try:
            _d(f"[MODEL] Attempting to load model: {path}")
            model = model_utils.load_model(path)
            _d(f"[MODEL] Successfully loaded model: {path.name}")
            return model
            
        except RuntimeError as exc:
            # Our custom loading error with multiple strategies attempted
            error_msg = f"Model loading failed after trying multiple strategies: {str(exc)}"
            _d(f"Model loading failed for {path}: {exc}", use_toast=False)
            return {"error": error_msg}
            
        except (FileNotFoundError, ValueError, KeyError, EOFError, pickle.UnpicklingError) as exc:
            # Standard pickle/joblib errors
            error_msg = f"Model file is corrupted or incompatible: {str(exc)}"
            _d(f"Model loading failed for {path}: {exc}", use_toast=False)
            return {"error": error_msg}
            
        except ImportError as exc:
            # Missing dependencies for model
            error_msg = f"Missing dependencies to load model: {str(exc)}. Try pip install scikit-learn lightgbm joblib"
            _d(f"Model loading failed for {path}: {exc}", use_toast=False)
            return {"error": error_msg}
            
        except Exception as exc:
            # Catch-all for any other errors
            error_msg = f"Unexpected error loading model: {str(exc)}"
            _d(f"Model loading failed for {path}: {exc}", use_toast=False)
            return {"error": error_msg}

    return {"error": _t("Model not found â€“ please train or select another.")}


def plot_3d_grid(df: pd.DataFrame, *, key: str, color_by_delta: bool = False):
    required_cols = {"grid_x", "grid_y", "grid_z", "temperature_grain"}
    if not required_cols.issubset(df.columns):
        st.info(_t("No spatial temperature data present."))
        return

    # Build point hover/label text
    texts = []
    has_pred = "predicted_temp" in df.columns
    for _, row in df.iterrows():
        if color_by_delta and has_pred:
            diff = row["predicted_temp"] - row["temperature_grain"]
            texts.append(
                f"Pred: {row['predicted_temp']:.1f}Â°C<br>Actual: {row['temperature_grain']:.1f}Â°C<br>Î”: {diff:+.1f}Â°C"
            )
        elif has_pred:
            diff = row["predicted_temp"] - row["temperature_grain"]
            texts.append(
                f"Pred: {row['predicted_temp']:.1f}Â°C<br>Actual: {row['temperature_grain']:.1f}Â°C<br>Î”: {diff:+.1f}Â°C"
            )
        else:
            texts.append(f"Actual: {row['temperature_grain']:.1f}Â°C")

    if color_by_delta and has_pred:
        color_vals = df["predicted_temp"] - df["temperature_grain"]
        c_scale = "RdBu_r"
        cbar_title = "Î” (Â°C)"
        # Set midpoint to 0 for delta mode so 0 is white
        colorbar_midpoint = 0
    else:
        color_vals = df["predicted_temp"] if has_pred else df["temperature_grain"]
        c_scale = "RdBu_r"  # red = hot, blue = cold
        cbar_title = "Temp (Â°C)" if not color_by_delta else "Pred (Â°C)"
        # Calculate midpoint to make 0 white for temperature mode
        min_val = color_vals.min()
        max_val = color_vals.max()
        if min_val < 0 and max_val > 0:
            # If we have both positive and negative values, set midpoint to 0
            colorbar_midpoint = 0
        else:
            # If all values are on one side of 0, use the middle of the range
            colorbar_midpoint = (min_val + max_val) / 2

    fig = go.Figure(data=go.Scatter3d(
        x=df["grid_x"],
        y=df["grid_z"],
        z=df["grid_y"],
        mode="markers",
        marker=dict(
            size=6,
            color=color_vals,
            colorscale=c_scale,
            colorbar=dict(title=cbar_title),
            cmid=colorbar_midpoint,  # Set the midpoint for the color scale
        ),
        text=texts,
        hovertemplate="%{text}<extra></extra>",
    ))
    # Ensure integer ticks on axes
    fig.update_layout(
        scene=dict(
            xaxis=dict(dtick=1, title="grid_x"),
            yaxis=dict(dtick=1, title="grid_z"),
            zaxis=dict(dtick=1, title="grid_y", autorange="reversed"),
            bgcolor="rgba(0,0,0,0)",
        ),
        height=600,
        margin=dict(l=0, r=0, b=0, t=0),
    )
    st.plotly_chart(fig, use_container_width=True, key=key)


def plot_time_series(
    df: pd.DataFrame,
    *,
    key: str,
    horizon_day: int = 1,
):
    """Plot actual vs predicted *average* grain temperature.

    Parameters
    ----------
    df : pd.DataFrame
        Concatenated dataframe that may contain historic rows, evaluation rows
        and/or future forecast rows.
    key : str
        Streamlit component key.
    horizon_day : int, default ``1``
        Forecast horizon to visualise (1-7).  ``1`` corresponds to the legacy
        behaviour that uses the *predicted_temp* column.  For *horizon_day > 1*
        the function looks for a column called ``pred_h{horizon_day}d``.
    """

    if "detection_time" not in df.columns:
        return  # nothing to plot

    # Determine which prediction column to use --------------------------
    pred_col = "predicted_temp" if horizon_day == 1 else f"pred_h{horizon_day}d"
    if pred_col not in df.columns:
        # Gracefully exit if the requested horizon is unavailable
        st.info(f"Prediction column '{pred_col}' not found â€“ unable to draw time-series for h+{horizon_day}.")
        return

    tmp = df.copy()
    # Use floor("D") to keep datetime64 dtype; avoids Plotly treating axis as categorical
    tmp["date"] = pd.to_datetime(tmp["detection_time"]).dt.floor("D")

    fig = go.Figure()

    # -------- Actual line ---------------------------------------------
    grp_actual = tmp.groupby("date").agg(actual=(TARGET_TEMP_COL, "mean")).reset_index()
    fig.add_trace(
        go.Scatter(
            x=grp_actual["date"],
            y=grp_actual["actual"],
            mode="lines+markers",
            name="Actual Avg",
            line=dict(color="#1f77b4"),
        )
    )

    # ------- Predicted line (continuous across eval + future) ----------
    grp_pred = (
        tmp.groupby("date").agg(predicted=(pred_col, "mean")).reset_index().sort_values("date")
    )

    # Determine cutoff between evaluation (has actual data) and future-only predictions
    actual_mask = tmp[TARGET_TEMP_COL].notna()
    last_actual_date = tmp.loc[actual_mask, "date"].max()

    if pd.isna(last_actual_date):
        pred_eval = pd.DataFrame()
        pred_future = grp_pred
    else:
        pred_eval = grp_pred[grp_pred["date"] <= last_actual_date]
        pred_future = grp_pred[grp_pred["date"] > last_actual_date]

    if not pred_eval.empty:
        fig.add_trace(
            go.Scatter(
                x=pred_eval["date"],
                y=pred_eval["predicted"],
                mode="lines+markers",
                name=f"Predicted h+{horizon_day} (eval)",
                line=dict(color="#ff7f0e"),
                connectgaps=True,
            )
        )

    if not pred_future.empty:
        fig.add_trace(
            go.Scatter(
                x=pred_future["date"],
                y=pred_future["predicted"],
                mode="lines+markers",
                name=f"Predicted h+{horizon_day} (future)",
                line=dict(color="#9467bd"),
                connectgaps=True,
            )
        )

    fig.update_layout(
        title=f"Average Grain Temperature â€“ h+{horizon_day}",
        xaxis_title="Date",
        yaxis_title="Temperature (Â°C)",
        xaxis=dict(rangeslider=dict(visible=True)),
    )
    st.plotly_chart(fig, use_container_width=True, key=key)


def list_available_models() -> list[str]:
    """Return unique model filenames from user-saved and preloaded dirs."""
    # Look for both uncompressed and compressed model files
    names = set()
    
    # Check MODELS_DIR
    if MODELS_DIR.exists():
        names.update({p.name for p in MODELS_DIR.glob("*.joblib")})
        names.update({p.name for p in MODELS_DIR.glob("*.joblib.gz")})
    
    # Check PRELOADED_MODEL_DIR
    if PRELOADED_MODEL_DIR.exists():
        names.update({p.name for p in PRELOADED_MODEL_DIR.glob("*.joblib")})
        names.update({p.name for p in PRELOADED_MODEL_DIR.glob("*.joblib.gz")})
    
    return sorted(names)


def split_train_eval(df: pd.DataFrame, horizon: int = 5):
    """Split by unique date; last 'horizon' dates form evaluation set."""
    df = df.copy()
    df["_date"] = pd.to_datetime(df["detection_time"]).dt.date
    unique_dates = sorted(df["_date"].unique())
    if len(unique_dates) <= horizon + 1:
        return df, pd.DataFrame()  # not enough data
    cutoff_dates = unique_dates[-horizon:]
    df_eval = df[df["_date"].isin(cutoff_dates)].copy()
    df_train = df[~df["_date"].isin(cutoff_dates)].copy()
    # map forecast_day index 1..horizon
    date_to_idx = {date: idx for idx, date in enumerate(cutoff_dates, start=1)}
    df_eval["forecast_day"] = df_eval["_date"].map(date_to_idx)
    df_train.drop(columns=["_date"], inplace=True)
    df_eval.drop(columns=["_date"], inplace=True)
    return df_train, df_eval


# -------------------------------------------------------------------
# NEW â€“ fractionâ€based chronological split (May-2025)
# -------------------------------------------------------------------


def split_train_eval_frac(df: pd.DataFrame, test_frac: float = 0.2):
    """Chronologically split *df* by unique date where the **last** fraction
    (``test_frac``) of dates becomes the evaluation set.

    Returns (df_train, df_eval) similar to ``split_train_eval`` but sized by
    proportion instead of fixed horizon.
    """
    df = df.copy()
    df["_date"] = pd.to_datetime(df["detection_time"], errors="coerce").dt.date
    unique_dates = sorted(df["_date"].dropna().unique())
    if not unique_dates:
        return df, pd.DataFrame()

    n_test_days = max(1, int(len(unique_dates) * test_frac))
    cutoff_dates = unique_dates[-n_test_days:]

    df_eval = df[df["_date"].isin(cutoff_dates)].copy()
    df_train = df[~df["_date"].isin(cutoff_dates)].copy()

    # map forecast_day index 1..n_test_days
    date_to_idx = {date: idx for idx, date in enumerate(cutoff_dates, start=1)}
    df_eval["forecast_day"] = df_eval["_date"].map(date_to_idx)

    df_train.drop(columns=["_date"], inplace=True)
    df_eval.drop(columns=["_date"], inplace=True)
    return df_train, df_eval


def forecast_summary(df_eval: pd.DataFrame) -> pd.DataFrame:
    if "forecast_day" not in df_eval.columns:
        return pd.DataFrame()

    # Pre-compute per-row absolute and squared errors to speed up group aggregations
    df_eval = df_eval.copy()
    if {TARGET_TEMP_COL, "predicted_temp"}.issubset(df_eval.columns):
        df_eval["abs_err"] = (df_eval[TARGET_TEMP_COL] - df_eval["predicted_temp"]).abs()
        df_eval["sqr_err"] = (df_eval[TARGET_TEMP_COL] - df_eval["predicted_temp"]) ** 2

    grp = (
        df_eval.groupby("forecast_day")
        .agg(
            actual_mean=(TARGET_TEMP_COL, "mean"),
            pred_mean=("predicted_temp", "mean"),
            mae=("abs_err", "mean"),
            rmse=("sqr_err", lambda s: np.sqrt(np.nanmean(s))),
        )
        .reset_index()
    )

    # Map numeric forecast_day â†’ actual calendar date (first occurrence)
    if "detection_time" in df_eval.columns:
        day_to_date = (
            pd.to_datetime(df_eval["detection_time"]).dt.floor("D").groupby(df_eval["forecast_day"]).first()
        )
        grp["date"] = grp["forecast_day"].map(day_to_date)
        # Re-order so date is the first column and drop forecast_day numeric index
        grp = grp[["date", "actual_mean", "pred_mean", "mae", "rmse"]]
        grp = grp.rename(columns={"date": "calendar_date"})
    
    # Percent absolute error (only where actual_mean is finite & non-zero)
    with np.errstate(divide="ignore", invalid="ignore"):
        grp["pct_error"] = (grp["pred_mean"] - grp["actual_mean"]).abs() / grp["actual_mean"].replace(0, np.nan) * 100

    # Confidence via RÂ² per day (skip NaNs)
    conf_vals: list[float] = []
    for day in df_eval["forecast_day"].unique():
        subset = df_eval[df_eval["forecast_day"] == day][[TARGET_TEMP_COL, "predicted_temp"]].dropna()
        if len(subset) > 1:
            r2 = r2_score(subset[TARGET_TEMP_COL], subset["predicted_temp"])
            conf_vals.append(max(0, min(100, r2 * 100)))
        else:
            conf_vals.append(np.nan)
    # Align list length with grp after possibly dropping forecast_day col
    grp["confidence_%"] = conf_vals[: len(grp)]

    return grp


def compute_overall_metrics(df_eval: pd.DataFrame) -> tuple[float, float]:
    """Return (confidence %, accuracy %) or (nan, nan) if not computable.
    This helper now drops rows containing NaNs before computing metrics to avoid
    ValueError from scikit-learn when all/any NaNs are present.
    """
    required = {TARGET_TEMP_COL, "predicted_temp"}
    if not required.issubset(df_eval.columns) or df_eval.empty:
        return float("nan"), float("nan")

    valid = df_eval[list(required)].dropna()
    if valid.empty:
        return float("nan"), float("nan")

    r2 = r2_score(valid[TARGET_TEMP_COL], valid["predicted_temp"])
    conf = max(0, min(100, r2 * 100))

    with np.errstate(divide="ignore", invalid="ignore"):
        pct_err = (valid[TARGET_TEMP_COL] - valid["predicted_temp"]).abs() / valid[TARGET_TEMP_COL].replace(0, np.nan)
    avg_pct_err = pct_err.mean(skipna=True) * 100 if not pct_err.empty else float("nan")
    acc = max(0, 100 - avg_pct_err)
    return conf, acc


# --------------------------------------------------
# Helper to build future rows for forecasting
def make_future(df: pd.DataFrame, horizon_days: int) -> pd.DataFrame:
    """Generate a future dataframe for the next ``horizon_days`` days.

    For each unique spatial location (grid_x/y/z) present in *df*, this function
    creates ``horizon_days`` duplicated rows with the *detection_time* advanced
    by 1..horizon_days. It also appends a *forecast_day* column (1-indexed).

    The resulting frame is passed through the same feature generators so it can
    be fed directly into the model for prediction.
    """
    if df.empty or "detection_time" not in df.columns:
        return pd.DataFrame()

    df = df.copy()
    df["detection_time"] = pd.to_datetime(df["detection_time"])
    latest_ts = df["detection_time"].max()

    # Keep spatial coords plus any constant categorical IDs (grain_type / warehouse_type)
    keep_cols = [c for c in df.columns if c in {"grid_x", "grid_y", "grid_z", "granary_id", "heap_id"}]
    sensors = df[keep_cols].drop_duplicates().reset_index(drop=True)

    # Add constant metadata if available (assuming single value across file)
    for const_col in ["grain_type", "warehouse_type"]:
        if const_col in df.columns:
            sensors[const_col] = df[const_col].dropna().iloc[0]

    # Prepare base detection_cycle if present
    max_cycle = df["detection_cycle"].max() if "detection_cycle" in df.columns else None

    frames: List[pd.DataFrame] = []
    for d in range(1, horizon_days + 1):
        tmp = sensors.copy()
        tmp["detection_time"] = latest_ts + timedelta(days=d)
        tmp["forecast_day"] = d
        if max_cycle is not None:
            tmp["detection_cycle"] = max_cycle + d
        frames.append(tmp)

    future_df = pd.concat(frames, ignore_index=True)
    # Feature engineering to match training pipeline
    future_df = features.create_time_features(future_df)
    future_df = features.create_spatial_features(future_df)
    # Ensure both legacy and new target columns exist so downstream feature
    # selection works regardless of current configuration.
    if "temperature_grain" not in future_df.columns:
        future_df["temperature_grain"] = np.nan
    if TARGET_TEMP_COL not in future_df.columns:
        future_df[TARGET_TEMP_COL] = np.nan
    return future_df
# --------------------------------------------------


def main():
    st.session_state.setdefault("evaluations", {})
    st.session_state.setdefault("forecasts", {})  # NEW: container for forecast results

    # Debug toggle â€“ placed at very top so early messages are captured
    st.sidebar.checkbox(_t("Verbose debug mode"), key="debug_mode", help="Show detailed internal processing messages", value=True)
    
    # Verify parallel processing is available
    try:
        parallel_info = features.get_parallel_info()
        _d(f"[PARALLEL] Parallel processing available: {parallel_info['parallel_enabled']}, max_workers: {parallel_info['max_workers']}, CPU cores: {parallel_info['cpu_count']}")
        if st.session_state.get("debug_mode"):
            st.toast(f"ðŸ”§ Parallel processing: {parallel_info['max_workers']} workers on {parallel_info['cpu_count']} cores", icon="ðŸ”§")
    except Exception as e:
        _d(f"[PARALLEL] Could not verify parallel processing: {e}")

    # ---------------- Language selector (appears very top) -------------
    def on_language_change():
        new_lang = "en" if st.session_state.get("lang_selector") == "English" else "zh"
        old_lang = st.session_state.get("lang", "en")
        if new_lang != old_lang:
            st.toast(f"ðŸŒ Language changed to {'English' if new_lang == 'en' else 'ä¸­æ–‡'}", icon="ðŸŒ")
        st.session_state.update({"lang": new_lang})
    
    st.sidebar.selectbox(
        "Language / è¯­è¨€",
        options=["English", "ä¸­æ–‡"],
        index=0 if st.session_state.get("lang", "en") == "en" else 1,
        key="lang_selector",
        on_change=on_language_change,
    )
    # Ensure lang key present
    st.session_state.setdefault("lang", "en")

    with st.sidebar.expander(_t('Data'), expanded=("uploaded_file" not in st.session_state)):
        uploaded_file = st.file_uploader(_t("Upload your own data file"), type=["csv", "parquet"], key="uploader")

        # ------------------------------------------------------------------
        # Offer bundled sample datasets so users can start instantly
        if PRELOADED_DATA_DIR.exists():
            # Include both CSV and Parquet files
            sample_files = sorted(list(PRELOADED_DATA_DIR.glob("*.csv")) + list(PRELOADED_DATA_DIR.glob("*.parquet")))
            if sample_files:
                st.caption(_t("Or pick a bundled sample dataset:"))
                sample_names = ["-- Select sample --"] + [p.name for p in sample_files]
                sample_choice = st.selectbox(
                    _t("Sample dataset"),  # non-empty label for accessibility
                    options=sample_names,
                    key="sample_selector",
                    label_visibility="collapsed",  # hide visually but keep for screen readers
                )
                if sample_choice and sample_choice != "-- Select sample --":
                    uploaded_file = PRELOADED_DATA_DIR / sample_choice  # path object -> pd.read_csv works
                    st.toast(f"ðŸ“‚ Loading sample dataset: {sample_choice}", icon="ðŸ“‚")
                    st.info(f"Sample dataset '{sample_choice}' selected.")

    if uploaded_file:
        # Show notification for file upload
        if hasattr(uploaded_file, 'name'):
            st.toast(f"Processing uploaded file: {uploaded_file.name}")
        else:
            st.toast("Processing uploaded file...")
        
        df = load_uploaded_file(uploaded_file)
        _d(f"[DATA] Uploaded file loaded â€“ shape={df.shape} cols={list(df.columns)[:10]}â€¦")
        with st.expander(_t("Raw Data"), expanded=False):
            st.dataframe(df, use_container_width=True)

        # ------------------------------------------------------------------
        # Auto-organise if the upload mixes multiple silos  (removed in v1.1)
        # ------------------------------------------------------------------
        # (functionality removed)
        
        # Full preprocessing once
        _d("Running full preprocessing on uploaded dataframe (cached)â€¦")
        df = _get_preprocessed_df(uploaded_file)
        st.toast(f"Data preprocessing complete. Dataset shape: {df.shape}")
        _d(f"[DATA] Preprocessing complete â€“ shape={df.shape} cols={list(df.columns)[:10]}â€¦")

        # --- Save preprocessed data as Parquet (always) ---
        import os
        from granarypredict.ingestion import save_granary_data
        base_name = os.path.splitext(os.path.basename(uploaded_file.name))[0]
        processed_dir = os.path.join("data", "processed")
        os.makedirs(processed_dir, exist_ok=True)
        processed_path = os.path.join(processed_dir, f"{base_name}_processed.parquet")
        save_granary_data(df, processed_path, format="parquet", compression="snappy")
        _d(f"[DATA] Saved preprocessed data as Parquet: {processed_path}")

        # Display sorted table directly below Raw Data
        df_sorted_display = df
        with st.expander(_t("Sorted Data"), expanded=False):
            _st_dataframe_safe(df_sorted_display, key="sorted")

        # ------------------------------
        # Global Warehouse â†’ Silo filter
        # ------------------------------

        st.markdown(f"### {_t('Location Filter')}")
        with st.container():
            # Detect possible column names coming from different CSV formats
            wh_col_candidates = [c for c in ["granary_id", "storepointName"] if c in df.columns]
            silo_col_candidates = [c for c in ["heap_id", "storeName"] if c in df.columns]

            wh_col = wh_col_candidates[0] if wh_col_candidates else None
            silo_col = silo_col_candidates[0] if silo_col_candidates else None

            # 1ï¸âƒ£ Warehouse selector â€“ always shown if the column exists
            if wh_col:
                warehouses = sorted(df[wh_col].dropna().unique())
                warehouses_opt = ["All"] + warehouses
                sel_wh_global = st.selectbox(
                    _t("Warehouse"),
                    options=warehouses_opt,
                    key="global_wh",
                )
            else:
                sel_wh_global = "All"

            # 2ï¸âƒ£ Silo selector â€“ rendered only after a specific warehouse is chosen
            if wh_col and sel_wh_global != "All" and silo_col:
                silos = sorted(df[df[wh_col] == sel_wh_global][silo_col].dropna().unique())
                silos_opt = ["All"] + silos
                sel_silo_global = st.selectbox(
                    _t("Silo"),
                    options=silos_opt,
                    key="global_silo",
                )
            else:
                sel_silo_global = "All"

            # Persist selection in session state for downstream use
            old_filters = st.session_state.get("global_filters", {})
            new_filters = {
                "wh": sel_wh_global,
                "silo": sel_silo_global,
            }
            
            # Show notification if filters changed
            if old_filters != new_filters:
                if sel_wh_global != "All" or sel_silo_global != "All":
                    filter_text = f"Warehouse: {sel_wh_global}"
                    if sel_silo_global != "All":
                        filter_text += f", Silo: {sel_silo_global}"
                    st.toast(f"Applied filters: {filter_text}")
                else:
                    st.toast("Displaying all locations")
            
            st.session_state["global_filters"] = new_filters

        with st.sidebar.expander(_t('Train / Retrain Model'), expanded=False):
            model_choice = st.selectbox(
                _t("Algorithm"),
                ["LightGBM"],
                index=0,
            )
            # Initialize LightGBM-specific variables outside conditional
            tune_optuna = False
            use_quantile = False
            n_trials = 0
            optuna_speed_mode = False  # Initialize to prevent UnboundLocalError
            optuna_parallel = False  # Initialize to prevent UnboundLocalError
            optuna_n_jobs = 1  # Initialize to prevent UnboundLocalError
            
            if model_choice == "LightGBM":
                st.caption(_t("LightGBM uses early stopping; optimal number of trees will be selected automatically."))
                n_trees = 2000  # upper bound (not shown to user)
                tune_optuna = st.checkbox(_t("Optuna hyperparameter optimization"), value=False, help=_t("Enable Optuna to automatically tune LightGBM parameters for optimal performance"))
                if tune_optuna and "optuna_enabled" not in st.session_state:
                    st.toast(_t("Optuna hyperparameter optimization enabled"))
                    st.session_state["optuna_enabled"] = True
                elif not tune_optuna and st.session_state.get("optuna_enabled"):
                    st.toast(_t("Using default LightGBM parameters"))
                    st.session_state["optuna_enabled"] = False
                
                use_quantile = st.checkbox(_t("Quantile regression objective"), value=True, help=_t("Use LightGBM quantile regression (alpha 0.5) for improved mean absolute error performance"))
                if use_quantile:
                    st.info(f"**{_t('Quantile Regression Active')}**: {_t('Using quantile objective with uncertainty quantification for enhanced 7-day forecasting accuracy.')}")
                if use_quantile and "quantile_enabled" not in st.session_state:
                    st.toast(_t("Quantile regression objective enabled"))
                    st.session_state["quantile_enabled"] = True
                elif not use_quantile and st.session_state.get("quantile_enabled"):
                    st.toast(_t("Using standard regression objective"))
                    st.session_state["quantile_enabled"] = False
                
                # Parameter cache controls (always available)
                st.subheader(f"ðŸ“¦ {_t('Parameter Cache')}")
                use_param_cache = st.checkbox(
                    _t("Use parameter cache"), 
                    value=True, 
                    help=_t("Automatically save/load optimal parameters from previous Optuna runs. Works even when Optuna is disabled!")
                )
                    
                col_cache1, col_cache2 = st.columns(2)
                with col_cache1:
                    force_reoptimize = st.checkbox(
                        _t("Force re-optimization"), 
                        value=False, 
                        help=_t("Run Optuna even if cached parameters exist")
                    )
                with col_cache2:
                    if st.button(_t("Clear cache"), help=_t("Clear all cached parameters")):
                        st.toast(f"ðŸ§½ {_t('Clearing parameter cache...')}", icon="ðŸ§½")
                        clear_cache()
                        st.success(_t("Parameter cache cleared!"))
                
                # Show cached parameter info with debugging
                cached_params = list_cached_params()
                if cached_params:
                    st.write(f"ðŸ“‹ **{_t('Cached parameters')}**: {len(cached_params)} {_t('sets available')}")
                    
                    # Debug: Show cache details for current CSV
                    if st.checkbox("ðŸ” Show cache debugging info", value=False):
                        csv_filename = uploaded_file.name if uploaded_file else "No file"
                        st.write(f"**Debug Info for {csv_filename}:**")
                        
                        # Show all cached files
                        for key, info in cached_params.items():
                            cached_csv = info.get('csv_filename', 'Unknown')
                            cached_mae = info.get('best_value', 'N/A')
                            cached_trials = info.get('n_trials', 'N/A')
                            cached_shape = info.get('data_shape', 'N/A')
                            
                            match_indicator = "âœ…" if cached_csv == csv_filename else "âŒ"
                            st.write(f"{match_indicator} **{cached_csv}** - MAE: {cached_mae}, Trials: {cached_trials}, Shape: {cached_shape}")
                        
                        # Show current configuration
                        if uploaded_file:
                            current_config = {
                                "quantile_mode": use_quantile,
                                "speed_mode": optuna_speed_mode if tune_optuna else "N/A",
                                "n_trials": n_trials if tune_optuna else "N/A",
                                "cache_enabled": use_param_cache,
                                "force_reopt": force_reoptimize
                            }
                            st.write(f"**Current Config**: {current_config}")
                
                if tune_optuna:
                    n_trials = st.slider(_t("Optuna trials"), 20, 200, 50, step=10)
                    optuna_speed_mode = st.checkbox(
                        _t("Fast Optuna mode"), 
                        value=True, 
                        help=_t("Use performance optimizations: 2-fold CV, lower tree limits, aggressive early stopping")
                    )
                    
                    # NEW: GPU Acceleration Settings (moved before parallel settings)
                    st.markdown(f"**{_t('GPU Acceleration')}**")
                    
                    # Check GPU availability first
                    try:
                        from granarypredict.multi_lgbm import detect_gpu_availability
                        gpu_config = detect_gpu_availability()
                        gpu_available = gpu_config['available']
                        
                        if gpu_available:
                            st.success(f"âœ… {_t('GPU detected and available for acceleration')}")
                        else:
                            st.warning(f"{_t('No GPU detected - will use CPU acceleration')}")
                    except Exception as e:
                        st.warning(f"{_t('Could not detect GPU availability')}: {str(e)[:50]}...")
                        gpu_available = False
                    
                    use_gpu_optuna = st.checkbox(
                        _t("Enable GPU acceleration"),
                        value=gpu_available,  # Default to True only if GPU is available
                        help=_t("Use GPU acceleration for faster LightGBM training during Optuna optimization"),
                        disabled=not gpu_available  # Disable if no GPU available
                    )
                    
                    if use_gpu_optuna and gpu_available:
                        gpu_optimization_mode = st.selectbox(
                            _t("GPU Optimization Mode"),
                            ["auto", "balanced", "speed", "accuracy"],
                            help=_t("Auto: Smart optimization based on dataset size. Balanced: Speed/accuracy trade-off. Speed: Maximum speed. Accuracy: Maximum precision.")
                        )
                        
                        gpu_device_id = st.slider(
                            _t("GPU Device ID"),
                            0, 3, 0, step=1,
                            help=_t("Select which GPU device to use (0 = first GPU)")
                        )
                        
                        gpu_use_double_precision = st.checkbox(
                            _t("Use double precision"),
                            value=True,
                            help=_t("Use double precision for better accuracy (slower but more precise)")
                        )
                    else:
                        gpu_optimization_mode = "cpu"
                        gpu_device_id = 0
                        gpu_use_double_precision = True
                        if not gpu_available:
                            st.info(f"ðŸ’» {_t('Using CPU acceleration - no GPU available')}")
                    
                    # Parallel Optuna optimization settings
                    st.markdown(f"**âš¡ {_t('Parallel Optimization')}**")
                    optuna_parallel = st.checkbox(
                        _t("Enable parallel trials"),
                        value=False,
                        help=_t("Run multiple Optuna trials in parallel for 2-4x faster optimization")
                    )
                    
                    if optuna_parallel:
                        # NEW: Smart parallel process recommendation based on GPU usage
                        if use_gpu_optuna and gpu_available:
                            # With GPU: Can use more parallel processes since GPU handles model training
                            max_processes = min(8, multiprocessing.cpu_count())
                            default_processes = min(4, multiprocessing.cpu_count())
                            help_text = _t("With GPU acceleration: Can use more parallel processes (GPU handles model training)")
                        else:
                            # Without GPU: Be more conservative since CPU cores are shared with LightGBM
                            max_processes = min(4, multiprocessing.cpu_count())
                            default_processes = min(2, multiprocessing.cpu_count())
                            help_text = _t("Without GPU: Conservative setting to avoid overwhelming CPU cores")
                        
                        optuna_n_jobs = st.slider(
                            _t("Number of parallel processes"), 
                            2, max_processes, default_processes, step=1,
                            help=help_text
                        )
                        
                        # NEW: Smart LightGBM threading configuration
                        if use_gpu_optuna and gpu_available:
                            # GPU mode: Use fewer CPU threads per model to free up cores for Optuna
                            lgbm_n_jobs = st.selectbox(
                                _t("LightGBM CPU threads per model"),
                                [1, 2, 4],
                                index=1,  # Default to 2
                                help=_t("With GPU: Use fewer CPU threads to free up cores for parallel Optuna trials")
                            )
                        else:
                            # CPU mode: Use all available cores for each model
                            lgbm_n_jobs = st.selectbox(
                                _t("LightGBM CPU threads per model"),
                                [1, 2, 4, -1],
                                index=3,  # Default to -1 (all cores)
                                help=_t("Without GPU: Use all CPU cores for maximum model training speed")
                            )
                        
                        # Performance expectations
                        if use_gpu_optuna and gpu_available:
                            expected_speedup = min(optuna_n_jobs * 2, 8)  # GPU provides additional speedup
                            st.caption(f"**{_t('Expected Speedup')}**: {_t('Approximately {}x faster optimization').format(expected_speedup)}")
                            st.caption(f"**{_t('GPU + Parallel')}**: {_t('GPU acceleration + {} parallel processes').format(optuna_n_jobs)}")
                        else:
                            expected_speedup = min(optuna_n_jobs, 4)
                            st.caption(f"**{_t('Expected Speedup')}**: {_t('Approximately {}x faster optimization').format(expected_speedup)}")
                            st.caption(f"**{_t('Recommended for')}**: {n_trials}+ {_t('trials with 4+ CPU cores available')}")
                    else:
                        optuna_n_jobs = 1
                        lgbm_n_jobs = -1  # Use all cores for single-threaded Optuna
                        st.caption(_t("Sequential mode: Recommended for small trial counts or debugging"))
                    
                    
            else:
                n_trees = st.slider(_t("Iterations / Trees"), 100, 1000, 300, step=100)
                optuna_speed_mode = False  # Default when Optuna is disabled
                optuna_parallel = False  # Default when Optuna is disabled
                optuna_n_jobs = 1  # Default when Optuna is disabled
                n_trials = 50  # Default when Optuna is disabled
            future_safe = st.checkbox(_t("Future-safe (exclude env vars)"), value=False)
            if future_safe and "future_safe_enabled" not in st.session_state:
                st.toast(_t("Future-safe mode enabled - environmental variables excluded"))
                st.session_state["future_safe_enabled"] = True
            elif not future_safe and st.session_state.get("future_safe_enabled"):
                st.toast(_t("All variables included, including environmental data"))
                st.session_state["future_safe_enabled"] = False
            
            anchor_early_stop = st.checkbox(
                _t("Anchor-day early stopping"), 
                value=True, 
                help=_t("Use 7-day consecutive forecasting accuracy for early stopping with optimized interval checking")
            )
            if anchor_early_stop and "anchor_stop_enabled" not in st.session_state:
                st.toast(_t("Anchor-day early stopping enabled for enhanced 7-day accuracy"))
                st.session_state["anchor_stop_enabled"] = True
            elif not anchor_early_stop and st.session_state.get("anchor_stop_enabled"):
                st.toast(_t("Using standard early stopping method"))
                st.session_state["anchor_stop_enabled"] = False
            
            # NEW: Horizon balancing to fix H+1 > H+7 bias
            st.subheader(_t("Horizon Balancing Configuration"))
            balance_horizons = st.checkbox(
                _t("Balance horizon training"), 
                value=True, 
                help=_t("Ensures equal priority for all forecast horizons (H+1 through H+7) during model training")
            )
            if balance_horizons and "horizon_balance_enabled" not in st.session_state:
                st.toast(_t("Horizon balancing enabled - correcting forecast horizon bias"))
                st.session_state["horizon_balance_enabled"] = True
            elif not balance_horizons and st.session_state.get("horizon_balance_enabled"):
                st.toast(_t("Using standard horizon weighting approach"))
                st.session_state["horizon_balance_enabled"] = False
            
            horizon_strategy = st.selectbox(
                _t("Horizon weighting strategy"),
                [_t("equal"), _t("increasing"), _t("decreasing")],
                index=0,
                help=_t("equal: All horizons get equal priority (recommended) | increasing: Later horizons get more weight | decreasing: Earlier horizons get more weight")
            )
            
            # Show notification when strategy changes
            if horizon_strategy != st.session_state.get("last_horizon_strategy", _t("equal")):
                if horizon_strategy == _t("increasing"):
                    st.toast(_t("Later horizons (H+7) will receive 4x more weight than earlier horizons (H+1)"))
                elif horizon_strategy == _t("decreasing"):
                    st.toast(_t("Earlier horizons (H+1) will receive 4x more weight than later horizons (H+7)"))
                else:
                    st.toast(_t("All horizons will receive equal weight (1.0x each)"))
                st.session_state["last_horizon_strategy"] = horizon_strategy

            # ---------------- Training split mode -----------------
            split_mode = st.radio(
                _t("Training split mode"),
                [_t("Percentage"), _t("Last 30 days")],
                index=0,
                horizontal=True,
                help="Choose how to divide data into training vs validation sets.",
            )

            if split_mode == _t("Percentage"):
                train_pct = st.slider(
                    _t("Train split (%)"),
                    50,
                    100,
                    80,
                    step=5,
                    help="Percentage of data used for training; set to 100% to train on the whole dataset without a validation split.",
                )
                use_last_30 = False
            else:
                # Fixed 30-day window selected â€“ ignore percentage slider.
                train_pct = None
                use_last_30 = True

            train_pressed = st.button(_t("Train on uploaded CSV"))
            if train_pressed:
                st.toast(_t("Initiating model training process..."))

        if train_pressed and uploaded_file:
            with st.spinner(_t("Training model â€“ please wait...")):
                if 'use_gpu_optuna' not in locals():
                    use_gpu_optuna = False
                if 'gpu_available' not in locals():
                    gpu_available = False
                if 'gpu_optimization_mode' not in locals():
                    gpu_optimization_mode = "cpu"
                if 'gpu_device_id' not in locals():
                    gpu_device_id = 0
                if 'gpu_use_double_precision' not in locals():
                    gpu_use_double_precision = True
                if 'lgbm_n_jobs' not in locals():
                    lgbm_n_jobs = -1
                # -------- Data preparation --------
                df = _get_preprocessed_df(uploaded_file)

                if "temperature_grain_h1d" not in df.columns:
                    df = features.add_multi_horizon_targets(df, horizons=HORIZON_TUPLE)

                if future_safe:
                    df = df.drop(columns=ENV_COLUMNS + FUTURE_SAFE_EXTRA, errors="ignore")

                # Ensure multi-horizon target columns present (fast-path CSVs may lack them)
                if "temperature_grain_h1d" not in df.columns:
                    df = features.add_multi_horizon_targets(df, horizons=HORIZON_TUPLE)

                # Consistent sorting & grouping
                df = comprehensive_sort(df)
                df = assign_group_id(df)

                # Feature matrix / target (MULTI-OUTPUT)
                X_all, y_all = features.select_feature_target_multi(
                    df, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE
                )  # NEW

                # -------- Group-aware hold-out with user-defined split --------
                _d(f"[TRAIN] Preparing train/test split â€“ total rows={len(df)} sensors={df[['grid_x','grid_y','grid_z']].drop_duplicates().shape[0] if {'grid_x','grid_y','grid_z'}.issubset(df.columns) else 'N/A'}")

                if use_last_30:
                    df_train_tmp, df_eval_tmp = split_train_last_n_days(df, n_days=30)
                    X_tr, y_tr = features.select_feature_target_multi(
                        df_train_tmp, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE
                    )
                    X_te, y_te = features.select_feature_target_multi(
                        df_eval_tmp, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE
                    )
                    perform_validation = not X_te.empty
                    _d(f"[SPLIT] Last-30days mode â€“ train rows={len(df_train_tmp)}, val rows={len(df_eval_tmp)}")
                    _d(f"[TARGETS] Multi-horizon targets shape: y_tr={y_tr.shape}, y_te={y_te.shape}, horizons={HORIZON_TUPLE}")
                    _d(f"[TARGETS] Target columns: {list(y_tr.columns) if hasattr(y_tr, 'columns') else 'single column'}")
                elif train_pct == 100:
                    X_tr, y_tr = X_all, y_all
                    X_te = y_te = pd.DataFrame()
                    perform_validation = False
                    _d("[SPLIT] 100% training â€“ no explicit validation set")
                else:
                    test_frac_chrono = max(0.05, 1 - train_pct / 100)
                    df_train_tmp, df_eval_tmp = split_train_eval_frac(df, test_frac=test_frac_chrono)
                    X_tr, y_tr = features.select_feature_target_multi(
                        df_train_tmp, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE
                    )
                    X_te, y_te = features.select_feature_target_multi(
                        df_eval_tmp, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE
                    )
                    perform_validation = not X_te.empty
                    _d(f"[SPLIT] Fraction mode ({train_pct}% train) â€“ train rows={len(df_train_tmp)}, val rows={len(df_eval_tmp)}")

                # Default starting parameters (moved outside conditional) - Using optimal parameters
                base_params = dict(
                    learning_rate=0.07172794499286328,  # Optimal parameter
                    max_depth=20,                       # Optimal parameter
                    num_leaves=133,                     # Optimal parameter
                    subsample=0.8901667731353657,       # Optimal parameter
                    colsample_bytree=0.7729605909501445, # Optimal parameter
                    min_child_samples=102,              # Optimal parameter
                    lambda_l1=1.4182488012070926,       # Optimal parameter
                    lambda_l2=1.7110926238653472,       # Optimal parameter
                    max_bin=416,                        # Optimal parameter
                    n_jobs=lgbm_n_jobs if 'lgbm_n_jobs' in locals() else -1,  # NEW: Use configured threading
                    # ðŸ†• COMPRESSION OPTIMIZATIONS (40-70% smaller files, no accuracy loss)
                    compress=True,                      # Enable built-in compression
                    compression_level=6,                # Compression level (1-9, higher = smaller)
                    save_binary=True,                   # Save in binary format (smaller)
                )

                # Apply quantile objective if selected
                if use_quantile:
                    base_params.update({
                        "objective": "quantile",
                        "alpha": 0.5,
                    })

                # Initialize variables that will be used later
                use_wrapper = False
                suffix = "lgbm"
                base_mdl = None

                if model_choice == "LightGBM":
                    # --------- Always check for cached parameters first ------------------------
                    csv_filename = uploaded_file.name
                    _d(f"[MODEL-CONFIG] LightGBM selected, tune_optuna: {tune_optuna}")
                    _d(f"[CACHE-DEBUG] Checking cache for CSV: {csv_filename}")
                    _d(f"[CACHE-DEBUG] use_param_cache: {use_param_cache}")
                    _d(f"[CACHE-DEBUG] force_reoptimize: {force_reoptimize}")
                    
                    # Create model configuration for cache lookup
                    model_config = {
                        "model_type": "LightGBM",
                        "future_safe": future_safe,
                        "use_quantile": use_quantile,
                        "balance_horizons": balance_horizons,
                        "horizon_strategy": horizon_strategy,  # Include horizon strategy
                        "anchor_early_stop": anchor_early_stop,
                        "optuna_speed_mode": optuna_speed_mode,
                        "optuna_parallel": optuna_parallel,
                        "optuna_n_jobs": optuna_n_jobs if optuna_parallel else 1,
                        "train_split": "last30" if use_last_30 else f"pct{train_pct}",
                        "n_trials": n_trials,  # Include number of trials
                        "conservative_mode": True,  # Always enabled
                        "uncertainty_estimation": True,  # Always enabled
                        "stability_feature_boost": 3.0,  # Include feature boost settings
                        "directional_feature_boost": 2.0,
                        "n_trees": n_trees,  # Include tree count
                        "data_rows": len(df),  # Include data size for better cache validation
                        "horizons": HORIZON_TUPLE  # Include forecast horizons
                    }
                    
                    # Check for cached parameters (regardless of Optuna setting)
                    cached_result = None
                    if use_param_cache and not force_reoptimize:
                        _d("[CACHE] Checking for cached optimal parameters...")
                        _d(f"[CACHE] Data filename: {csv_filename}")
                        _d(f"[CACHE] Model config keys: {list(model_config.keys())}")
                        _d(f"[CACHE] Model config: {model_config}")
                        cached_result = load_optimal_params(csv_filename, df, model_config)
                        _d(f"[CACHE] Cache loading result: {cached_result is not None}")
                        if cached_result:
                            best_params, best_value, timestamp = cached_result
                            _d(f"[CACHE] Found cached params: {best_params}")
                            _d(f"[CACHE] Best value: {best_value}, timestamp: {timestamp}")
                    else:
                        _d(f"[CACHE] Skipping cache check - use_param_cache: {use_param_cache}, force_reoptimize: {force_reoptimize}")
                    
                    # Use cached parameters if available
                    if cached_result and not force_reoptimize:
                        # Use cached parameters
                        best_params, best_value, timestamp = cached_result
                        base_params.update(best_params)
                        
                        # Keep quantile objective if it was selected
                        if use_quantile:
                            base_params.update({
                                "objective": "quantile",
                                "alpha": 0.5,
                            })
                        
                        _d(f"[CACHE-HIT] Using cached optimal parameters from {timestamp}")
                        _d(f"[CACHE-HIT] Best cached anchor-7d MAE: {best_value:.4f}")
                        _d(f"[CACHE-HIT] Cached parameters: {best_params}")
                        
                        st.success(f"ðŸ“¦ **{_t('Using cached optimal parameters!')}**\n{_t('Best MAE')}: {best_value:.4f} | {_t('Cached')}: {timestamp}")
                        
                        # Enhanced cached parameters display
                        with st.expander("ðŸ” **Cached Parameters Details**", expanded=False):
                            col1, col2 = st.columns(2)
                            with col1:
                                st.write("**Optimal Parameters:**")
                                for param, value in best_params.items():
                                    if isinstance(value, float):
                                        st.write(f"- **{param}**: {value:.4f}")
                                    else:
                                        st.write(f"- **{param}**: {value}")
                                        
                            with col2:
                                st.write("**Cache Information:**")
                                st.write(f"- **Best MAE**: {best_value:.4f}")
                                st.write(f"- **Cached**: {timestamp}")
                                st.write(f"- **Data File**: {csv_filename}")
                                st.write(f"- **Data Shape**: {df.shape}")
                                st.write(f"- **Speed Mode**: {optuna_speed_mode}")
                                st.write(f"- **Trials**: {n_trials}")
                                
                        st.toast(f"ðŸ“¦ Loaded cached parameters: MAE {best_value:.4f}", icon="ðŸ“¦")
                        
                    # If no cached parameters found, decide whether to run Optuna or use defaults
                    elif tune_optuna:
                            # Run Optuna optimization
                            try:
                                import optuna
                                
                                # Provide detailed cache miss information
                                if force_reoptimize:
                                    _d(f"[CACHE-SKIP] Force re-optimization enabled for {csv_filename}")
                                    st.info("ðŸ”„ **Force re-optimization enabled** - running Optuna despite available cache")
                                else:
                                    _d(f"[CACHE-MISS] No cached parameters found for {csv_filename}")
                                    _d(f"[CACHE-MISS] Configuration: {model_config}")
                                    st.info("ðŸ” **No cached parameters found** - running Optuna optimization")
                                    
                                # Show current cached parameters for reference
                                cached_params = list_cached_params()
                                if cached_params:
                                    _d(f"[CACHE-INFO] Found {len(cached_params)} cached parameter sets:")
                                    for key, info in cached_params.items():
                                        _d(f"[CACHE-INFO] {key}: {info.get('csv_filename')} - MAE: {info.get('best_value', 'N/A')}")
                                else:
                                    _d("[CACHE-INFO] No cached parameters available")
                                
                                # Clear any previous trial results and show starting message
                                st.session_state["optuna_trial_results"] = []
                                with st.container():
                                    st.info(_t("Starting Optuna hyperparameter optimization: {} trials").format(n_trials))
                                    if optuna_parallel and optuna_n_jobs > 1:
                                        st.info(_t("Using {} parallel processes for enhanced optimization speed").format(optuna_n_jobs))
                                    st.info(_t("Live trial results will be displayed below as optimization progresses..."))

                                # --------------------------------------------------
                                # Determine optimisation split
                                # --------------------------------------------------
                                if X_te.empty:
                                    # 100 % training selected âžœ create internal 95/5 split
                                    _d("[OPTUNA] Creating internal 95/5 split for tuning (100% train mode)")
                                    _opt_tr_df, _opt_val_df = split_train_eval_frac(df, test_frac=0.05)
                                    X_opt_tr, y_opt_tr = features.select_feature_target_multi(
                                        _opt_tr_df, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE
                                    )
                                    X_opt_val, y_opt_val = features.select_feature_target_multi(
                                        _opt_val_df, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE
                                    )
                                else:
                                    X_opt_tr, y_opt_tr, X_opt_val, y_opt_val = X_tr, y_tr, X_te, y_te

                                                                # ------------------------------------------------------------------
                                # Optuna objective using GroupKFold by calendar-week and MAE metric
                                # ------------------------------------------------------------------

                                # Create grouping by ISO calendar week to avoid leakage within the same week
                                if X_te.empty:
                                    # We created internal split, use _opt_tr_df
                                    _grp_src_df = _opt_tr_df.copy()
                                else:
                                    # Using external validation, use original df rows
                                    _grp_src_df = df.loc[X_opt_tr.index].copy()
                                
                                _grp_src_df["_week"] = pd.to_datetime(_grp_src_df["detection_time"], errors="coerce").dt.isocalendar().week
                                groups_arr = _grp_src_df.loc[X_opt_tr.index, "_week"].to_numpy()

                                def objective(trial):
                                    params = {
                                        "learning_rate": trial.suggest_float("learning_rate", 0.005, 0.15, log=True),
                                        "max_depth": trial.suggest_int("max_depth", 3, 20),  # Increased from 10 to 20
                                        "num_leaves": trial.suggest_int("num_leaves", 16, 256),  # Increased from 128
                                        "subsample": trial.suggest_float("subsample", 0.5, 1.0),
                                        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.4, 1.0),
                                        "min_child_samples": trial.suggest_int("min_child_samples", 5, 500),  # Increased from 300
                                        "lambda_l1": trial.suggest_float("lambda_l1", 0.0, 2.0),
                                        "lambda_l2": trial.suggest_float("lambda_l2", 0.0, 2.0),
                                        "max_bin": trial.suggest_int("max_bin", 255, 511),  # For better precision with deep trees
                                        "n_jobs": lgbm_n_jobs,  # NEW: Use configured threading
                                        # ðŸ†• COMPRESSION OPTIMIZATIONS (40-70% smaller files, no accuracy loss)
                                        "compress": True,                      # Enable built-in compression
                                        "compression_level": 6,                # Compression level (1-9, higher = smaller)
                                        "save_binary": True,                   # Save in binary format (smaller)
                                    }
                                    
                                    # NEW: GPU Acceleration Parameters
                                    if use_gpu_optuna and gpu_available:
                                        # Add GPU device parameters
                                        params.update({
                                            "device": "gpu",
                                            "gpu_platform_id": 0,
                                            "gpu_device_id": gpu_device_id,
                                            "gpu_use_dp": gpu_use_double_precision,
                                        })
                                        
                                        # Optimize GPU-specific parameters based on mode
                                        if gpu_optimization_mode == "auto":
                                            # Auto-optimize based on dataset size
                                            dataset_size = len(X_opt_tr)
                                            if dataset_size > 50000:
                                                params.update({
                                                    "gpu_use_dp": False,  # Use single precision for large datasets
                                                    "max_bin": 255,  # Smaller bins for GPU efficiency
                                                })
                                        elif gpu_optimization_mode == "speed":
                                            params.update({
                                                "gpu_use_dp": False,  # Single precision for speed
                                                "max_bin": 255,  # Smaller bins
                                                "feature_fraction": 0.8,  # Reduce feature sampling
                                            })
                                        elif gpu_optimization_mode == "accuracy":
                                            params.update({
                                                "gpu_use_dp": True,  # Double precision for accuracy
                                                "max_bin": 511,  # Larger bins for precision
                                            })
                                    else:
                                        # CPU fallback
                                        params["device"] = "cpu"
                                    
                                    # Apply quantile objective if selected
                                    if use_quantile:
                                        params.update({
                                            "objective": "quantile",
                                            "alpha": 0.5,
                                        })

                                    # ========================================================
                                    # ANCHOR-DAY PERFORMANCE OPTIMIZATION
                                    # ========================================================
                                    # Instead of generic MAE, optimize for continuous 7-day 
                                    # forecast performance using anchor-day methodology
                                    
                                    # OPTIMIZATION: Conditional CV folds based on speed mode
                                    n_folds = 2 if optuna_speed_mode else 3
                                    kf = GroupKFold(n_splits=n_folds)
                                    fold_anchor_maes = []
                                    
                                    for train_idx, val_idx in kf.split(X_opt_tr, y_opt_tr, groups=groups_arr):
                                        X_tr_fold = X_opt_tr.iloc[train_idx]
                                        y_tr_fold = y_opt_tr.iloc[train_idx]
                                        X_val_fold = X_opt_tr.iloc[val_idx]
                                        y_val_fold = y_opt_tr.iloc[val_idx]

                                        # OPTIMIZATION: Conditional performance settings based on speed mode
                                        trial_trees = min(500, n_trees) if optuna_speed_mode else n_trees
                                        trial_early_stop = 50 if optuna_speed_mode else 100
                                        
                                        mdl_tmp = MultiLGBMRegressor(
                                            base_params=params,
                                            upper_bound_estimators=trial_trees,
                                            early_stopping_rounds=trial_early_stop,
                                            uncertainty_estimation=True,
                                            n_bootstrap_samples=30,
                                            directional_feature_boost=2.0,  # 2x boost for directional features
                                            conservative_mode=True,  # Enable conservative predictions
                                            stability_feature_boost=3.0,  # 3x boost for stability features
                                            use_gpu=(use_gpu_optuna and gpu_available),  # NEW: GPU acceleration with availability check
                                            gpu_optimization=(gpu_optimization_mode == "auto"),  # NEW: Auto-optimization
                                        )
                                        
                                        # Generate anchor-day evaluation on this fold
                                        try:
                                            # OPTIMIZATION: Get the dataframe subset for this validation fold (minimal copy)
                                            val_df_fold = df.loc[X_val_fold.index, 
                                                               ['detection_time', 'temperature_grain'] + 
                                                               [f'temperature_grain_h{h}d' for h in HORIZON_TUPLE]
                                                              ].copy()
                                        except Exception as e:
                                            # Fallback to using the full validation set if optimization fails
                                            val_df_fold = df.loc[X_val_fold.index].copy()
                                        
                                        mdl_tmp.fit(
                                            X_tr_fold, y_tr_fold, 
                                            eval_set=(X_val_fold, y_val_fold), 
                                            verbose=False,
                                            anchor_df=val_df_fold,  # Use fold validation df for anchor-day methodology
                                            horizon_tuple=HORIZON_TUPLE,
                                            use_anchor_early_stopping=anchor_early_stop,
                                            balance_horizons=balance_horizons,  # NEW: Apply horizon balancing
                                            horizon_strategy=horizon_strategy,  # NEW: Pass strategy selection

                                        )
                                        
                                        # Continue with anchor-day evaluation
                                        try:
                                            
                                            # Generate predictions for all horizons
                                            preds_tmp = mdl_tmp.predict(X_val_fold)
                                            
                                            # Attach multi-horizon predictions to validation dataframe
                                            for idx, h in enumerate(HORIZON_TUPLE):
                                                if idx < preds_tmp.shape[1]:
                                                    val_df_fold.loc[X_val_fold.index, f"pred_h{h}d"] = preds_tmp[:, idx]
                                            
                                            # Compute anchor-day performance (simplified version)
                                            anchor_maes = []
                                            
                                            # Get unique anchor dates (days where we have 7-day continuous forecasts)
                                            val_df_fold["anchor_date"] = pd.to_datetime(val_df_fold["detection_time"]).dt.date
                                            anchor_dates = val_df_fold["anchor_date"].unique()
                                            
                                            # OPTIMIZATION: Conditional anchor date sampling based on speed mode
                                            n_anchor_dates = 5 if optuna_speed_mode else 10
                                            for anchor_date in anchor_dates[-n_anchor_dates:]:
                                                anchor_rows = val_df_fold[val_df_fold["anchor_date"] == anchor_date]
                                                if len(anchor_rows) < 5:  # Need minimum sensors
                                                    continue
                                                    
                                                # Compute MAE for each horizon on this anchor date
                                                horizon_maes = []
                                                for h in HORIZON_TUPLE:
                                                    pred_col = f"pred_h{h}d"
                                                    if pred_col in anchor_rows.columns:
                                                        actual_col = f"temperature_grain_h{h}d"
                                                        if actual_col in anchor_rows.columns:
                                                            mask = anchor_rows[[actual_col, pred_col]].notna().all(axis=1)
                                                            if mask.sum() > 0:
                                                                mae_h = np.abs(anchor_rows.loc[mask, actual_col] - anchor_rows.loc[mask, pred_col]).mean()
                                                                horizon_maes.append(mae_h)
                                            
                                            if horizon_maes:
                                                anchor_maes.append(np.mean(horizon_maes))
                                            
                                            if anchor_maes:
                                                fold_anchor_maes.append(np.mean(anchor_maes))
                                            else:
                                                fold_anchor_maes.append(999.0)  # Penalty for no valid anchor evaluation
                                                
                                        except Exception as e:
                                            fold_anchor_maes.append(999.0)  # Penalty for evaluation failure

                                    mean_anchor_mae = float(np.mean(fold_anchor_maes)) if fold_anchor_maes else 999.0

                                    # Store trial results for persistent display instead of disappearing toast
                                    import streamlit as _st
                                    trial_results = _st.session_state.setdefault("optuna_trial_results", [])
                                    trial_results.append({
                                        "trial": trial.number,
                                        "mae": mean_anchor_mae,
                                        "params": {k: v for k, v in trial.params.items()}
                                    })
                                    # Keep only last 20 trials to show more progress
                                    if len(trial_results) > 20:
                                        trial_results.pop(0)
                                    
                                    _d(f"[OPTUNA] Trial {trial.number} â€“ Anchor-7d MAE {mean_anchor_mae:.4f}")
                                    return mean_anchor_mae
                                
                                # ------------------------------------------------------------------
                                # Create and run Optuna study with parallel optimization
                                # ------------------------------------------------------------------
                                study = optuna.create_study(direction="minimize")
                                
                                # Initialize trial results display
                                st.session_state["optuna_trial_results"] = []
                                trial_placeholder = st.empty()
                                progress_bar = st.progress(0)
                                status_text = st.empty()
                                
                                # Parallel optimization with progress feedback
                                if optuna_parallel and optuna_n_jobs > 1:
                                    _d(f"[OPTUNA] Starting parallel optimization: {n_trials} trials using {optuna_n_jobs} processes")
                                    st.toast(f"Parallel Optuna optimization: {optuna_n_jobs} processes Ã— {n_trials} trials")
                                    
                                    # Create a callback to update trial results display
                                    def trial_callback(study, trial):
                                        trial_results = st.session_state.get("optuna_trial_results", [])
                                        
                                        # Update progress bar and status using actual trial number
                                        current_trial = trial.number + 1
                                        progress = current_trial / n_trials
                                        progress_bar.progress(min(progress, 1.0))
                                        status_text.text(f"Trial {current_trial}/{n_trials} completed - Current MAE: {trial.value:.4f}")
                                        
                                        if trial_results:
                                            with trial_placeholder.container():
                                                st.markdown("### **Live Optuna Trial Results**")
                                                
                                                # Show best trial so far
                                                best_trial = min(trial_results, key=lambda x: x['mae'])
                                                st.success(f"**Best Trial**: #{best_trial['trial']} - MAE: {best_trial['mae']:.4f}")
                                                
                                                # Show recent trials table (last 10 trials)
                                                recent_trials = trial_results[-10:]  # Last 10 trials
                                                trials_df = pd.DataFrame([
                                                    {
                                                        "Trial": t["trial"],
                                                        "MAE": f"{t['mae']:.4f}",
                                                        "Learning Rate": f"{t['params'].get('learning_rate', 0):.4f}",
                                                        "Max Depth": t['params'].get('max_depth', 0),
                                                        "Num Leaves": t['params'].get('num_leaves', 0)
                                                    }
                                                    for t in recent_trials
                                                ])
                                                st.dataframe(trials_df, use_container_width=True, key=f"trials_{current_trial}")
                                    
                                    study.optimize(objective, n_trials=n_trials, n_jobs=optuna_n_jobs, callbacks=[trial_callback])
                                    st.toast(f"Parallel optimization completed using {optuna_n_jobs} processes")
                                else:
                                    _d(f"[OPTUNA] Starting sequential optimization: {n_trials} trials")
                                    
                                    # Create a callback to update trial results display
                                    def trial_callback(study, trial):
                                        trial_results = st.session_state.get("optuna_trial_results", [])
                                        
                                        # Update progress bar and status using actual trial number
                                        current_trial = trial.number + 1
                                        progress = current_trial / n_trials
                                        progress_bar.progress(min(progress, 1.0))
                                        status_text.text(f"Trial {current_trial}/{n_trials} completed - Current MAE: {trial.value:.4f}")
                                        
                                        if trial_results:
                                            with trial_placeholder.container():
                                                st.markdown("### **Live Optuna Trial Results**")
                                                
                                                # Show best trial so far
                                                best_trial = min(trial_results, key=lambda x: x['mae'])
                                                st.success(f"**Best Trial**: #{best_trial['trial']} - MAE: {best_trial['mae']:.4f}")
                                                
                                                # Show recent trials table (last 10 trials)
                                                recent_trials = trial_results[-10:]  # Last 10 trials
                                                trials_df = pd.DataFrame([
                                                    {
                                                        "Trial": t["trial"],
                                                        "MAE": f"{t['mae']:.4f}",
                                                        "Learning Rate": f"{t['params'].get('learning_rate', 0):.4f}",
                                                        "Max Depth": t['params'].get('max_depth', 0),
                                                        "Num Leaves": t['params'].get('num_leaves', 0)
                                                    }
                                                    for t in recent_trials
                                                ])
                                                st.dataframe(trials_df, use_container_width=True, key=f"trials_{current_trial}")
                                    
                                    study.optimize(objective, n_trials=n_trials, callbacks=[trial_callback])
                                
                                # Update base_params with best parameters
                                best_params = study.best_params
                                base_params.update(best_params)
                                
                                # Keep quantile objective if it was selected
                                if use_quantile:
                                    base_params.update({
                                        "objective": "quantile",
                                        "alpha": 0.5,
                                    })
                                
                                _d(f"[OPTUNA] Optimization complete. Best anchor-7d MAE: {study.best_value:.4f}")
                                _d(f"[OPTUNA] Best params: {best_params}")
                                
                                # Final summary of optimization results - hide progress indicators
                                progress_bar.empty()
                                status_text.empty()
                                
                                trial_results = st.session_state.get("optuna_trial_results", [])
                                if trial_results:
                                    with trial_placeholder.container():
                                        st.markdown("### **Optuna Optimization Complete**")
                                        
                                        col1, col2, col3 = st.columns(3)
                                        with col1:
                                            st.metric("Total Trials", len(trial_results))
                                        with col2:
                                            best_mae = min(t['mae'] for t in trial_results)
                                            st.metric("Best MAE", f"{best_mae:.4f}")
                                        with col3:
                                            worst_mae = max(t['mae'] for t in trial_results)
                                            improvement = ((worst_mae - best_mae) / worst_mae * 100)
                                            st.metric("Improvement", f"{improvement:.1f}%")
                                        
                                        # Show all trials in an expandable section
                                        with st.expander("**View All Trial Results**", expanded=False):
                                            all_trials_df = pd.DataFrame([
                                                {
                                                    "Trial": t["trial"],
                                                    "MAE": f"{t['mae']:.4f}",
                                                    "Learning Rate": f"{t['params'].get('learning_rate', 0):.4f}",
                                                    "Max Depth": t['params'].get('max_depth', 0),
                                                    "Num Leaves": t['params'].get('num_leaves', 0),
                                                    "Subsample": f"{t['params'].get('subsample', 0):.3f}",
                                                    "Colsample": f"{t['params'].get('colsample_bytree', 0):.3f}"
                                                }
                                                for t in trial_results
                                            ])
                                            st.dataframe(all_trials_df.sort_values("MAE"), use_container_width=True, key="final_trials")
                                        
                                        st.success(f"**Best Parameters Found**: Learning Rate: {best_params.get('learning_rate', 0):.4f}, "
                                                 f"Max Depth: {best_params.get('max_depth', 0)}, Num Leaves: {best_params.get('num_leaves', 0)}")
                                
                                # Save optimal parameters to cache
                                if use_param_cache:
                                    try:
                                        cache_key = save_optimal_params(
                                            csv_filename=csv_filename,
                                            df=df,
                                            model_config=model_config,
                                            optimal_params=best_params,
                                            best_value=study.best_value,
                                            n_trials=n_trials
                                        )
                                        _d(f"[CACHE] Saved optimal parameters with key: {cache_key}")
                                        st.success(f"ðŸ’¾ Optimal parameters saved to cache!")
                                    except Exception as cache_exc:
                                        _d(f"[CACHE] Failed to save parameters: {cache_exc}")
                                        st.warning("Failed to save parameters to cache")
                                
                                # Performance feedback
                                n_folds = 2 if optuna_speed_mode else 3
                                trial_trees = min(500, n_trees) if optuna_speed_mode else n_trees
                                parallel_info = f", {optuna_n_jobs} parallel jobs" if optuna_parallel and optuna_n_jobs > 1 else ""
                                speed_info = f" (Fast mode: {n_folds}-fold CV, {trial_trees} trees{parallel_info})" if optuna_speed_mode else f"{parallel_info}"
                                st.toast(f"Optuna optimization completed. Best MAE: {study.best_value:.4f}{speed_info}")
                                
                            except Exception as exc:
                                _d(f"[OPTUNA] Tuning failed or Optuna not installed: {exc}")
                                st.warning(f"Optuna optimization failed: {exc}")
                    
                    # No cached parameters and Optuna disabled - use default parameters
                    else:
                        if not cached_result:
                            _d(f"[MODEL-CONFIG] No cached parameters found and Optuna disabled - using default parameters")
                            _d(f"[MODEL-CONFIG] Default base_params: {base_params}")
                            if force_reoptimize:
                                st.info("ðŸ”„ **Force re-optimization enabled** but Optuna is disabled - using default parameters")
                            else:
                                st.info("ðŸ“‹ **Using default parameters** - enable Optuna or use parameter cache for optimized settings")

                    # ================================================================
                    # PERFORMANCE OPTIMIZATIONS FOR 7-DAY FORECASTING
                    # ================================================================
                    # 1. Anchor-day early stopping: Check every 10 iterations (10x faster)
                    # 2. Fast Optuna mode: 2-fold CV, lower tree limits, aggressive stopping  
                    # 3. Conditional early stopping: 50 rounds with anchor mode vs 100 standard
                    # 4. Vectorized computations: Batch operations for anchor MAE calculation
                    # 5. Minimal dataframe copying: Only essential columns in Optuna loops
                    # ================================================================
                    
                    main_early_stop = 50 if anchor_early_stop else 100
                    
                    base_mdl = MultiLGBMRegressor(
                        base_params=base_params,
                        # ðŸš€ OPTIMIZED: Using granarypredict defaults for speed improvements
                        # upper_bound_estimators=1000 (default), early_stopping_rounds=50 (default)
                        uncertainty_estimation=True,
                        n_bootstrap_samples=25,  # ðŸš€ OPTIMIZED: Reduced from 100 for 75% speed improvement
                        directional_feature_boost=2.0,  # 2x boost for directional features
                        conservative_mode=True,  # Enable conservative predictions
                        stability_feature_boost=3.0,  # 3x boost for stability features
                        use_gpu=(use_gpu_optuna and gpu_available),  # NEW: GPU acceleration with availability check
                        gpu_optimization=(gpu_optimization_mode == "auto"),  # NEW: Auto-optimization
                    )
                    use_wrapper = False
                    _d(f"[TRAIN] LightGBM initialised â€“ upper_bound={n_trees}, early_stop={main_early_stop}, base_params={base_params}")
                    _d(f"[TRAIN-UNCERTAINTY] Uncertainty estimation ENABLED: n_bootstrap_samples=100")
                    _d(f"[TRAIN-CONSERVATIVE] Conservative mode ENABLED: stability_feature_boost=3.0x")
                    _d(f"[TRAIN-DIRECTIONAL] Directional feature boost: 2.0x for movement prediction")
                    st.toast("ðŸ”¬ Training with conservative mode: 3x stability boost + 2x directional boost", icon="ðŸ”¬")
                    
                    # Enhanced conservative system feedback
                    with st.expander(_t("Conservative System Status"), expanded=True):
                        st.success(f"**{_t('Conservative Temperature Prediction')}**: {_t('Active')}")
                        st.write(f"**{_t('System Configuration')}:**")
                        col1, col2 = st.columns(2)
                        with col1:
                            st.info(f"**{_t('Stability Features')}**: {_t('8 thermal physics features')}")
                            st.info(f"**{_t('Stability Boost')}**: {_t('3.0x feature importance')}")
                            st.info(f"**{_t('Uncertainty Samples')}**: {_t('50 bootstrap samples')}")
                        with col2:
                            st.info(f"**{_t('Directional Features')}**: {_t('2.0x importance boost')}")
                            st.info(f"**{_t('Horizon Balancing')}**: {_t('Enabled')}")
                            st.info(f"**{_t('Conservative Loss')}**: {_t('Thermal inertia penalties')}")
                        
                        st.markdown(f"**{_t('Expected Benefits')}:**")
                        st.markdown(f"""
                        - **{_t('More stable predictions')}**: {_t('Respects grain thermal inertia')}
                        - **{_t('Reduced aggressive changes')}**: {_t('Conservative temperature evolution')}
                        - **{_t('Better 7-day accuracy')}**: {_t('Less cumulative error buildup')}
                        - **{_t('Uncertainty quantification')}**: {_t('Confidence intervals for all predictions')}
                        """)
                        
                        # Real-time training feedback
                        st.toast("Conservative system initialized - thermal stability features active")
                        if hasattr(st, 'rerun'):
                            pass  # Don't force rerun during training

                # ---------------- Fit -----------------------
                if use_wrapper:
                    mdl = MultiOutputRegressor(base_mdl)
                    mdl.fit(X_tr, y_tr)
                    _d("[TRAIN] Wrapper model fit complete")
                else:
                    if perform_validation and not X_te.empty:
                        # Anchor-day early stopping using external validation split for 7-day consecutive accuracy
                        base_mdl.fit(
                            X_tr, y_tr, 
                            eval_set=(X_te, y_te), 
                            verbose=False,
                            anchor_df=df_eval_tmp,  # Pass evaluation dataframe for anchor-day methodology
                            horizon_tuple=HORIZON_TUPLE,
                            use_anchor_early_stopping=anchor_early_stop,
                            balance_horizons=balance_horizons,  # NEW: Apply horizon balancing
                            horizon_strategy=horizon_strategy,  # NEW: Pass strategy selection
                        )
                        _d(f"[TRAIN] Anchor-day early-stopping complete â€“ best_iter={base_mdl.best_iteration_}")
                        mdl = base_mdl
                    else:
                        # ----------------------------------------------------------
                        # No validation split (user selected 100 % train) â€“> create
                        # an internal 95/5 chronological split to pick the best
                        # iteration, then refit on the full dataset with that
                        # fixed n_estimators so behaviour matches the legacy flow.
                        # ----------------------------------------------------------
                        int_train_df, int_val_df = split_train_eval_frac(df, test_frac=0.05)

                        X_int_tr, y_int_tr = features.select_feature_target_multi(
                            int_train_df, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE
                        )
                        X_int_val, y_int_val = features.select_feature_target_multi(
                            int_val_df, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE
                        )

                        finder = MultiLGBMRegressor(
                            base_params=base_params,
                            # ðŸš€ OPTIMIZED: Using granarypredict defaults for speed improvements
                            # upper_bound_estimators=1000 (default), early_stopping_rounds=50 (default)
                            uncertainty_estimation=True,
                            n_bootstrap_samples=25,  # ðŸš€ OPTIMIZED: Reduced from 100 for 75% speed improvement
                            directional_feature_boost=2.0,  # 2x boost for directional features
                            conservative_mode=True,  # Enable conservative predictions
                            stability_feature_boost=3.0,  # 3x boost for stability features
                            use_gpu=(use_gpu_optuna and gpu_available),  # NEW: GPU acceleration with availability check
                            gpu_optimization=(gpu_optimization_mode == "auto"),  # NEW: Auto-optimization
                        )
                        _d(f"[TRAIN-INTERNAL] Creating finder model with uncertainty estimation enabled")
                        finder.fit(
                            X_int_tr, y_int_tr, 
                            eval_set=(X_int_val, y_int_val), 
                            verbose=False,
                            anchor_df=int_val_df,
                            horizon_tuple=HORIZON_TUPLE,
                            use_anchor_early_stopping=anchor_early_stop,
                            balance_horizons=balance_horizons,
                            horizon_strategy=horizon_strategy,
                        )
                        best_n = finder.best_iteration_
                        
                        # Refit on **all** data with the chosen tree count
                        final_params = base_params | {"n_estimators": best_n}
                        final_lgbm = MultiLGBMRegressor(
                            base_params=final_params,
                            upper_bound_estimators=best_n,
                            early_stopping_rounds=0,
                            uncertainty_estimation=True,
                            n_bootstrap_samples=100,  # Increased from 50 for better uncertainty estimation
                            directional_feature_boost=2.0,  # 2x boost for directional features
                            conservative_mode=True,  # Enable conservative predictions
                            stability_feature_boost=3.0,  # 3x boost for stability features
                            use_gpu=(use_gpu_optuna and gpu_available),  # NEW: GPU acceleration with availability check
                            gpu_optimization=(gpu_optimization_mode == "auto"),  # NEW: Auto-optimization
                        )
                        _d(f"[TRAIN-FINAL] Creating final model with uncertainty estimation enabled for full dataset")
                        final_lgbm.fit(X_all, y_all, balance_horizons=balance_horizons, horizon_strategy=horizon_strategy)
                        mdl = final_lgbm
                        _d(f"[TRAIN] Refit on full data finished with best_n trees")
                        _d(
                            f"[TRAIN] Internal split sizes â€“ train={len(int_train_df)}, val={len(int_val_df)}; "
                            f"best_n={best_n}"
                        )

                _d(f"{model_choice} model trained (wrapper={use_wrapper})")

                # Validation on unseen groups (if possible)
                if perform_validation and not X_te.empty:
                    preds = mdl.predict(X_te)
                    _d("Predictions generated on validation split")
                    mae_val = mean_absolute_error(y_te, preds)
                    rmse_val = mean_squared_error(y_te, preds) ** 0.5
                    _d(f"Validation metrics â†’ MAE: {mae_val:.3f}, RMSE: {rmse_val:.3f}")
                else:
                    mae_val = rmse_val = float("nan")

                # -------- Persist model --------
                csv_stem = pathlib.Path(uploaded_file.name).stem.replace(" ", "_").lower()
                best_iter_val = int(getattr(mdl, 'best_iteration_', n_trees))
                _d(f"[SAVE] Persisting model with best_iter={best_iter_val}")
                model_name = f"{csv_stem}_{'fs_' if future_safe else ''}{suffix}_{best_iter_val}.joblib"
                _d(f"[SAVE] Model written to {model_name}")
                model_utils.save_model(mdl, name=model_name)

            if np.isnan(mae_val):
                st.toast("Model training completed successfully")
                st.sidebar.success(f"{model_choice} training completed (validation split contained no ground-truth targets).")
            else:
                st.toast(f"Training completed successfully. MAE: {mae_val:.2f}, RMSE: {rmse_val:.2f}")
                st.sidebar.success(f"{model_choice} training completed. MAE: {mae_val:.2f}, RMSE: {rmse_val:.2f}")
            # Persist split settings for later evaluation
            st.session_state["last_train_split_mode"] = "last30" if use_last_30 else "percentage"
            if not use_last_30:
                st.session_state["last_train_pct"] = 100 if train_pct == 100 else train_pct

        # Parallel processing status
        with st.sidebar.expander(_t("Performance Optimization"), expanded=False):
            try:
                parallel_info = features.get_parallel_info()
                if parallel_info['parallel_enabled']:
                    st.success(f"**{_t('Parallel Processing')}**: {_t('Active')}")
                    st.info(f"**{_t('Workers')}**: {parallel_info['max_workers']} of {parallel_info['cpu_count']} CPU cores")
                    st.info(f"**{_t('Expected Speedup')}**: {_t('3-5x faster feature engineering')}")
                    
                    # Show Optuna parallelization status if training is active
                    if 'optuna_parallel' in locals() and optuna_parallel:
                        st.success(f"**Optuna Parallel**: {optuna_n_jobs} processes")
                        st.info(f"**{_t('Expected Speedup')}**: {_t('2-4x faster hyperparameter optimization')}")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        if st.button(_t("Enable All Cores")):
                            features.set_parallel_processing(True, parallel_info['cpu_count'])
                            st.rerun()
                    with col2:
                        if st.button(_t("Disable Parallel")):
                            features.set_parallel_processing(False)
                            st.rerun()
                else:
                    st.warning(f"**{_t('Parallel Processing')}**: {_t('Disabled')}")
                    if st.button(_t("Enable Parallel Processing")):
                        features.set_parallel_processing(True)
                        st.rerun()
            except Exception as e:
                st.error(_t("Could not retrieve parallel processing information: {}").format(e))

        # Parameter cache management
        with st.sidebar.expander(_t("Parameter Cache"), expanded=False):
            cached_params = list_cached_params()
            if cached_params:
                st.write(f"**{_t('Cached parameter sets')}**: {len(cached_params)}")
                
                # Show cache details
                for cache_key, cache_info in cached_params.items():
                    with st.expander(f"{cache_info['csv_filename']} (MAE: {cache_info['best_value']:.3f})", expanded=False):
                        st.write(f"**{_t('Best MAE')}**: {cache_info['best_value']:.4f}")
                        st.write(f"**{_t('Trials')}**: {cache_info['n_trials']}")
                        st.write(f"**{_t('Data shape')}**: {cache_info['data_shape']}")
                        st.write(f"**{_t('Cached')}**: {cache_info['timestamp']}")
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            if st.button(_t("Clear this"), key=f"clear_{cache_key}"):
                                st.toast(_t("Clearing cache for {}...").format(cache_info['csv_filename']))
                                clear_cache(cache_info['csv_filename'])
                                st.rerun()
                
                col1, col2 = st.columns(2)
                with col1:
                    if st.button(_t("Clear all cache")):
                        st.toast(_t("Clearing all cached parameters..."))
                        clear_cache()
                        st.rerun()
                with col2:
                    if st.button("Refresh"):
                        st.toast("Refreshing cache view...")
                        st.rerun()
            else:
                st.write("No cached parameters available.")
                st.caption("Train a model with Optuna optimization to create cache entries.")

        # Existing model evaluation
        with st.sidebar.expander(_t('Evaluate Model'), expanded=False):
            model_files = list_available_models()
            if not model_files:
                st.write(_t("No saved models yet."))
                eval_pressed = False
                selected_model = None
                eval_fc_pressed = False
                all_models_chk = False
            else:
                selected_model = st.selectbox(_t("Model file"), model_files)
                # Checkbox to act on all models
                all_models_chk = st.checkbox(_t("Apply to all models"), key="chk_eval_all")

                col_eval, col_evalfc = st.columns([1,1])
                with col_eval:
                    eval_pressed = st.button(_t("Evaluate"), key="btn_eval_single", use_container_width=True)
                    if eval_pressed:
                        st.toast("Starting model evaluation...")
                with col_evalfc:
                    eval_fc_pressed = st.button(_t("Eval & Forecast"), key="btn_eval_fc", use_container_width=True)
                    if eval_fc_pressed:
                        st.toast("Starting evaluation and forecast generation...")

        if (eval_pressed or eval_fc_pressed):
            if uploaded_file is None:
                st.warning(_t("Please upload a CSV first to evaluate."))
            else:
                # Determine which models to evaluate
                if all_models_chk:
                    target_models = list_available_models()
                else:
                    if selected_model is None:
                        st.warning(_t("No model selected. Please select a model to evaluate."))
                        return
                    target_models = [selected_model]
                
                # Check if any models are available
                if not target_models:
                    st.warning(_t("No trained models found. Please train a model first before evaluation."))
                    st.info(_t("Go to 'Train / Retrain Model' section to train a new model."))
                    return
                
                with st.spinner(_t("Evaluating model(s) â€“ please wait...")):
                    try:
                        df = _get_preprocessed_df(uploaded_file)
                        _d(f"[EVAL] Preprocessed data shape: {df.shape}")
                        
                        # Validate essential columns exist
                        required_cols = [TARGET_TEMP_COL, "detection_time"]
                        missing_cols = [col for col in required_cols if col not in df.columns]
                        if missing_cols:
                            st.error(f"âŒ Missing required columns: {missing_cols}")
                            st.info("Please ensure your data has 'temperature_grain' and 'detection_time' columns.")
                            return
                        
                        # Use same train/test split fraction recorded during training (default 20%)
                        test_frac = max(0.01, 1 - st.session_state.get("last_train_pct", 80)/100)

                        # ---------- Split matching the training configuration ----------
                        split_mode_prev = st.session_state.get("last_train_split_mode", "percentage")

                        if split_mode_prev == "last30":
                            df_train_base, df_eval_base = split_train_last_n_days(df, n_days=30)
                        else:
                            df_train_base, df_eval_base = split_train_eval_frac(df, test_frac=test_frac)

                        _d(f"Evaluation split â€“ train rows: {len(df_train_base)}, test rows: {len(df_eval_base)}")
                        
                        # Check if we have enough data for evaluation
                        if len(df_eval_base) == 0:
                            st.error("âŒ No evaluation data available after splitting. Please check your data or adjust split settings.")
                            return
                            
                        use_gap_fill = False  # skip calendar gap generation â€“ evaluate only real rows

                        try:
                            X_train_base, _ = features.select_feature_target_multi(
                                df_train_base, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE, allow_na=True
                            )  # NEW
                            _d(f"[EVAL] Feature selection successful - X_train_base shape: {X_train_base.shape}")
                        except Exception as e:
                            st.error(f"âŒ Error in feature selection: {str(e)}")
                            st.info("This usually happens when the data doesn't have the required columns or structure.")
                            return
                    except Exception as e:
                        st.error(f"âŒ Error during data preprocessing: {str(e)}")
                        _d(f"[EVAL ERROR] Data preprocessing failed: {e}")
                        return

                    for mdl_name in target_models:
                        # Model loading with enhanced debug information
                        try:
                            _d(f"[EVAL] Starting evaluation for model: {mdl_name}")
                            file_path = MODELS_DIR / mdl_name if not (MODELS_DIR / mdl_name).is_absolute() else pathlib.Path(mdl_name)
                            if not file_path.exists():
                                # Try preloaded directory
                                file_path = PRELOADED_MODEL_DIR / mdl_name
                            
                            _d(f"[EVAL] Model file path: {file_path}")
                            _d(f"[EVAL] File exists: {file_path.exists()}")
                            if file_path.exists():
                                file_size = file_path.stat().st_size / (1024*1024)
                                _d(f"[EVAL] File size: {file_size:.2f} MB")
                            
                            mdl_result = load_trained_model(mdl_name)
                            if isinstance(mdl_result, dict) and "error" in mdl_result:
                                st.error(f"âŒ Model '{mdl_name}': {mdl_result['error']}")
                                st.info("ðŸ’¡ **Troubleshooting Tips:**")
                                st.info("1. The model file might be corrupted or incompatible")
                                st.info("2. If you recently added compression, try the model repair tool:")
                                st.code("python scripts/fix_compressed_models.py --repair " + mdl_name)
                                st.info("3. Or list all models to see their status:")
                                st.code("python scripts/fix_compressed_models.py --list")
                                continue
                            mdl = mdl_result
                            _d(f"[EVAL] Model '{mdl_name}' loaded successfully")

                            df_train = df_train_base.copy()
                            df_eval = df_eval_base.copy()
                        except Exception as e:
                            st.error(f"âŒ Model '{mdl_name}': Unexpected loading error - {str(e)}")
                            with st.expander("ðŸ” Debug Details", expanded=False):
                                st.code(f"Error: {str(e)}")
                                st.code(f"Error Type: {type(e).__name__}")
                                if st.session_state.get("debug_mode"):
                                    st.code(f"Traceback:\n{traceback.format_exc()}")
                            continue
                        
                        # If model is future-safe, drop env/extra cols from evaluation/training sets to mimic determinate-only input
                        if is_future_safe_model(mdl_name):
                            df_train = df_train.drop(columns=ENV_COLUMNS + FUTURE_SAFE_EXTRA, errors="ignore")
                            df_eval = df_eval.drop(columns=ENV_COLUMNS + FUTURE_SAFE_EXTRA, errors="ignore")
                            _d(f"[EVAL] Future-safe mode: removed environmental columns for model '{mdl_name}'")

                        # ---------------- Ensure category codes align with training ----------------
                        cat_cols_train_loop = df_train.select_dtypes(include=["object", "category"]).columns
                        categories_map = {c: pd.Categorical(df_train[c]).categories.tolist() for c in cat_cols_train_loop}

                        # Feature selection with error handling
                        try:
                            X_eval, _ = features.select_feature_target_multi(
                                df_eval, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE, allow_na=True
                            )  # NEW
                            _d(f"[EVAL] Feature selection for evaluation: X_eval shape {X_eval.shape}")
                        except Exception as e:
                            st.error(f"âŒ Model '{mdl_name}': Feature selection failed - {str(e)}")
                            continue
                            
                        # Align features to the model's expected input
                        try:
                            feature_cols_mdl = get_feature_cols(mdl, X_eval)
                            _d(f"[EVAL] Model expects {len(feature_cols_mdl)} features")
                            X_eval_aligned = X_eval.reindex(columns=feature_cols_mdl, fill_value=0)
                            
                            # Check for significant feature misalignment
                            missing_features = set(feature_cols_mdl) - set(X_eval.columns)
                            if missing_features:
                                _d(f"[EVAL] Warning: Model expects {len(missing_features)} features not in data: {list(missing_features)[:5]}...")
                                if len(missing_features) > len(feature_cols_mdl) * 0.5:  # More than 50% missing
                                    st.warning(f"âš ï¸ Model '{mdl_name}': Many expected features are missing ({len(missing_features)}/{len(feature_cols_mdl)}). Results may be unreliable.")
                            
                        except Exception as e:
                            st.error(f"âŒ Model '{mdl_name}': Feature alignment failed - {str(e)}")
                            continue
                            
                        # NEW â€“ generate aligned training design matrix for debugging visualisation
                        try:
                            X_train, _ = features.select_feature_target_multi(
                                df_train, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE, allow_na=True
                            )  # NEW
                            X_train_aligned = X_train.reindex(columns=feature_cols_mdl, fill_value=0)
                        except Exception as e:
                            _d(f"[EVAL] Warning: Could not create training matrix for debugging: {e}")
                            X_train_aligned = pd.DataFrame()  # Fallback
                            
                        # Model prediction with error handling
                        try:
                            preds = model_utils.predict(mdl, X_eval_aligned)
                            _d(f"[EVAL] Predictions generated â€“ shape={preds.shape} for model={mdl_name}")
                            
                            # Log uncertainty estimation if applicable
                            if isinstance(mdl, MultiLGBMRegressor) and hasattr(mdl, 'uncertainty_estimation'):
                                if mdl.uncertainty_estimation:
                                    _d(f"[UNCERTAINTY] Model uses uncertainty estimation (n_bootstrap={getattr(mdl, 'n_bootstrap_samples', 50)})")
                                    st.toast(f"ðŸ”¬ {mdl_name}: Uncertainty estimation active with {getattr(mdl, 'n_bootstrap_samples', 50)} bootstrap samples", icon="ðŸ”¬")
                                else:
                                    _d(f"[UNCERTAINTY] Model uncertainty estimation disabled - point predictions only")
                                    st.toast(f"{mdl_name}: No uncertainty estimation - point predictions only", icon="ðŸŽ¯")
                                    
                        except Exception as e:
                            st.error(f"âŒ Model '{mdl_name}': Prediction failed - {str(e)}")
                            continue

                        # -------- Attach predictions to df_eval --------
                        if getattr(preds, "ndim", 1) == 2:
                            # Multi-output â€“ assign for each configured horizon available
                            for idx, h in enumerate(HORIZON_TUPLE):
                                if idx < preds.shape[1]:
                                    df_eval.loc[X_eval_aligned.index, f"pred_h{h}d"] = preds[:, idx]
                            # For backward-compatibility plots keep original col name (use horizon-1)
                            df_eval.loc[X_eval_aligned.index, "predicted_temp"] = preds[:, 0]
                        else:
                            # Single-output â€“ treat as horizon 1 only
                            df_eval.loc[X_eval_aligned.index, "predicted_temp"] = preds
                            df_eval.loc[X_eval_aligned.index, "pred_h1d"] = preds

                        df_eval["is_forecast"] = False

                        # Combine training (actual only) and evaluation rows for full context time-series
                        df_train_plot = df_train.copy()
                        df_train_plot["is_forecast"] = False
                        df_predplot_all = pd.concat([df_eval, df_train_plot], ignore_index=True)

                        # metrics
                        df_eval_actual = df_eval[df_eval[TARGET_TEMP_COL].notna()].copy()
                        # ----- Metrics per horizon -----
                        def _metric(col):
                            mask = df_eval_actual[[TARGET_TEMP_COL, col]].notna().all(axis=1)
                            if not mask.any():
                                return float("nan"), float("nan"), float("nan")
                            err = df_eval_actual.loc[mask, TARGET_TEMP_COL] - df_eval_actual.loc[mask, col]
                            mae_c = err.abs().mean()
                            rmse_c = (err ** 2).mean() ** 0.5
                            mape_c = (err.abs() / df_eval_actual.loc[mask, TARGET_TEMP_COL]).mean() * 100
                            return mae_c, rmse_c, mape_c

                        # ---- Compute metrics for **all** horizons up to HORIZON_DAYS ----
                        mae_by_h, rmse_by_h, mape_by_h = {}, {}, {}
                        for h in HORIZON_TUPLE:
                            col_name = f"pred_h{h}d"
                            if col_name in df_eval_actual.columns:
                                mae_v, rmse_v, mape_v = _metric(col_name)
                            else:
                                mae_v = rmse_v = mape_v = float("nan")
                            mae_by_h[h] = mae_v
                            rmse_by_h[h] = rmse_v
                            mape_by_h[h] = mape_v

                        # For backwardâ€compatibility, keep first horizon values
                        mae_h1 = mae_by_h.get(1, float("nan"))
                        rmse_h1 = rmse_by_h.get(1, float("nan"))
                        mape_h1 = mape_by_h.get(1, float("nan"))

                        mae = mae_h1
                        rmse = rmse_h1
                        mape = mape_h1
                        conf, acc = compute_overall_metrics(df_eval_actual)

                        # -------------- Feature Importance ----------------
                        def _compute_importance(model, feature_cols):
                            if isinstance(model, (MultiOutputRegressor, MultiLGBMRegressor)):
                                # average over outputs
                                imps = np.mean([
                                    getattr(est, "feature_importances_", np.zeros(len(feature_cols)))
                                    for est in model.estimators_
                                ], axis=0)
                            elif hasattr(model, "feature_importances_"):
                                imps = getattr(model, "feature_importances_")
                            elif hasattr(model, "coef_"):
                                imps = np.abs(getattr(model, "coef_"))
                            else:
                                imps = np.zeros(len(feature_cols))
                            return imps

                        feat_importances = _compute_importance(mdl, feature_cols_mdl)
                        fi_df = (
                            pd.DataFrame({"feature": feature_cols_mdl, "importance": feat_importances})
                            .sort_values("importance", ascending=False)
                            .reset_index(drop=True)
                        )

                        st.session_state["evaluations"][mdl_name] = {
                            "df_eval": df_eval,
                            "df_predplot_all": df_predplot_all,
                            "confidence": conf,
                            "accuracy": acc,
                            "rmse": rmse,
                            "mae": mae,
                            "mape": mape,
                            "mae_h1": mae_h1,
                            "mae_h2": mae_by_h.get(2, float("nan")),
                            "mae_h3": mae_by_h.get(3, float("nan")),
                            "rmse_h1": rmse_h1,
                            "rmse_h2": rmse_by_h.get(2, float("nan")),
                            "rmse_h3": rmse_by_h.get(3, float("nan")),
                            "feature_cols": feature_cols_mdl,
                            "feature_importance": fi_df,
                            "categories_map": categories_map,
                            "horizon": len(df_eval["forecast_day"].unique()) if "forecast_day" in df_eval.columns else 0,
                            "df_base": df,
                            "model_name": mdl_name,
                            "future_safe": is_future_safe_model(mdl_name),
                            # NEW debug matrices
                            "X_train": X_train_aligned,
                            "X_eval": X_eval_aligned,
                            "mae_by_h": mae_by_h,
                            "rmse_by_h": rmse_by_h,
                            "mape_by_h": mape_by_h,
                        }

                    # last evaluated model as active
                    if target_models:
                        st.session_state["active_model"] = target_models[0]

                    st.toast(f"Evaluation completed for {len(target_models)} model(s)")
                    st.sidebar.success("Model evaluation completed successfully.")

                    # If user requested Eval & Forecast, automatically create forecast for selected model
                    if eval_fc_pressed:
                        st.sidebar.write(_t("Generating forecastâ€¦"))
                        # Use the same target_models from evaluation for forecasting
                models_to_fc = target_models
                
                # Check if any models are available for forecasting
                if not models_to_fc:
                    st.warning(_t("No trained models found. Please train a model first before generating forecasts."))
                    st.info(_t("Go to 'Train / Retrain Model' section to train a new model."))
                    return
                
                for mdl in models_to_fc:
                    generate_and_store_forecast(mdl, horizon=HORIZON_DAYS)
                st.toast(f"Forecast generated for {len(models_to_fc)} model(s)")
                st.sidebar.success("Forecast generation completed successfully.")

    # Render evaluation view (chosen via dropdown instead of tabs)
    if st.session_state["evaluations"]:
        eval_keys = list(st.session_state["evaluations"].keys())
        active_model = st.session_state.get("active_model", eval_keys[0])

        chosen_model = st.selectbox(
            _t("Select evaluated model"),
            options=eval_keys,
            index=eval_keys.index(active_model) if active_model in eval_keys else 0,
        )
        # Persist selection so next rerun keeps the same model
        if chosen_model != st.session_state.get("active_model"):
            st.toast(f"Selected model: {chosen_model}")
        st.session_state["active_model"] = chosen_model

        # Inner tabs for the chosen model
        inner_tabs = st.tabs([_t("Evaluation"), _t("Forecast")])

        # --- Evaluation Tab ---
        with inner_tabs[0]:
            render_evaluation(chosen_model)

        # --- Forecast Tab ---
        with inner_tabs[1]:
            if chosen_model in st.session_state.get("forecasts", {}):
                render_forecast(chosen_model)
            else:
                st.info(_t("No forecast generated yet for this model."))
                if st.button(_t("Generate Forecast"), key=f"btn_gen_fc_main_{chosen_model}"):
                    st.toast("Generating forecast for selected model...")
                    with st.spinner(_t("Generating forecastâ€¦")):
                        if generate_and_store_forecast(chosen_model, horizon=HORIZON_DAYS):
                            st.success(_t("Forecast generated â€“ switch tabs to view."))

    # --------------------------------------------------
    # Leaderboard (full-width collapsible panel) -----------------------------------
    evals = st.session_state["evaluations"]

    st.markdown("---")
    with st.expander(_t('Model Leaderboard'), expanded=False):
        if not evals:
            st.write(_t("No evaluations yet."))
        else:
            data = []
            for name, d in evals.items():
                data.append(
                    {
                        "model": name,
                        "confidence": d.get("confidence", float("nan")),
                        "accuracy": d.get("accuracy", float("nan")),
                        "rmse": d.get("rmse", float("nan")),
                        "mae": d.get("mae", float("nan")),
                    }
                )
            df_leader = (
                pd.DataFrame(data)
                .sort_values(["confidence", "accuracy"], ascending=False)
                .reset_index(drop=True)
            )
            df_leader.insert(0, "rank", df_leader.index + 1)
            st.dataframe(df_leader, use_container_width=True)

    # Optionally still expose full log
    if st.session_state.get("debug_mode"):
        dbg_log = st.session_state.get("debug_msgs", [])
        if dbg_log:
            with st.expander(_t('Debug Log (full)'), expanded=False):
                st.code("\n".join(dbg_log), language="text")



# ================== NEW HELPER RENDER FUNCTIONS ==================

def render_evaluation(model_name: str):
    """Render the evaluation view for a given *model_name* inside its tab."""
    res = st.session_state["evaluations"][model_name]
    # Categories captured during initial evaluation (may be empty)
    categories_map = res.get("categories_map", {})
    df_eval = res["df_eval"]
    df_predplot_all = res["df_predplot_all"]

    # Ensure 'forecast_day' exists for downstream UI widgets
    if "forecast_day" not in df_eval.columns and "detection_time" in df_eval.columns:
        date_series = pd.to_datetime(df_eval["detection_time"]).dt.floor("D")
        unique_dates_sorted = sorted(date_series.unique())
        date2idx = {d: idx for idx, d in enumerate(unique_dates_sorted, start=1)}
        df_eval["forecast_day"] = date_series.map(date2idx)
        # Apply same mapping to the combined prediction frame if present
        if "detection_time" in df_predplot_all.columns:
            df_predplot_all["forecast_day"] = pd.to_datetime(df_predplot_all["detection_time"]).dt.floor("D").map(date2idx)

    # ---------------- Warehouse â†’ Silo cascading filters -----------------
    wh_col_candidates = [c for c in ["granary_id", "storepointName"] if c in df_eval.columns]
    silo_col_candidates = [c for c in ["heap_id", "storeName"] if c in df_eval.columns]

    wh_col = wh_col_candidates[0] if wh_col_candidates else None
    silo_col = silo_col_candidates[0] if silo_col_candidates else None

    # Apply global location filter (chosen in the sidebar)
    global_filters = st.session_state.get("global_filters", {})
    sel_wh = global_filters.get("wh", "All")
    sel_silo = global_filters.get("silo", "All")

    if wh_col:
        if sel_wh != "All":
            df_eval = df_eval[df_eval[wh_col] == sel_wh]
            df_predplot_all = df_predplot_all[df_predplot_all[wh_col] == sel_wh]

        if silo_col and sel_silo != "All":
            df_eval = df_eval[df_eval[silo_col] == sel_silo]
            df_predplot_all = df_predplot_all[df_predplot_all[silo_col] == sel_silo]

    # -------- Metrics (re-computed on filtered subset) ---------
    conf_val, acc_val = compute_overall_metrics(df_eval)
    if {TARGET_TEMP_COL, "predicted_temp"}.issubset(df_eval.columns) and not df_eval.empty:
        mae_val = (df_eval[TARGET_TEMP_COL] - df_eval["predicted_temp"]).abs().mean()
        rmse_val = ((df_eval[TARGET_TEMP_COL] - df_eval["predicted_temp"]) ** 2).mean() ** 0.5
        mape_val = ((df_eval[TARGET_TEMP_COL] - df_eval["predicted_temp"]).abs() / df_eval[TARGET_TEMP_COL]).mean() * 100
    else:
        rmse_val = mae_val = mape_val = float("nan")

    # Per-horizon metrics up to HORIZON_DAYS
    mae_by_h = res.get("mae_by_h", {})
    rmse_by_h = res.get("rmse_by_h", {})
    mape_by_h = res.get("mape_by_h", {})

    # ---- Overview row (conf / acc) -----------------------------------
    overview_cols = st.columns(2)
    with overview_cols[0]:
        st.metric(_t("Conf (%)"), "--" if pd.isna(conf_val) else f"{conf_val:.2f}")
    with overview_cols[1]:
        st.metric(_t("Acc (%)"), "--" if pd.isna(acc_val) else f"{acc_val:.2f}")

    # ---- MAE per horizon ---------------------------------------------
    st.markdown(f"#### {_t('MAE per horizon')}")
    cols_mae = st.columns(len(HORIZON_TUPLE))
    for idx, h in enumerate(HORIZON_TUPLE):
        v = mae_by_h.get(h, float("nan"))
        with cols_mae[idx]:
            st.metric(f"MAE h+{h}", "--" if pd.isna(v) else f"{v:.2f}")

    # ---- RMSE per horizon --------------------------------------------
    st.markdown(f"#### {_t('RMSE per horizon')}")
    cols_rmse = st.columns(len(HORIZON_TUPLE))
    for idx, h in enumerate(HORIZON_TUPLE):
        v = rmse_by_h.get(h, float("nan"))
        with cols_rmse[idx]:
            st.metric(f"RMSE h+{h}", "--" if pd.isna(v) else f"{v:.2f}")

    # ---- MAPE per horizon (optional) ----------------------------------
    st.markdown(f"#### {_t('MAPE per horizon')}")
    cols_mape = st.columns(len(HORIZON_TUPLE))
    for idx, h in enumerate(HORIZON_TUPLE):
        v = mape_by_h.get(h, float("nan"))
        with cols_mape[idx]:
            st.metric(f"MAPE h+{h}", "--" if pd.isna(v) else f"{v:.2f}")

    # ---------------- Explanation caption -----------------------------
    st.caption(
        _t(
            "Row-wise horizon metrics (above) average the error of each h-day-ahead prediction across all evaluation rows.\n"
            "For a real-world, bulletin-style view of performance, switch to the 'Anchor 7-day' tab, where metrics are computed by freezing predictions on an anchor day and comparing them with observations that occur h days later."
        )
    )

    st.markdown("---")

    tab_labels = [_t("Summary"), _t("Predictions"), _t("3D Grid"), _t("Time Series"), _t("Anchor 7-day"), _t("Uncertainty"), _t("Extremes"), _t("Debug")]
    summary_tab, pred_tab, grid_tab, ts_tab, anchor_tab, uncertainty_tab, extremes_tab, debug_tab = st.tabs(tab_labels)

    with summary_tab:
        if "predicted_temp" in df_eval.columns:
            st.subheader(_t("Forecast Summary (per day)"))
            st.dataframe(
                forecast_summary(df_eval),
                use_container_width=True,
                key=f"summary_{model_name}_{len(df_eval['forecast_day'].unique()) if 'forecast_day' in df_eval.columns else 0}",
            )

            def exceeds(row):
                thresh = GRAIN_ALERT_THRESHOLDS.get(row.get("grain_type"), ALERT_TEMP_THRESHOLD)
                return row["predicted_temp"] >= thresh

            if df_eval.apply(exceeds, axis=1).any():
                st.error(_t("High temperature forecast detected for at least one grain type â€“ monitor closely!"))
            else:
                st.success(_t("All predicted temperatures within safe limits for their grain types"))

            # ---------------- Top Features ----------------
            fi_df = res.get("feature_importance")
            if fi_df is not None and not fi_df.empty:
                st.markdown(f"### {_t('Top Predictive Features')}")
                st.dataframe(fi_df, use_container_width=True, key=f"feat_imp_{model_name}")

    with pred_tab:
        _st_dataframe_safe(df_predplot_all, key=f"pred_df_{model_name}_{len(df_eval['forecast_day'].unique()) if 'forecast_day' in df_eval.columns else 0}")

    with grid_tab:
        # Build list of unique dates present in evaluation subset only
        unique_dates = sorted(pd.to_datetime(df_eval["detection_time"], errors="coerce").dt.floor("D").unique())
        date_choice = st.selectbox(
            _t("Select date"),
            options=[d.strftime("%Y-%m-%d") for d in unique_dates],
            key=f"day_{model_name}_grid_{len(unique_dates)}",
        )
        sel_date = pd.to_datetime(date_choice)
        df_predplot = df_predplot_all[pd.to_datetime(df_predplot_all["detection_time"], errors="coerce").dt.floor("D") == sel_date]
        plot_3d_grid(
            df_predplot,
            key=f"grid_{model_name}_{date_choice}",
            color_by_delta=True,
        )

    with ts_tab:
        # ---------------- Horizon selector -----------------------
        horizon_opts = list(range(1, HORIZON_DAYS + 1))
        sel_horizon = st.selectbox(
            _t("Forecast horizon (h + ? days)"),
            options=horizon_opts,
            index=0,
            key=f"ts_horizon_{model_name}",
        )

        # Merge past (training+eval) with future predictions so the trend is continuous
        hist_df = res.get("df_predplot_all")
        if hist_df is None:
            hist_df = pd.DataFrame()
        combined_df = pd.concat([hist_df, df_eval], ignore_index=True, sort=False)
        plot_time_series(
            combined_df,
            key=f"time_{model_name}_{sel_horizon}_{len(df_eval['forecast_day'].unique()) if 'forecast_day' in df_eval.columns else 0}",
            horizon_day=sel_horizon,
        )

    # -------------- ANCHOR 7-DAY TAB -------------------
    with anchor_tab:
        st.subheader("7-Day Forecast from Anchor Day (forecast_day=1)")

        # Add explanatory caption once at top of tab
        st.caption(
            _t(
                "Anchor metrics emulate operational use: predictions are frozen on the selected anchor day (forecast_day = 1) "
                "and each horizon h is scored against the real temperature measured h days later.\n"
                "This gives the most realistic estimate of future-forecast performance."
            )
        )

        if "forecast_day" not in df_eval.columns:
            st.info("forecast_day column missing â€“ cannot compute anchor forecast.")
        else:
            # Build mapping from forecast_day -> calendar date (min date for that day)
            day_date_pairs = []
            for d_val in sorted(df_eval["forecast_day"].unique()):
                d_date = (
                    pd.to_datetime(df_eval[df_eval["forecast_day"] == d_val]["detection_time"])
                    .dt.floor("D")
                    .min()
                )
                if pd.notna(d_date):
                    day_date_pairs.append((d_date.strftime("%Y-%m-%d"), d_val))

            if not day_date_pairs:
                st.info("No anchor dates available.")
                return

            date_options = [p[0] for p in day_date_pairs]
            mapping_day = {p[0]: p[1] for p in day_date_pairs}

            sel_date_str = st.selectbox("Select anchor date", options=date_options, index=0, key=f"anchor_sel_{model_name}")
            sel_anchor = mapping_day[sel_date_str]

            anchor_rows = df_eval[df_eval["forecast_day"] == sel_anchor].copy()
            if anchor_rows.empty:
                st.info(f"No rows for selected anchor date {sel_date_str}.")
            else:
                # Determine anchor calendar date from selection
                anchor_date = pd.to_datetime(sel_date_str)

                records = []
                for h in HORIZON_TUPLE:
                    pred_col = f"pred_h{h}d"
                    target_date = anchor_date + pd.Timedelta(days=h)

                    # ---- Collect per-sensor prediction & actual ----
                    pred_subset = anchor_rows.copy()
                    if pred_col not in pred_subset.columns:
                        continue  # skip if horizon not available
                    pred_subset = pred_subset.assign(pred_val=pred_subset[pred_col])

                    act_subset = df_eval[pd.to_datetime(df_eval["detection_time"]).dt.floor("D") == target_date].copy()
                    act_subset = act_subset.assign(actual_val=act_subset[TARGET_TEMP_COL])

                    key_cols = [c for c in ["granary_id", "heap_id", "grid_x", "grid_y", "grid_z"] if c in pred_subset.columns and c in act_subset.columns]

                    merged = pred_subset[key_cols + ["pred_val"]].merge(
                        act_subset[key_cols + ["actual_val"]], on=key_cols, how="inner"
                    )

                    if merged.empty:
                        mae = max_err = float("nan")
                    else:
                        diffs = (merged["pred_val"] - merged["actual_val"]).abs()
                        mae = diffs.mean()
                        max_err = diffs.max()

                    pred_mean = merged["pred_val"].mean() if not merged.empty else float("nan")
                    actual_mean = merged["actual_val"].mean() if not merged.empty else float("nan")

                    records.append({
                        "horizon_day": h,
                        "forecast_date": target_date.strftime("%Y-%m-%d"),
                        "predicted_mean": round(pred_mean, 2) if pd.notna(pred_mean) else "--",
                        "actual_mean": round(actual_mean, 2) if pd.notna(actual_mean) else "--",
                        "MAE": round(mae, 2) if pd.notna(mae) else "--",
                        "max_abs_err": round(max_err, 2) if pd.notna(max_err) else "--",
                    })

                anchor_tbl = pd.DataFrame(records)
                st.dataframe(anchor_tbl, use_container_width=True, key=f"anchor_{model_name}")

                # ---- Plot predicted vs actual over 7-day horizon ----
                try:
                    import plotly.graph_objects as go
                    plot_df = anchor_tbl.replace("--", np.nan).dropna(subset=["predicted_mean", "actual_mean"]).copy()
                    plot_df["predicted_mean"] = pd.to_numeric(plot_df["predicted_mean"])
                    plot_df["actual_mean"] = pd.to_numeric(plot_df["actual_mean"])
                    if not plot_df.empty:
                        fig_anchor = go.Figure()
                        fig_anchor.add_trace(
                            go.Scatter(
                                x=plot_df["forecast_date"],
                                y=plot_df["predicted_mean"],
                                mode="lines+markers",
                                name="Predicted",
                            )
                        )
                        fig_anchor.add_trace(
                            go.Scatter(
                                x=plot_df["forecast_date"],
                                y=plot_df["actual_mean"],
                                mode="lines+markers",
                                name="Actual",
                            )
                        )
                        fig_anchor.update_layout(
                            title="Anchor-day 7-Day Forecast vs Actual",
                            xaxis_title="Date",
                            yaxis_title="Temperature (Â°C)",
                            xaxis=dict(tickformat="%Y-%m-%d"),
                        )
                        st.plotly_chart(fig_anchor, use_container_width=True, key=f"anchor_plot_{model_name}")
                except Exception as exc:
                    _d(f"Anchor plot error: {exc}")

                # ------------------------------------------------------
                # NEW: Sensor-level error tables per horizon (Î” > 0.5 Â°C)
                # ------------------------------------------------------
                THRESH_DIFF = 0.5  # degrees C
                st.markdown("---")
                st.markdown("### Sensor-level discrepancies (> 0.5 Â°C)")

                # Pre-compute yesterday date lookup once for performance
                full_df_dates = df_eval.copy()
                full_df_dates["_date"] = pd.to_datetime(full_df_dates["detection_time"]).dt.floor("D")

                for h in HORIZON_TUPLE:
                    target_date = anchor_date + pd.Timedelta(days=h)
                    pred_col = f"pred_h{h}d"
                    if pred_col not in anchor_rows.columns:
                        continue  # skip horizon not available

                    # Predicted rows (anchor rows already filtered by sensor keys)
                    preds_h = anchor_rows.copy()
                    preds_h = preds_h.assign(predicted_temp=preds_h[pred_col])

                    # Actual rows at target date
                    actual_h = df_eval[pd.to_datetime(df_eval["detection_time"]).dt.floor("D") == target_date].copy()
                    actual_h = actual_h.assign(actual_temp=actual_h[TARGET_TEMP_COL])

                    key_cols = [c for c in ["granary_id", "heap_id", "grid_x", "grid_y", "grid_z"] if c in preds_h.columns and c in actual_h.columns]

                    merged_h = preds_h[key_cols + ["predicted_temp"]].merge(
                        actual_h[key_cols + ["actual_temp", "detection_time"]], on=key_cols, how="inner"
                    )

                    if merged_h.empty:
                        st.info(f"h+{h}: No matching sensor readings available.")
                        continue

                    # Yesterday temp (target_date âˆ’ 1 day)
                    yest_date = target_date - pd.Timedelta(days=1)
                    yest_rows = full_df_dates[full_df_dates["_date"] == yest_date].copy()
                    yest_rows = yest_rows.assign(yesterday_temp=yest_rows[TARGET_TEMP_COL])

                    merged_h = merged_h.merge(
                        yest_rows[key_cols + ["yesterday_temp"]], on=key_cols, how="left"
                    )

                    merged_h["diff"] = (merged_h["predicted_temp"] - merged_h["actual_temp"]).abs()
                    merged_h = merged_h[merged_h["diff"] > THRESH_DIFF]

                    if merged_h.empty:
                        st.info(f"h+{h}: No sensor differences > {THRESH_DIFF} Â°C.")
                        continue

                    show_cols = key_cols + ["predicted_temp", "actual_temp", "yesterday_temp", "diff"]
                    st.markdown(f"#### h+{h}")
                    _st_dataframe_safe(merged_h[show_cols].sort_values("diff", ascending=False), key=f"anchor_sensor_h{h}_{model_name}")
                # End new block

        # ---- Overall MAE across ALL anchor dates (aggregated) ----
        mae_vals_global: list[float] = []
        for anchor_val in sorted(df_eval["forecast_day"].unique()):
            anchor_rows_all = df_eval[df_eval["forecast_day"] == anchor_val].copy()
            if anchor_rows_all.empty:
                continue
            anchor_date_all = pd.to_datetime(anchor_rows_all["detection_time"]).dt.floor("D").min()
            for h in HORIZON_TUPLE:
                pred_col = f"pred_h{h}d"
                if pred_col not in anchor_rows_all.columns:
                    continue
                pred_subset_all = anchor_rows_all.assign(pred_val=anchor_rows_all[pred_col])

                target_date_all = anchor_date_all + pd.Timedelta(days=h)
                act_subset_all = df_eval[pd.to_datetime(df_eval["detection_time"]).dt.floor("D") == target_date_all].copy()
                act_subset_all = act_subset_all.assign(actual_val=act_subset_all[TARGET_TEMP_COL])

                key_cols = [c for c in ["granary_id", "heap_id", "grid_x", "grid_y", "grid_z"] if c in pred_subset_all.columns and c in act_subset_all.columns]
                merged_all = pred_subset_all[key_cols + ["pred_val"]].merge(
                    act_subset_all[key_cols + ["actual_val"]], on=key_cols, how="inner"
                )

                if not merged_all.empty:
                    mae_val = (merged_all["pred_val"] - merged_all["actual_val"]).abs().mean()
                    if pd.notna(mae_val):
                        mae_vals_global.append(mae_val)

        if mae_vals_global:
            avg_mae_global = float(np.nanmean(mae_vals_global))
            max_mae_global = float(np.nanmax(mae_vals_global))
            st.markdown("---")
            st.markdown("### Aggregate MAE Metrics Across All Anchors")
            col_avg, col_max = st.columns(2)
            with col_avg:
                st.metric("Avg MAE (all anchors Ã— 7 days)", f"{avg_mae_global:.2f}")
            with col_max:
                st.metric("Max MAE (all anchors Ã— 7 days)", f"{max_mae_global:.2f}")

            # -------- Per-horizon aggregate MAE -----------------------
            mae_by_h: dict[int, list[float]] = {h: [] for h in HORIZON_TUPLE}
            maxerr_by_h: dict[int, list[float]] = {h: [] for h in HORIZON_TUPLE}

            for anchor_val in sorted(df_eval["forecast_day"].unique()):
                anchor_rows_all = df_eval[df_eval["forecast_day"] == anchor_val].copy()
                if anchor_rows_all.empty:
                    continue
                anchor_date_all = pd.to_datetime(anchor_rows_all["detection_time"]).dt.floor("D").min()

                for h in HORIZON_TUPLE:
                    pred_col = f"pred_h{h}d"
                    if pred_col not in anchor_rows_all.columns:
                        continue

                    pred_subset_all = anchor_rows_all.assign(pred_val=anchor_rows_all[pred_col])
                    target_date_all = anchor_date_all + pd.Timedelta(days=h)

                    act_subset_all = df_eval[
                        pd.to_datetime(df_eval["detection_time"]).dt.floor("D") == target_date_all
                    ].copy()
                    act_subset_all = act_subset_all.assign(actual_val=act_subset_all[TARGET_TEMP_COL])

                    key_cols = [
                        c
                        for c in ["granary_id", "heap_id", "grid_x", "grid_y", "grid_z"]
                        if c in pred_subset_all.columns and c in act_subset_all.columns
                    ]

                    merged_all = pred_subset_all[key_cols + ["pred_val"]].merge(
                        act_subset_all[key_cols + ["actual_val"]], on=key_cols, how="inner"
                    )

                    if not merged_all.empty:
                        diffs = (merged_all["pred_val"] - merged_all["actual_val"]).abs()
                        mae_val = diffs.mean()
                        max_val = diffs.max()
                        if pd.notna(mae_val):
                            mae_by_h[h].append(mae_val)
                        if pd.notna(max_val):
                            maxerr_by_h[h].append(max_val)

            # Display horizon-wise MAE metrics if any values present
            any_vals = any(len(v) > 0 for v in mae_by_h.values())
            if any_vals:
                st.markdown("### MAE by Forecast Horizon (All Anchors)")
                cols_h = st.columns(len(HORIZON_TUPLE))
                for idx, h in enumerate(HORIZON_TUPLE):
                    vals = mae_by_h[h]
                    mae_h = np.nan if not vals else float(np.nanmean(vals))
                    with cols_h[idx]:
                        lbl = f"h+{h} MAE"
                        st.metric(lbl, "--" if pd.isna(mae_h) else f"{mae_h:.2f}")

                # ---------- Max absolute error per horizon -------------
                cols_maxh = st.columns(len(HORIZON_TUPLE))
                for idx, h in enumerate(HORIZON_TUPLE):
                    vals = maxerr_by_h[h]
                    max_h = np.nan if not vals else float(np.nanmax(vals))
                    with cols_maxh[idx]:
                        lbl = f"h+{h} Max |Error|"
                        st.metric(lbl, "--" if pd.isna(max_h) else f"{max_h:.2f}")

    # ------------------ EXTREMES TAB ------------------
    with extremes_tab:
        st.subheader(_t("Daily Extremes (h+1)"))

        df_eval_actual = df_eval[df_eval[TARGET_TEMP_COL].notna()].copy()
        if df_eval_actual.empty or "pred_h1d" not in df_eval_actual.columns and "predicted_temp" not in df_eval_actual.columns:
            st.info(_t("No horizon-1 predictions available to compute extremes."))
        else:
            # Use column alias â€“ pred_h1d preferred but fall back to predicted_temp
            pred_col = "pred_h1d" if "pred_h1d" in df_eval_actual.columns else "predicted_temp"
            df_eval_actual["date"] = pd.to_datetime(df_eval_actual["detection_time"]).dt.date
            df_eval_actual["error"] = df_eval_actual[pred_col] - df_eval_actual[TARGET_TEMP_COL]

            rows = []
            for d, grp in df_eval_actual.groupby("date"):
                daily_avg_dev = grp["error"].abs().mean()
                # Over-prediction (max positive error)
                over_row = grp.loc[grp["error"].idxmax()]
                # Under-prediction (most negative error)
                under_row = grp.loc[grp["error"].idxmin()]

                for typ, r in [("Over", over_row), ("Under", under_row)]:
                    rows.append(
                        {
                            "date": d,
                            "type": typ,
                            "predicted": r[pred_col],
                            "actual": r[TARGET_TEMP_COL],
                            "error": r["error"],
                            "avg_daily_abs_error": daily_avg_dev,
                            "grid_x": r.get("grid_x"),
                            "grid_y": r.get("grid_y"),
                            "grid_z": r.get("grid_z"),
                        }
                    )

            extremes_df = pd.DataFrame(rows)
            # Sort by date then over/under
            extremes_df.sort_values(["date", "type"], inplace=True)
            _st_dataframe_safe(extremes_df, key=f"extremes_{model_name}_{len(rows)}")

            # -------------- Time-series plots ----------------
            if not extremes_df.empty:
                # Ensure date as datetime for proper plotting
                extremes_df["date"] = pd.to_datetime(extremes_df["date"])

                # ---- 1. Average daily absolute error plot ----
                import plotly.graph_objects as go
                daily_avg = (
                    extremes_df[["date", "avg_daily_abs_error"]]
                    .drop_duplicates(subset="date")
                    .sort_values("date")
                )
                fig_avg = go.Figure(
                    data=[
                        go.Scatter(
                            x=daily_avg["date"],
                            y=daily_avg["avg_daily_abs_error"],
                            mode="lines+markers",
                            name="Avg |Error|",
                        )
                    ]
                )
                fig_avg.update_layout(
                    title="Average Daily Absolute Error (h+1)",
                    xaxis_title="Date",
                    yaxis_title="Avg |Error| (Â°C)",
                    xaxis=dict(tickformat="%Y-%m-%d"),
                )
                st.plotly_chart(fig_avg, use_container_width=True, key=f"avg_err_plot_{model_name}")

                # Helper to plot over/under lines
                def _plot_pred_vs_actual(sub_df: pd.DataFrame, title: str, key: str):
                    import plotly.graph_objects as go
                    sub_df = sub_df.sort_values("date")
                    fig = go.Figure()
                    fig.add_trace(
                        go.Scatter(
                            x=sub_df["date"],
                            y=sub_df["actual"],
                            mode="lines+markers",
                            name="Actual",
                        )
                    )
                    fig.add_trace(
                        go.Scatter(
                            x=sub_df["date"],
                            y=sub_df["predicted"],
                            mode="lines+markers",
                            name="Predicted",
                        )
                    )
                    fig.update_layout(
                        title=title,
                        xaxis_title="Date",
                        yaxis_title="Temperature (Â°C)",
                        xaxis=dict(tickformat="%Y-%m-%d"),
                    )
                    st.plotly_chart(fig, use_container_width=True, key=key)

                # ---- 2. Over-prediction line plot ----
                over_df = extremes_df[extremes_df["type"] == "Over"]
                if not over_df.empty:
                    _plot_pred_vs_actual(over_df, "Over-Prediction (h+1)", f"over_plot_{model_name}")

                # ---- 3. Under-prediction line plot ----
                under_df = extremes_df[extremes_df["type"] == "Under"]
                if not under_df.empty:
                    _plot_pred_vs_actual(under_df, "Under-Prediction (h+1)", f"under_plot_{model_name}")

    # ------------------ UNCERTAINTY ANALYSIS TAB ------------------
    with uncertainty_tab:
        st.subheader("ðŸ”¬ Uncertainty Analysis & Prediction Confidence")
        
        # Load the model to check if it has uncertainty estimation capabilities
        try:
            mdl_result = load_trained_model(model_name)
            if isinstance(mdl_result, dict) and "error" in mdl_result:
                st.error(mdl_result["error"])
                return
            mdl = mdl_result
            
            # Conservative system status display

                
            # Uncertainty estimation status
            if hasattr(mdl, 'uncertainty_estimation') and mdl.uncertainty_estimation:
                st.success("**Uncertainty Estimation**: ACTIVE")
                
                # Try to get uncertainty intervals from the most recent prediction
                prediction_intervals = getattr(mdl, '_last_prediction_intervals', None)
                
                if prediction_intervals and isinstance(prediction_intervals, dict):
                    st.markdown("### ðŸ“ˆ Prediction Confidence Intervals")
                    
                    # Display uncertainty statistics
                    uncertainties = prediction_intervals.get('uncertainties')
                    if uncertainties is not None:
                        avg_uncertainty = np.mean(uncertainties)
                        max_uncertainty = np.max(uncertainties)
                        min_uncertainty = np.min(uncertainties)
                        
                        # Uncertainty metrics
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("Avg Uncertainty", f"{avg_uncertainty:.3f}Â°C")
                        with col2:
                            st.metric("Max Uncertainty", f"{max_uncertainty:.3f}Â°C")
                        with col3:
                            st.metric("Min Uncertainty", f"{min_uncertainty:.3f}Â°C")
                        with col4:
                            reliability = "High" if avg_uncertainty < 0.5 else "Medium" if avg_uncertainty < 1.0 else "Low"
                            st.metric("Reliability", reliability)
                        
                        # Uncertainty by horizon
                        if uncertainties.ndim > 1 and uncertainties.shape[1] > 1:
                            st.markdown("#### ðŸ“Š Uncertainty by Forecast Horizon")
                            
                            horizon_uncertainty = []
                            for h in range(min(uncertainties.shape[1], len(HORIZON_TUPLE))):
                                horizon_h = HORIZON_TUPLE[h]
                                unc_h = np.mean(uncertainties[:, h])
                                horizon_uncertainty.append({
                                    'Horizon': f'h+{horizon_h}',
                                    'Days Ahead': horizon_h,
                                    'Avg Uncertainty (Â°C)': round(unc_h, 3),
                                    'Confidence Level': "High" if unc_h < 0.5 else "Medium" if unc_h < 1.0 else "Low"
                                })
                            
                            uncertainty_df = pd.DataFrame(horizon_uncertainty)
                            st.dataframe(uncertainty_df, use_container_width=True, key=f"uncertainty_by_horizon_{model_name}")
                            
                            # Plot uncertainty by horizon
                            import plotly.graph_objects as go
                            fig_unc = go.Figure()
                            fig_unc.add_trace(go.Scatter(
                                x=[f"h+{h}" for h in HORIZON_TUPLE[:len(horizon_uncertainty)]],
                                y=[row['Avg Uncertainty (Â°C)'] for row in horizon_uncertainty],
                                mode='lines+markers',
                                name='Average Uncertainty',
                                line=dict(color='orange', width=3),
                                marker=dict(size=8)
                            ))
                            fig_unc.update_layout(
                                title="Prediction Uncertainty by Forecast Horizon",
                                xaxis_title="Forecast Horizon",
                                yaxis_title="Average Uncertainty (Â°C)",
                                template="plotly_white"
                            )
                            st.plotly_chart(fig_unc, use_container_width=True, key=f"uncertainty_plot_{model_name}")
                        
                        # Confidence intervals
                        st.markdown("#### ðŸŽ¯ Available Confidence Intervals")
                        
                        ci_info = []
                        for key in prediction_intervals.keys():
                            if key.startswith('lower_') or key.startswith('upper_'):
                                confidence_level = key.split('_')[1] + '%'
                                ci_type = 'Lower Bound' if key.startswith('lower_') else 'Upper Bound'
                                ci_info.append({
                                    'Confidence Level': confidence_level,
                                    'Type': ci_type,
                                    'Available': 'âœ…'
                                })
                        
                        if ci_info:
                            ci_df = pd.DataFrame(ci_info)
                            st.dataframe(ci_df, use_container_width=True, key=f"confidence_intervals_{model_name}")
                        
                        # Explanation
                        st.markdown("#### ðŸ’¡ How to Interpret Uncertainty")
                        st.markdown("""
                        - **Low uncertainty** (< 0.5Â°C): High confidence predictions
                        - **Medium uncertainty** (0.5-1.0Â°C): Moderate confidence  
                        - **High uncertainty** (> 1.0Â°C): Lower confidence, use with caution
                        - **68% CI**: ~2/3 of actual values should fall within this range
                        - **95% CI**: ~19/20 of actual values should fall within this range
                        """)
                        
                else:
                    st.info("ðŸ“Š Uncertainty intervals will be available after making predictions")
                    
            else:
                st.warning("ðŸ“Š **Uncertainty Estimation**: âŒ DISABLED")
                st.info("ðŸ’¡ Enable uncertainty estimation in model training to get confidence intervals")
                
        except Exception as e:
            st.error(f"Could not load model for uncertainty analysis: {e}")
            st.info("Please ensure the model is properly trained and saved.")

    # ------------------ DEBUG TAB ------------------
    with debug_tab:
        st.subheader(f"{_t('Feature Matrices (first 100 rows)')} (Training â€“ X_train)")
        x_train_dbg = res.get("X_train")
        if x_train_dbg is not None:
            st.write(_t("Training â€“ X_train"))
            _st_dataframe_safe(x_train_dbg, key=f"xtrain_{model_name}_{len(df_eval['forecast_day'].unique()) if 'forecast_day' in df_eval.columns else 0}")
        x_eval_dbg = res.get("X_eval")
        if x_eval_dbg is not None:
            st.write(_t("Evaluation â€“ X_eval"))
            _st_dataframe_safe(x_eval_dbg, key=f"xeval_{model_name}_{len(df_eval['forecast_day'].unique()) if 'forecast_day' in df_eval.columns else 0}")
        st.write(_t("Model Feature Columns (order)"))
        st.code(", ".join(res.get("feature_cols", [])))


def render_forecast(model_name: str):
    """Render the forecast view (if available) for *model_name*."""
    forecast_data = st.session_state.get("forecasts", {}).get(model_name)
    if not forecast_data:
        st.info(_t("No forecast generated for this model yet."))
        return

    # -------- Initial dataframe & warehouse/silo filters --------
    future_df = forecast_data["future_df"]

    # Apply global location filter (chosen in the sidebar)
    global_filters = st.session_state.get("global_filters", {})
    sel_wh_fc = global_filters.get("wh", "All")
    sel_silo_fc = global_filters.get("silo", "All")

    wh_col_candidates = [c for c in ["granary_id", "storepointName"] if c in future_df.columns]
    silo_col_candidates = [c for c in ["heap_id", "storeName"] if c in future_df.columns]

    wh_col = wh_col_candidates[0] if wh_col_candidates else None
    silo_col = silo_col_candidates[0] if silo_col_candidates else None

    if wh_col:
        if sel_wh_fc != "All":
            future_df = future_df[future_df[wh_col] == sel_wh_fc]

        if silo_col and sel_silo_fc != "All":
            future_df = future_df[future_df[silo_col] == sel_silo_fc]

    # Update forecast_data copy with filtered df for downstream plots
    df_plot_base = future_df.copy()

    # -------- Metrics forwarded from last evaluation (confidence etc.) ---------
    res_eval = st.session_state.get("evaluations", {}).get(model_name, {})
    conf_val = res_eval.get("confidence", float("nan"))
    acc_val = res_eval.get("accuracy", float("nan"))
    rmse_val = res_eval.get("rmse", float("nan"))
    mae_val = res_eval.get("mae", float("nan"))
    mape_val = res_eval.get("mape", float("nan"))

    metric_cols = st.columns(5)
    with metric_cols[0]:
        st.metric(_t("Confidence (%)"), "--" if pd.isna(conf_val) else f"{conf_val:.3f}")
    with metric_cols[1]:
        st.metric(_t("Accuracy (%)"), "--" if pd.isna(acc_val) else f"{acc_val:.3f}")
    with metric_cols[2]:
        st.metric(_t("RMSE"), "--" if pd.isna(rmse_val) else f"{rmse_val:.3f}")
    with metric_cols[3]:
        st.metric(_t("MAE"), "--" if pd.isna(mae_val) else f"{mae_val:.3f}")
    with metric_cols[4]:
        st.metric(_t("MAPE (%)"), "--" if pd.isna(mape_val) else f"{mape_val:.3f}")

    st.markdown("---")

    # Tabs similar to evaluation (+Debug)
    summary_tab, pred_tab, grid_tab, ts_tab, extremes_tab, debug_tab = st.tabs([_t("Summary"), _t("Predictions"), _t("3D Grid"), _t("Time Series"), _t("Extremes"), _t("Debug")])

    with summary_tab:
        # Only predicted statistics available
        grp = (
            future_df.groupby("forecast_day")
            .agg(pred_mean=("predicted_temp", "mean"), pred_max=("predicted_temp", "max"), pred_min=("predicted_temp", "min"))
            .reset_index()
        )
        st.subheader(_t("Forecast Summary (predicted)"))
        _st_dataframe_safe(grp, key=f"forecast_summary_{model_name}_{len(future_df['forecast_day'].unique()) if 'forecast_day' in future_df.columns else 0}")

        # Offer download of raw predictions CSV if available
        csv_path = forecast_data.get("csv_path")
        if csv_path and pathlib.Path(csv_path).exists():
            with open(csv_path, "rb") as _f:
                st.download_button(
                    label="Download predictions CSV",
                    data=_f.read(),
                    file_name=pathlib.Path(csv_path).name,
                    mime="text/csv",
                )

    with pred_tab:
        _st_dataframe_safe(future_df, key=f"future_pred_df_{model_name}_{len(future_df['forecast_day'].unique()) if 'forecast_day' in future_df.columns else 0}")

    with grid_tab:
        day_choice = st.selectbox(
            "Select day",
            options=list(range(1, len(future_df['forecast_day'].unique()) + 1)),
            key=f"future_day_{model_name}_{len(future_df['forecast_day'].unique()) if 'forecast_day' in future_df.columns else 0}",
        )
        day_df = future_df[future_df.get("forecast_day", 1) == day_choice]
        plot_3d_grid(day_df, key=f"future_grid_{model_name}_{len(future_df['forecast_day'].unique()) if 'forecast_day' in future_df.columns else 0}")

    with ts_tab:
        # Merge past (training+eval) with future predictions so the trend is continuous
        hist_df = res_eval.get("df_predplot_all")
        if hist_df is None:
            hist_df = pd.DataFrame()
        combined_df = pd.concat([hist_df, future_df], ignore_index=True, sort=False)
        plot_time_series(
            combined_df,
            key=f"future_ts_{model_name}_{len(future_df['forecast_day'].unique()) if 'forecast_day' in future_df.columns else 0}",
        )

    # ------------------ EXTREMES TAB ------------------
    with extremes_tab:
        st.subheader(_t("Daily Predicted Extremes"))

        if future_df.empty or "predicted_temp" not in future_df.columns:
            st.info(_t("No predictions found to compute extremes."))
        else:
            tmp = future_df.copy()
            tmp["date"] = pd.to_datetime(tmp["detection_time"]).dt.date

            rows = []
            for d, grp in tmp.groupby("date"):
                # Highest and lowest predicted temperature for the day
                max_row = grp.loc[grp["predicted_temp"].idxmax()]
                min_row = grp.loc[grp["predicted_temp"].idxmin()]

                for typ, r in [("Max", max_row), ("Min", min_row)]:
                    rows.append(
                        {
                            "date": d,
                            "type": typ,
                            "predicted": r["predicted_temp"],
                            "grid_x": r.get("grid_x"),
                            "grid_y": r.get("grid_y"),
                            "grid_z": r.get("grid_z"),
                        }
                    )

            extreme_pred_df = pd.DataFrame(rows)
            if extreme_pred_df.empty:
                st.info(_t("No predictions found to compute extremes."))
            else:
                extreme_pred_df.sort_values(["date", "type"], inplace=True)
                _st_dataframe_safe(extreme_pred_df, key=f"forecast_extremes_{model_name}_{len(rows)}")

    # ------------------ DEBUG TAB ------------------
    with debug_tab:
        st.subheader(f"{_t('Future Feature Matrix (first 100 rows)')} (Training â€“ X_train)")
        x_future_dbg = st.session_state.get("forecasts", {}).get(model_name, {}).get("X_future")
        if x_future_dbg is not None:
            st.dataframe(x_future_dbg.head(100), use_container_width=True)
            # Compare to evaluation matrix if available
            eval_res = st.session_state["evaluations"].get(model_name, {})
            x_eval_dbg = eval_res.get("X_eval")
            if x_eval_dbg is not None:
                delta = (x_eval_dbg.mean() - x_future_dbg.mean()).abs().sort_values(ascending=False)
                st.subheader(_t("|Mean(X_eval) âˆ’ Mean(X_future)| (Top 20)"))
                st.dataframe(delta.head(20).to_frame(name="abs_diff"), use_container_width=True)
        else:
            st.info(_t("X_future matrix not available yet."))

# --------------------------------------------------
# Helper to create & store forecast
def generate_and_store_forecast(model_name: str, horizon: int) -> bool:
    """Generate future_df for *model_name* and store in session_state['forecasts'].
    Returns True if successful, False otherwise."""
    try:
        res_eval = st.session_state.get("evaluations", {}).get(model_name)
        if res_eval is None:
            st.error(_t("Please evaluate the model first."))
            _d(f"[ERROR] No evaluation found for model: {model_name}")
            return False

        base_df = res_eval.get("df_base")
        categories_map = res_eval.get("categories_map", {})
        mdl_result = load_trained_model(model_name)
        if isinstance(mdl_result, dict) and "error" in mdl_result:
            st.error(mdl_result["error"])
            _d(f"[ERROR] Model load failed for {model_name}: {mdl_result['error']}")
            return False
        mdl = mdl_result

        if not isinstance(base_df, pd.DataFrame):
            st.error(_t("Unable to access base data for forecasting."))
            _d(f"[ERROR] base_df is not a DataFrame for model: {model_name}")
            return False
        _d(f"[EXPLORE] base_df shape: {base_df.shape}, columns: {list(base_df.columns)}")
        _d(f"[EXPLORE] categories_map: {categories_map}")
        _d(f"[EXPLORE] Model type: {type(mdl)}")
    except Exception as exc:
        st.error(_t(f"Critical error during forecast setup: {exc}"))
        _d(f"[CRITICAL] Exception in setup: {exc}")
        import traceback
        _d(traceback.format_exc())
        return False

    # Special handling if the model is *direct* multi-output and the requested
    # horizon fits within the model's native multi-output dimensions.
    try:
        if isinstance(mdl, (MultiOutputRegressor, MultiLGBMRegressor)) and horizon <= HORIZON_DAYS:
        # 1. Take **last known row** per physical sensor as input snapshot
            sensors_key = [c for c in [
                "granary_id", "heap_id", "grid_x", "grid_y", "grid_z"
            ] if c in base_df.columns]
            _d(f"[EXPLORE] sensors_key: {sensors_key}")
            last_rows = (
                base_df.sort_values("detection_time")
                .groupby(sensors_key, dropna=False)
                .tail(1)
                .copy()
            )
            _d(f"[EXPLORE] last_rows shape: {last_rows.shape}")
            # Prepare design matrix
            X_snap, _ = features.select_feature_target_multi(
                last_rows, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE, allow_na=True
            )
            _d(f"[EXPLORE] X_snap shape: {X_snap.shape}")
            model_feats = get_feature_cols(mdl, X_snap)
            _d(f"[EXPLORE] model_feats: {model_feats}")
            X_snap_aligned = X_snap.reindex(columns=model_feats, fill_value=0)
            _d(f"[EXPLORE] X_snap_aligned shape: {X_snap_aligned.shape}")
            preds_mat = model_utils.predict(mdl, X_snap_aligned)  # shape (n, 3)
            _d(f"[EXPLORE] preds_mat shape: {getattr(preds_mat, 'shape', None)}")
        
        # Log uncertainty estimation during forecasting
        if isinstance(mdl, MultiLGBMRegressor) and hasattr(mdl, 'uncertainty_estimation'):
            if mdl.uncertainty_estimation:
                _d(f"[FORECAST-UNCERTAINTY] Applied uncertainty estimation during forecasting (n_bootstrap={getattr(mdl, 'n_bootstrap_samples', 50)})")
                st.toast(f"ðŸ”® Forecast with uncertainty: {getattr(mdl, 'n_bootstrap_samples', 50)} bootstrap samples", icon="ðŸ”®")
                

            else:
                _d(f"[FORECAST-UNCERTAINTY] No uncertainty estimation - point forecast only")
                st.toast("âš ï¸ Forecast without uncertainty - no confidence intervals", icon="âš ï¸")

            n_out = preds_mat.shape[1] if getattr(preds_mat, "ndim", 1) == 2 else 1
            # Build future frames for 1, 2, 3-day horizons ------------------
            all_future_frames: list[pd.DataFrame] = []
            last_dt = pd.to_datetime(last_rows["detection_time"]).max()
            for h in range(1, horizon + 1):
                day_frame = last_rows.copy()
                day_frame["detection_time"] = last_dt + timedelta(days=h)
                day_frame["forecast_day"] = h
                idx = min(h - 1, n_out - 1)  # fallback to last available output
                if getattr(preds_mat, "ndim", 1) == 2:
                    pred_val = preds_mat[:, idx]
                else:
                    pred_val = preds_mat  # 1-D: same value for all horizons
                day_frame["predicted_temp"] = pred_val
                day_frame["temperature_grain"] = pred_val
                day_frame[TARGET_TEMP_COL] = pred_val
                day_frame["is_forecast"] = True
                all_future_frames.append(day_frame)
            future_df = pd.concat(all_future_frames, ignore_index=True)
            # Clear actual temperature values for forecast rows to avoid confusion in plots
            future_df.loc[future_df["is_forecast"], TARGET_TEMP_COL] = np.nan
            # Assign debug matrix for consistency with fallback path
            X_day_aligned = X_snap_aligned.copy()
        else:
            # Fallback â€“ original recursive loop (supports arbitrary horizons)
            hist_df = base_df.copy()
            all_future_frames: list[pd.DataFrame] = []
            for d in range(1, horizon + 1):
                # Generate placeholder rows for ONE day ahead
                day_df = make_future(hist_df, horizon_days=1)
                day_df = _inject_future_lag(day_df, hist_df)
                day_df["forecast_day"] = d
                # Extra features (lags, rolling) before encoding
                day_df = features.add_time_since_last_measurement(day_df)
                day_df = features.add_multi_lag_parallel(day_df, lags=(1,2,3,4,5,6,7,14,30))
                day_df = features.add_rolling_stats_parallel(day_df, window_days=7)
                # Apply categories levels
                for col, cats in categories_map.items():
                    if col in day_df.columns:
                        day_df[col] = pd.Categorical(day_df[col], categories=cats)
                X_day, _ = features.select_feature_target_multi(
                    day_df, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE, allow_na=True
                )
                _d(f"[EXPLORE] Day {d} X_day shape: {X_day.shape}")
                model_feats = get_feature_cols(mdl, X_day)
                _d(f"[EXPLORE] Day {d} model_feats: {model_feats}")
                X_day_aligned = X_day.reindex(columns=model_feats, fill_value=0)
                _d(f"[EXPLORE] Day {d} X_day_aligned shape: {X_day_aligned.shape}")
                preds = model_utils.predict(mdl, X_day_aligned)
                _d(f"[EXPLORE] Day {d} preds shape: {getattr(preds, 'shape', None)}")
                # Log uncertainty for recursive forecasting (if multi-output)
                if isinstance(mdl, MultiLGBMRegressor) and hasattr(mdl, 'uncertainty_estimation') and hasattr(preds, 'ndim') and preds.ndim == 2:
                    if mdl.uncertainty_estimation:
                        _d(f"[RECURSIVE-UNCERTAINTY] Day {d}: Uncertainty estimation applied to {preds.shape[1]} horizons")
                    else:
                        _d(f"[RECURSIVE-UNCERTAINTY] Day {d}: No uncertainty estimation - point predictions only")
                if hasattr(preds, 'ndim') and preds.ndim == 2:
                    preds_step = preds[:, 0]
                else:
                    preds_step = preds
                day_df["predicted_temp"] = preds_step
                day_df["temperature_grain"] = preds_step  # feed back as history for next lag
                day_df[TARGET_TEMP_COL] = preds_step
                day_df["is_forecast"] = True
                hist_df = pd.concat([hist_df, day_df], ignore_index=True, sort=False)
                all_future_frames.append(day_df)
            future_df = pd.concat(all_future_frames, ignore_index=True)
            # Clear actual temperature values for forecast rows to avoid confusion in plots
            future_df.loc[future_df["is_forecast"], TARGET_TEMP_COL] = np.nan
        st.session_state.setdefault("forecasts", {})[model_name] = {
            "future_df": future_df,
            "future_horizon": horizon,
            "X_future": X_day_aligned,  # last horizon step matrix for debug
        }
        _d(f"[FORECAST] Stored forecast â€“ rows={len(future_df)}")
    except Exception as exc:
        st.error(_t(f"Error during forecast generation: {exc}"))
        _d(f"[ERROR] Exception in forecast generation: {exc}")
        import traceback
        _d(traceback.format_exc())
        return False

    # ---------------- Persist predictions to CSV -----------------
    try:
        # Keep only essential columns for the user-facing CSV
        core_cols = [
            c for c in [
                "granary_id",
                "heap_id",
                "grid_x",
                "grid_y",
                "grid_z",
                "detection_time",
                "forecast_day",
                "predicted_temp",
            ]
            if c in future_df.columns
        ]
        out_df = future_df[core_cols].copy()
        _d(f"[EXPLORE] out_df shape: {out_df.shape}, columns: {list(out_df.columns)}")
        # Ensure output directory exists
        out_dir = pathlib.Path("data/forecasts")
        out_dir.mkdir(parents=True, exist_ok=True)
        forecast_name = f"{pathlib.Path(model_name).stem}_forecast_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        forecast_path = out_dir / forecast_name
        # Save as Parquet with snappy compression (60-80% smaller, 10x faster)
        from granarypredict.ingestion import save_granary_data
        parquet_path = save_granary_data(
            df=out_df,
            filepath=forecast_path,
            format='parquet',
            compression='snappy'
        )
        # Store path for UI download
        st.session_state["forecasts"][model_name]["parquet_path"] = str(parquet_path)
        _d(f"[FORECAST] Parquet written to {parquet_path}")
    except Exception as exc:
        st.error(_t(f"Error writing forecast CSV: {exc}"))
        _d(f"[ERROR] Could not write forecast CSV: {exc}")
        import traceback
        _d(traceback.format_exc())
        return False

    return True


# ---------------- Utility to extract feature column order from a model -----------------
def get_feature_cols(model, X_fallback: pd.DataFrame) -> list[str]:
    """Return the exact feature columns the *model* expects.

    1. scikit-learn 1.0+ estimators expose ``feature_names_in_``.
    2. LightGBM exposes ``feature_name_``.
    3. Fallback: use the columns of *X_fallback* (already aligned for current dataset).
    """
    if hasattr(model, "feature_names_in_"):
        return list(getattr(model, "feature_names_in_"))
    if hasattr(model, "feature_name_"):
        return list(getattr(model, "feature_name_"))
    return list(X_fallback.columns)


# ----------------- Helper for future lag injection -----------------
def _inject_future_lag(future_df: pd.DataFrame, history_df: pd.DataFrame) -> pd.DataFrame:
    """Populate lag_temp_1d in *future_df* using the last known
    temperature_grain for each sensor from *history_df*.  Assumes both frames
    contain grid_x/y/z columns.
    """
    if {"grid_x", "grid_y", "grid_z", "temperature_grain"}.issubset(history_df.columns):
        last_vals = (
            history_df.sort_values("detection_time")
            .dropna(subset=["temperature_grain"])
            .groupby(["grid_x", "grid_y", "grid_z"])["temperature_grain"]
            .last()
        )
        idx = future_df.set_index(["grid_x", "grid_y", "grid_z"]).index
        future_df["lag_temp_1d"] = [last_vals.get(key, np.nan) for key in idx]
    return future_df


# ---------------------------------------------------------------------
# Common dataframe preprocessing (clean â†’ fill â†’ features â†’ lag â†’ sort)
# ---------------------------------------------------------------------

def _preprocess_df(df: pd.DataFrame) -> pd.DataFrame:
    """Run the full data-prep pipeline exactly once."""
    if df.empty:
        return df
    
    # Apply column standardization first (convert batch->detection_time, temp->temperature_grain, etc.)
    _d("Starting column standardization...")
    from granarypredict.ingestion import standardize_granary_csv
    df = standardize_granary_csv(df)
    _d(f"Column standardization complete: {list(df.columns)}")
    
    _d("Starting basic_cleanâ€¦")
    before_cols = list(df.columns)
    df = cleaning.basic_clean(df)
    _d(f"basic_clean: cols before={len(before_cols)} after={len(df.columns)} rows={len(df)}")
    # Use Polars for performance-critical preprocessing if available
    try:
        from granarypredict.polars_adapter import to_polars, to_pandas, PolarsFeatures
        import polars as pl
        # Convert to Polars if not already
        df_pl = to_polars(df)
        # Example: time features, lags, rolling stats (customize as needed)
        df_pl = PolarsFeatures.create_time_features_polars(df_pl)
        df_pl = PolarsFeatures.add_lags_polars(df_pl)
        df_pl = PolarsFeatures.add_rolling_stats_polars(df_pl)
        # Convert back to Pandas for ML compatibility
        df = to_pandas(df_pl)
    except Exception as e:
        import logging
        logging.getLogger("Dashboard").warning(f"Polars preprocessing failed, using Pandas: {e}")
        # Fallback: original Pandas-based preprocessing (implement as needed)
        # ...existing pandas preprocessing code...
        pass
    return df
    # -------------------------------------------------------------
    # 1ï¸âƒ£ Insert missing calendar-day rows first
    # -------------------------------------------------------------
    df = insert_calendar_gaps(df)
    _d("insert_calendar_gaps: added rows for missing dates")

    # -------------------------------------------------------------
    # 2ï¸âƒ£ Interpolate numeric columns per sensor across the now-complete
    #    timeline so gap rows take the average of surrounding real values.
    # -------------------------------------------------------------
    df = interpolate_sensor_numeric(df)
    _d("interpolate_sensor_numeric: linear interpolation applied per sensor")

    # -------------------------------------------------------------
    # NEW â€“ Subsample to one record every 4 hours per sensor to
    #       mitigate over-representation of densely sampled probes.


    # Subsampling per sensor removed for this pipeline version

    # -------------------------------------------------------------
    # 3ï¸âƒ£ Final fill_missing to tidy up any residual NaNs (categoricals etc.)
    # -------------------------------------------------------------
    na_before = df.isna().sum().sum()
    df = cleaning.fill_missing(df)
    na_after = df.isna().sum().sum()
    _d(f"fill_missing (final): total NaNs before={na_before} after={na_after}")

    df = features.create_time_features(df)
    _d("create_time_features: added year/month/day/hour cols")
    df = features.create_spatial_features(df)
    _d("create_spatial_features: removed grid_index if present")
    # Add time-since-last-measurement features for data quality assessment
    df = features.add_time_since_last_measurement(df)
    _d("add_time_since_last_measurement: added data quality temporal features")
    # lag features will be created inside add_multi_lag (includes 1-day)

    # -------------------------------------------------------------
    # 4ï¸âƒ£ Extra temperature features (multi-lag, rolling stats, delta) - PARALLEL VERSION
    # -------------------------------------------------------------
    df = features.add_multi_lag_parallel(df, lags=(1,2,3,4,5,6,7,14,30))
    df = features.add_rolling_stats_parallel(df, window_days=7)
    _d("add_multi_lag_parallel & add_rolling_stats_parallel: extra features added with multiprocessing")
    
    # -------------------------------------------------------------
    # 4.5ï¸âƒ£ Lean directional features for temperature movement prediction
    # -------------------------------------------------------------
    df = features.add_directional_features_lean(df)
    _d("add_directional_features_lean: 6 directional features added for trend prediction")
    
    # -------------------------------------------------------------
    # 4.6ï¸âƒ£ Stability features for conservative temperature prediction - PARALLEL VERSION
    # -------------------------------------------------------------
    df = features.add_stability_features_parallel(df)
    _d("add_stability_features_parallel: 8 stability features added for conservative predictions with multiprocessing")
    
    # -------------------------------------------------------------
    # 4.7ï¸âƒ£ Horizon-specific directional features for multi-model accuracy
    # -------------------------------------------------------------
    df = features.add_horizon_specific_directional_features(df, max_horizon=HORIZON_DAYS)
    _d(f"add_horizon_specific_directional_features: Enhanced directional features for {HORIZON_DAYS}-day forecasting added")

    # -------------------------------------------------------------
    # 5ï¸âƒ£ Multi-horizon targets (1â€“3 days ahead)
    # -------------------------------------------------------------
    df = features.add_multi_horizon_targets(df, horizons=HORIZON_TUPLE)  # NEW
    _d("add_multi_horizon_targets: future target columns added")

    # Ensure group identifiers available for downstream splitting/evaluation
    df = assign_group_id(df)
    _d("assign_group_id: _group_id column added to dataframe")
    df = comprehensive_sort(df)
    _d("comprehensive_sort: dataframe sorted by granary/heap/grid/date")

    return df


# ---------------------------------------------------------------------
# Helper: insert rows for missing calendar dates per sensor
# ---------------------------------------------------------------------




# ---------------------------------------------------------------------
# Helper to fetch raw (possibly organised) dataframe
# ---------------------------------------------------------------------

def _get_active_df(uploaded_file):
    """Return the raw dataframe â€“ organised slice concat if available."""
    if st.session_state.get("organized_df") is not None:
        return st.session_state["organized_df"].copy()
    return load_uploaded_file(uploaded_file)


# ---------------------------------------------------------------------
# Helper to fetch whichever DataFrame (raw/organised) is active & processed
# ---------------------------------------------------------------------

def _preprocess_cached(df: pd.DataFrame) -> pd.DataFrame:
    """Wrapper around the heavy preprocessing pipeline (no Streamlit caching â€“ we persist processed CSVs instead)."""
    _d("_preprocess_cached: running full pipeline with parellel processing (no Streamlit cache)")
    return _preprocess_df(df)

def _get_preprocessed_df(uploaded_file):
    """Return a fully-preprocessed dataframe (cached in session)."""
    # --------------------------------------------------------
    # 0ï¸âƒ£ Fast-path: if the uploaded file is already a processed
    #    CSV (name ends with _processed.csv or resides in data/processed),
    #    simply load and return it.
    # --------------------------------------------------------
    if _looks_processed(uploaded_file):
        try:
            _d("âœ… Detected preprocessed file â€“ loading directly, skipping heavy pipeline")
            if isinstance(uploaded_file, (str, pathlib.Path)):
                file_path = pathlib.Path(uploaded_file)
                if file_path.suffix.lower() == '.parquet':
                    df_fast = pd.read_parquet(uploaded_file)
                else:
                    df_fast = pd.read_csv(uploaded_file, encoding="utf-8")
            else:
                # Handle StreamlitUploadedFile
                if uploaded_file.name.lower().endswith('.parquet'):
                    df_fast = pd.read_parquet(uploaded_file)
                else:
                    df_fast = pd.read_csv(uploaded_file, encoding="utf-8")
            
            # Check if standardization is needed (for old processed files)
            if 'detection_time' not in df_fast.columns:
                _d("âš ï¸ Preprocessed file missing standardized columns, applying standardization...")
                from granarypredict.ingestion import standardize_granary_csv
                df_fast = standardize_granary_csv(df_fast)
                _d(f"Applied standardization to processed file: {list(df_fast.columns)[:5]}...")
            
            st.session_state["processed_df"] = df_fast.copy()
            return df_fast
        except Exception as exc:
            _d(f"Could not load preprocessed file fast-path: {exc}; falling back to pipeline")

    raw_df = _get_active_df(uploaded_file)

    # Use cached preprocessing to avoid repeating heavy work across reruns
    proc = _preprocess_cached(raw_df)
    _d("ðŸ”„ Received dataframe from _preprocess_cached (may be cache hit or miss)")

    # --------------------------------------------------------
    # Persist a processed CSV alongside others for future fast-path
    # --------------------------------------------------------
    try:
        if hasattr(uploaded_file, "name"):
            orig_name = pathlib.Path(uploaded_file.name).stem
        else:
            orig_name = pathlib.Path(uploaded_file).stem if isinstance(uploaded_file, (str, pathlib.Path)) else "uploaded"

        processed_dir = pathlib.Path("data/preloaded")
        processed_dir.mkdir(parents=True, exist_ok=True)
        out_parquet = processed_dir / f"{orig_name}_processed.parquet"
        if not out_parquet.exists():
            from granarypredict.ingestion import save_granary_data
            save_granary_data(proc, out_parquet, format="parquet", compression="snappy")
            _d(f"ðŸ’¾ Saved processed Parquet to {out_parquet}")
    except Exception as exc:
        _d(f"Could not persist processed CSV: {exc}")

    st.session_state["processed_df"] = proc.copy()
    return proc


# ---------------------------------------------------------------------
# Helper to safely display DataFrames in Streamlit (Categoryâ†’str)
# ---------------------------------------------------------------------

def _st_dataframe_safe(df: pd.DataFrame, key: str | None = None):
    """Wrapper around st.dataframe that converts category columns to string
    to avoid pyarrow ArrowInvalid errors when categories mix numeric & text.
    """
    df_disp = df.copy()
    for col in df_disp.select_dtypes(include=["category"]).columns:
        df_disp[col] = df_disp[col].astype(str)
    st.dataframe(df_disp, use_container_width=True, key=key)


# ---------------------------------------------------------------------
# Helper to guess if an uploaded file is already processed
# ---------------------------------------------------------------------

def _looks_processed(upload):
    """Return True if *upload* path or name suggests preprocessed dataset."""
    if isinstance(upload, (str, pathlib.Path)):
        p = pathlib.Path(upload)
        if "data/processed" in p.as_posix() or p.name.endswith("_processed.csv") or p.name.endswith("_processed.parquet"):
            return True
    elif hasattr(upload, "name"):
        name = upload.name
        if name.endswith("_processed.csv") or name.endswith("_processed.parquet"):
            return True
    return False


# ---------------------------------------------------------------------
# Helper: numeric interpolation per sensor across calendar-completed frame
# ---------------------------------------------------------------------


from granarypredict.cleaning_helpers import (
    insert_calendar_gaps,
    interpolate_sensor_numeric,
)


# -------------------------------------------------------------------
# NEW â€“ fixedâ€window split (last *n_days* for training)  Jun-2025
# -------------------------------------------------------------------


def split_train_last_n_days(df: pd.DataFrame, n_days: int = 30):
    """Return (df_train, df_eval) where training data is restricted to the
    most recent *n_days* of records (by unique date).  All earlier data
    becomes the evaluation set.

    Parameters
    ----------
    df : pd.DataFrame
        Full dataset ordered arbitrarily.
    n_days : int, default 30
        Number of unique dates to keep for training.
    """
    if df.empty or "detection_time" not in df.columns:
        return df, pd.DataFrame()

    df = df.copy()
    df["_date"] = pd.to_datetime(df["detection_time"]).dt.date

    unique_dates = sorted(df["_date"].dropna().unique())
    if not unique_dates:
        return df, pd.DataFrame()

    # Latest *n_days* are reserved for evaluation; all earlier data for training
    cutoff_dates = unique_dates[-n_days:]

    df_eval = df[df["_date"].isin(cutoff_dates)].copy()
    df_train = df[~df["_date"].isin(cutoff_dates)].copy()

    # Assign forecast_day within evaluation set: 1 = oldest day in eval, â€¦
    date_to_idx = {date: idx for idx, date in enumerate(sorted(cutoff_dates), start=1)}
    df_eval["forecast_day"] = df_eval["_date"].map(date_to_idx).astype(int)

    df_train.drop(columns=["_date"], inplace=True)
    df_eval.drop(columns=["_date"], inplace=True)
    return df_train, df_eval


# ---------------- NEW â€“ evaluation sub-sampler --------------------

def _subsample_every_k_days(df: pd.DataFrame, k: int = 3, *, date_col: str = "detection_time") -> pd.DataFrame:
    """Return *df* where only one row every *k* days (by *date_col*) is kept.

    This reduces overlap between successive multi-horizon predictions in the
    evaluation split.  Rows belonging to dates that are not selected are
    dropped entirely.  If *date_col* is missing or the frame is empty the
    input is returned unchanged.
    """
    if df.empty or date_col not in df.columns:
        return df

    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    df["_date_only"] = df[date_col].dt.date

    unique_dates = sorted(df["_date_only"].unique())
    if not unique_dates:
        return df.drop(columns=["_date_only"], errors="ignore")

    keep_dates = set(unique_dates[::k])  # every k-th day starting from first
    df = df[df["_date_only"].isin(keep_dates)].copy()
    df.drop(columns=["_date_only"], inplace=True)
    return df


if __name__ == "__main__":
    main() 