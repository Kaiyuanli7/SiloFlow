# SiloFlow Service - Grain Temperature Forecasting API

## ğŸŒ¾ Overview

The SiloFlow Service is an automated grain temperature forecasting platform that provides REST API endpoints for processing grain sensor data and generating multi-horizon temperature forecasts. The service is designed for enterprise-scale operations with automated data retrieval from MySQL databases, intelligent preprocessing, model training, and forecasting capabilities.

## ğŸ“‹ Table of Contents

- [Architecture Overview](#-architecture-overview)
- [Quick Start](#-quick-start)
- [API Endpoints](#-api-endpoints)
- [Data Retrieval System](#-data-retrieval-system)
- [Configuration](#-configuration)
- [Testing & Development](#-testing--development)
- [File Structure](#-file-structure)
- [Troubleshooting](#-troubleshooting)
- [Production Deployment](#-production-deployment)

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SiloFlow Service Architecture                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Data Sources   â”‚    â”‚   Processing    â”‚    â”‚    Outputs      â”‚ â”‚
â”‚  â”‚                 â”‚    â”‚    Pipeline     â”‚    â”‚                 â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ â€¢ MySQL DB      â”‚â”€â”€â”€â–¶â”‚ â€¢ Data Ingestionâ”‚â”€â”€â”€â–¶â”‚ â€¢ REST API      â”‚ â”‚
â”‚  â”‚ â€¢ CSV/Parquet   â”‚    â”‚ â€¢ Preprocessing â”‚    â”‚ â€¢ Forecasts     â”‚ â”‚
â”‚  â”‚ â€¢ Manual Upload â”‚    â”‚ â€¢ Model Trainingâ”‚    â”‚ â€¢ Model Files   â”‚ â”‚
â”‚  â”‚                 â”‚    â”‚ â€¢ Forecasting   â”‚    â”‚                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                Component Details                            â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ â€¢ FastAPI Service (main.py, routes/)                       â”‚ â”‚
â”‚  â”‚ â€¢ Automated Processor (automated_processor.py)             â”‚ â”‚
â”‚  â”‚ â€¢ Granary Pipeline (granary_pipeline.py)                   â”‚ â”‚
â”‚  â”‚ â€¢ Data Retrieval System (scripts/data_retrieval/)          â”‚ â”‚
â”‚  â”‚ â€¢ Testing GUI (scripts/testing/testingservice.py)          â”‚ â”‚
â”‚  â”‚ â€¢ Utilities (utils/)                                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- MySQL database access (optional for database retrieval)
- Dependencies: `pip install -r requirements.txt`

### 1. Start the Service
```bash
# Option 1: Using the startup script (recommended)
python start_service.py

# Option 2: Direct FastAPI startup
python -m uvicorn main:app --host 0.0.0.0 --port 8000 --reload

# Option 3: Using Python directly
python main.py
```

The service will start on `http://localhost:8000`

### 2. Verify Service Health
```bash
curl http://localhost:8000/health
```

### 3. View API Documentation
Open your browser to: `http://localhost:8000/docs`

### 4. Quick Test with Sample Data
```bash
# Upload a CSV file for processing
curl -X POST "http://localhost:8000/process" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@your_data.csv"

# Generate forecasts for all processed granaries
curl http://localhost:8000/forecast
```

## ğŸ“‹ API Endpoints

### Core Processing Endpoints

#### `POST /process`
**Purpose**: Process-only endpoint for data ingestion and preprocessing without training
- **Input**: CSV or Parquet file with grain sensor data
- **Process**: Ingestion â†’ Granary splitting â†’ Preprocessing â†’ Feature engineering
- **Output**: Processing status for each granary

```bash
curl -X POST "http://localhost:8000/process" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@sensor_data.csv"
```

#### `POST /pipeline`
**Purpose**: Full pipeline including processing, training, and forecasting
- **Input**: CSV or Parquet file + optional horizon parameter
- **Process**: Ingestion â†’ Preprocessing â†’ Model Training â†’ Forecasting
- **Output**: Complete pipeline results with forecasts

```bash
curl -X POST "http://localhost:8000/pipeline" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@sensor_data.csv" \
     -F "horizon=7"
```

#### `GET /forecast`
**Purpose**: Generate forecasts for all granaries with trained models
- **Input**: None (uses existing processed data and models)
- **Process**: Loads models â†’ Generates 1-day forecasts
- **Output**: Forecast data for all available granaries

```bash
curl http://localhost:8000/forecast
```

### Utility Endpoints

#### `GET /health`
**Purpose**: Service health check
```bash
curl http://localhost:8000/health
```

#### `GET /models`
**Purpose**: List all available trained models
```bash
curl http://localhost:8000/models
```

#### `POST /train`
**Purpose**: Train models for specific or all granaries
```bash
curl -X POST http://localhost:8000/train
```

### Response Format
All endpoints return JSON responses with this structure:
```json
{
  "status": "success|error",
  "timestamp": "2025-07-22T10:30:00",
  "data": {...},
  "errors": [...],
  "granaries_processed": 2,
  "processing_time": "120.5s"
}
```

## ğŸ“Š Data Retrieval System

The service includes automated data retrieval from MySQL databases with memory-optimized streaming.

### Configuration
Configure database access in `config/streaming_config.json`:
```json
{
  "database": {
    "host": "your-db-host",
    "port": 3306,
    "database": "cloud_lq",
    "user": "your-username",
    "password": "your-password"
  }
}
```

### Data Retrieval Scripts

#### Full Database Streaming
```bash
cd scripts/data_retrieval
python sql_data_streamer.py --start-date 2024-01-01 --end-date 2024-12-31
```

#### Simple Single-Silo Retrieval
```bash
python simple_data_retrieval.py \
  --granary-name "èš¬å†ˆåº“" \
  --silo-id "41f2257ce3d64083b1b5f8e59e80bc4d" \
  --start-date "2024-07-17" \
  --end-date "2024-07-18"
```

#### Automated Batch Retrieval
```bash
python automated_data_retrieval.py \
  --date-range \
  --start 2024-12-01 \
  --end 2024-12-07 \
  --granary "èš¬å†ˆåº“"
```

### Database Utilities
- `list_granaries.py` - List all available granaries in the database
- `get_silos_for_granary.py` - Get all silos for a specific granary
- `get_date_ranges.py` - Check available date ranges for each silo

## âš™ï¸ Configuration

### Main Configuration Files

#### `config/streaming_config.json` - Database Configuration
```json
{
  "database": {
    "host": "your-host",
    "port": 3306,
    "database": "cloud_lq",
    "user": "username",
    "password": "password"
  },
  "processing": {
    "initial_chunk_size": 50000,
    "memory_threshold_percent": 75,
    "output_dir": "data/streaming"
  }
}
```

#### `config/data_paths.json` - Directory Structure
```json
{
  "granaries_dir": "data/granaries",
  "processed_dir": "data/processed",
  "models_dir": "data/models",
  "forecasts_dir": "data/forecasts"
}
```

### Directory Structure
```
service/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ granaries/          # Raw granary-specific data files
â”‚   â”œâ”€â”€ processed/          # Cleaned and preprocessed data
â”‚   â”œâ”€â”€ models/            # Trained ML models (.joblib files)
â”‚   â”œâ”€â”€ forecasts/         # Generated forecast files
â”‚   â””â”€â”€ temp/              # Temporary processing files
â”œâ”€â”€ temp_uploads/          # Incoming file uploads
â””â”€â”€ logs/                  # Service logs
```

## ğŸ§ª Testing & Development

### Interactive Testing GUI
Launch the comprehensive testing interface:
```bash
cd scripts/testing
python testingservice.py
```

Features:
- **HTTP Service Testing**: Test all API endpoints
- **Simple Data Retrieval**: GUI for database retrieval
- **Database Explorer**: Browse available granaries and silos
- **Batch Processing**: Process multiple files
- **Logs & Monitoring**: View service logs and performance

### Client Testing
Test remote service instances:
```bash
cd scripts/client
python siloflow_client_tester.py --server your-server-ip --create-sample
```

### Command Line Testing
```bash
# Test service health
curl http://localhost:8000/health

# Test with sample file
curl -X POST "http://localhost:8000/process" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@test_data.csv"
```

## ğŸ“ File Structure

```
service/
â”œâ”€â”€ main.py                     # FastAPI application entry point
â”œâ”€â”€ start_service.py           # Service startup script
â”œâ”€â”€ core.py                    # Core singleton instances
â”œâ”€â”€ automated_processor.py     # Main processing engine
â”œâ”€â”€ granary_pipeline.py        # Data pipeline orchestrator
â”œâ”€â”€ routes/                    # API route definitions
â”‚   â”œâ”€â”€ __init__.py           # Router aggregation
â”‚   â”œâ”€â”€ pipeline.py           # /process and /pipeline endpoints
â”‚   â”œâ”€â”€ forecast.py           # /forecast endpoint
â”‚   â”œâ”€â”€ health.py             # /health endpoint
â”‚   â”œâ”€â”€ models.py             # /models and /train endpoints
â”‚   â””â”€â”€ train.py              # Training-specific endpoints
â”œâ”€â”€ utils/                     # Utility modules
â”‚   â”œâ”€â”€ data_paths.py         # Centralized path management
â”‚   â””â”€â”€ database_utils.py     # Database utilities
â”œâ”€â”€ scripts/                   # Standalone scripts
â”‚   â”œâ”€â”€ data_retrieval/       # Database retrieval scripts
â”‚   â”‚   â”œâ”€â”€ sql_data_streamer.py          # Main streaming script
â”‚   â”‚   â”œâ”€â”€ simple_data_retrieval.py     # Simple retrieval
â”‚   â”‚   â””â”€â”€ automated_data_retrieval.py  # Automated batch retrieval
â”‚   â”œâ”€â”€ database/             # Database utility scripts
â”‚   â”‚   â”œâ”€â”€ list_granaries.py            # List available granaries
â”‚   â”‚   â”œâ”€â”€ get_silos_for_granary.py     # Get silos for granary
â”‚   â”‚   â””â”€â”€ get_date_ranges.py           # Check date ranges
â”‚   â”œâ”€â”€ testing/              # Testing and development tools
â”‚   â”‚   â””â”€â”€ testingservice.py            # GUI testing interface
â”‚   â”œâ”€â”€ client/               # Client testing scripts
â”‚   â””â”€â”€ parquet_inspector.py  # Parquet file inspection utility
â”œâ”€â”€ config/                    # Configuration files
â”‚   â”œâ”€â”€ streaming_config.json # Database configuration
â”‚   â”œâ”€â”€ data_paths.json       # Directory paths
â”‚   â””â”€â”€ production_config.json# Production settings
â”œâ”€â”€ docs/                      # Additional documentation
â””â”€â”€ README.md                  # This file
```

## ğŸ”§ Core Components Explained

### `automated_processor.py` - Processing Engine
The heart of the system that handles:
- File ingestion and format detection
- Granary separation and data splitting
- Preprocessing pipeline (cleaning, gap insertion, interpolation)
- Feature engineering (temporal, spatial, lag features)
- Model training with hyperparameter optimization
- Multi-horizon forecasting (1-7 days)
- Memory management and error handling

### `granary_pipeline.py` - Pipeline Orchestrator
Modular pipeline for processing individual granaries:
- **Ingest**: Data sorting, deduplication, standardization
- **Preprocess**: Cleaning, gap insertion, feature engineering
- **Train**: Model fitting with Dashboard-optimized settings
- **Forecast**: Multi-horizon prediction

### `routes/` - API Layer
FastAPI route definitions:
- **pipeline.py**: Main processing endpoints (`/process`, `/pipeline`)
- **forecast.py**: Forecasting endpoint (`/forecast`)
- **health.py**: Health check endpoint (`/health`)
- **models.py**: Model management (`/models`, `/train`)

### `scripts/data_retrieval/` - Database Integration
- **sql_data_streamer.py**: Memory-optimized streaming from MySQL
- **simple_data_retrieval.py**: Single-silo retrieval
- **automated_data_retrieval.py**: Batch retrieval with date ranges

## ğŸš¨ Troubleshooting

### Common Issues

#### Service Won't Start
- **Check dependencies**: `pip install -r requirements.txt`
- **Check ports**: Ensure port 8000 is available
- **Check logs**: `service.log` in the service directory

#### Memory Issues During Processing
- **Reduce chunk size** in streaming_config.json
- **Lower memory threshold** (e.g., from 75% to 60%)
- **Close other applications** to free memory

#### Database Connection Failed
- **Check configuration** in `config/streaming_config.json`
- **Test connection** using `scripts/testing/testingservice.py`
- **Verify credentials** and network access

#### Models Not Found
- **Run preprocessing first**: Use `/process` endpoint
- **Check model directory**: `data/models/` should contain `.joblib` files
- **Retrain models**: Use `/train` endpoint

### Log Files
- **service.log**: Main service logs
- **sql_data_streamer.log**: Database retrieval logs
- **simple_data_retrieval.log**: Simple retrieval logs

### Performance Monitoring
```bash
# Check service health
curl http://localhost:8000/health

# Monitor resource usage
python -c "import psutil; print(f'Memory: {psutil.virtual_memory().percent}%')"

# Check disk space
python -c "import psutil; print(f'Disk: {psutil.disk_usage('.').free/1024**3:.1f}GB free')"
```

## ğŸŒ Production Deployment

### System Requirements
- **OS**: Windows/Linux with Python 3.8+
- **Memory**: 8GB+ recommended (16GB+ for large datasets)
- **Storage**: 50GB+ free space for data and models
- **Network**: Access to MySQL database

### Production Checklist
1. **Environment Setup**:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Linux/Mac
   # or
   .venv\Scripts\activate.bat  # Windows
   pip install -r requirements.txt
   ```

2. **Configuration**:
   - Update `config/streaming_config.json` with production database
   - Configure `config/data_paths.json` for production paths
   - Set up SSL certificates for HTTPS

3. **Security**:
   - Change default passwords
   - Configure CORS properly
   - Set up authentication if needed
   - Use environment variables for sensitive data

4. **Monitoring**:
   - Set up log rotation
   - Monitor disk space and memory usage
   - Configure health check alerts
   - Set up backup procedures for models and data

5. **Service Management**:
   ```bash
   # For production, use a process manager like systemd or supervisor
   # Example systemd service file:
   [Unit]
   Description=SiloFlow Service
   After=network.target
   
   [Service]
   Type=simple
   User=siloflow
   WorkingDirectory=/path/to/service
   ExecStart=/path/to/.venv/bin/python start_service.py
   Restart=always
   
   [Install]
   WantedBy=multi-user.target
   ```

### Performance Optimization
- **Use SSD storage** for faster I/O operations
- **Configure memory limits** based on available RAM
- **Use database connection pooling** for high-load scenarios
- **Enable model compression** to save storage space
- **Set up data retention policies** to manage disk usage

### Scaling Options
- **Horizontal scaling**: Deploy multiple service instances behind a load balancer
- **Database optimization**: Use read replicas, indexing, and query optimization
- **Model serving**: Use dedicated model serving infrastructure for high-throughput forecasting
- **Containerization**: Use Docker for consistent deployments

## ğŸ“ API Usage Examples

### Complete Workflow Example
```python
import requests
import pandas as pd

# 1. Health check
response = requests.get('http://localhost:8000/health')
print(response.json())

# 2. Process data
with open('sensor_data.csv', 'rb') as f:
    files = {'file': ('sensor_data.csv', f, 'text/csv')}
    response = requests.post('http://localhost:8000/process', files=files)
    print(response.json())

# 3. Train models
response = requests.post('http://localhost:8000/train')
print(response.json())

# 4. Generate forecasts
response = requests.get('http://localhost:8000/forecast')
forecasts = response.json()
print(f"Generated forecasts for {forecasts['forecasts_count']} granaries")
```

### Data Retrieval and Processing Workflow
```bash
# 1. Retrieve data from database
cd scripts/data_retrieval
python sql_data_streamer.py --start-date 2024-01-01 --end-date 2024-01-31

# 2. Process the retrieved data
curl -X POST "http://localhost:8000/process" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@data/granaries/combined_granaries.csv"

# 3. Train models
curl -X POST http://localhost:8000/train

# 4. Generate forecasts
curl http://localhost:8000/forecast
```

## ğŸ“ Support & Maintenance

### Daily Operations
1. **Health Check**: `curl http://localhost:8000/health`
2. **Log Review**: Check service logs for errors
3. **Data Retrieval**: Run incremental data updates
4. **Forecast Generation**: Generate daily forecasts

### Weekly Maintenance
1. **Model Performance Review**: Check forecast accuracy
2. **Data Quality Check**: Review input data statistics
3. **System Resource Check**: Monitor CPU, memory, disk usage
4. **Backup**: Backup models and configuration files

### Monthly Maintenance
1. **Model Retraining**: Retrain with fresh data
2. **Performance Optimization**: Review and optimize settings
3. **Security Updates**: Update dependencies and credentials
4. **Capacity Planning**: Review growth and resource needs

---

**SiloFlow Service v2.0** - Automated Grain Temperature Forecasting Platform  
For additional support, check the documentation in the `docs/` directory or review the inline code comments.
