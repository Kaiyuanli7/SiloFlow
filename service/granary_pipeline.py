#!/usr/bin/env python3
"""
Granary Data Pipeline - Modular CLI & Automation
===============================================

Usage:
    python granary_pipeline.py ingest --input <raw.csv>
    python granary_pipeline.py preprocess --input <granary.csv> --output <processed.csv>
    python granary_pipeline.py train --granary <name> [--tune] [--trials 100] [--timeout 600]
    python granary_pipeline.py forecast --granary <name> --horizon <days>

This script orchestrates:
- Ingestion: sorting, deduplication, standardization
- Preprocessing: cleaning, gap insertion, interpolation, feature engineering
- Training: model fitting & hyperparameter optimization with Optuna tuning (default)
- Forecasting: multi-horizon prediction

Training Configuration:
- GPU Auto-Detection: Automatically detects and uses GPU if available, falls back to CPU
- Optuna Tuning (default): Automatically finds optimal hyperparameters for each granary
- Fixed Parameters: Uses pre-configured parameters (use --no-tune flag)
- Quantile regression: Uses quantile objective with alpha=0.5 for improved MAE
- Anchor-day early stopping: Uses 7-day consecutive forecasting accuracy
- Horizon balancing: Applies increasing horizon strategy for better long-term predictions
- Conservative mode: 3x stability feature boost + 2x directional feature boost
- 95/5 split: Internal split for finding optimal iterations, then train on 100% data

Examples:
    # Train with Optuna tuning (recommended)
    python granary_pipeline.py train --granary ABC123
    
    # Train with custom tuning parameters
    python granary_pipeline.py train --granary ABC123 --trials 50 --timeout 300
    
    # Train with fixed parameters (faster but potentially less accurate)
    python granary_pipeline.py train --granary ABC123 --no-tune

All steps are modular and importable for future automation/cloud deployment.
"""
import pandas as pd
import numpy as np
import pathlib
import logging
import sys
import json
import gc
import shutil
from pathlib import Path
from typing import Optional, List, Dict, Tuple, Union
import argparse
import os

# Polars optimization for massive datasets
try:
    import polars as pl
    HAS_POLARS = True
    print("*** POLARS INTEGRATION: Polars loaded successfully - performance optimizations enabled")
    print("*** BACKEND SELECTION: Will use Polars for datasets >50K rows and files >100MB")
    print(f"*** POLARS VERSION: {pl.__version__}")
except ImportError:
    HAS_POLARS = False
    pl = None
    print("*** POLARS NOT AVAILABLE: Using pandas fallback (install polars for better performance)")
    print("*** INSTALL COMMAND: pip install polars pyarrow")

# Polars optimization functions for massive dataset support
def load_data_optimized(file_path: str) -> pd.DataFrame:
    """
    Optimized data loading with automatic backend selection.
    Uses Polars for large files, converts to pandas for compatibility.
    """
    file_path = Path(file_path)
    file_size_mb = file_path.stat().st_size / (1024 * 1024)
    
    if HAS_POLARS and file_size_mb > 100:  # Use Polars for files > 100MB
        try:
            logger.info(f"*** POLARS BACKEND: Loading {file_size_mb:.1f}MB file with Polars for optimal performance")
            print(f"*** POLARS BACKEND: Loading {file_size_mb:.1f}MB file with Polars for optimal performance")
            
            if file_path.suffix.lower() == '.parquet':
                df_pl = pl.read_parquet(file_path)
            else:
                df_pl = pl.read_csv(file_path)
            
            # Convert to pandas for compatibility
            df = df_pl.to_pandas()
            logger.info(f"*** POLARS SUCCESS: Loaded and converted to pandas (shape: {df.shape})")
            print(f"*** POLARS SUCCESS: Loaded and converted to pandas (shape: {df.shape})")
            return df
        except Exception as e:
            logger.warning(f"*** POLARS FALLBACK: Polars loading failed, using pandas: {e}")
            print(f"*** POLARS FALLBACK: Polars loading failed, using pandas: {e}")
            # Fallback to pandas
            from granarypredict.ingestion import read_granary_csv
            return read_granary_csv(file_path)
    else:
        backend_reason = "file too small" if HAS_POLARS else "Polars not available"
        logger.info(f"*** PANDAS BACKEND: Loading {file_size_mb:.1f}MB file with pandas ({backend_reason})")
        print(f"*** PANDAS BACKEND: Loading {file_size_mb:.1f}MB file with pandas ({backend_reason})")
        from granarypredict.ingestion import read_granary_csv
        return read_granary_csv(file_path)


def add_lags_optimized(df: pd.DataFrame, lags: list = [1, 2, 3, 7], 
                      temp_col: str = "temperature_grain") -> pd.DataFrame:
    """
    Memory-optimized lag computation that prevents allocation errors.
    Uses Polars for massive performance improvement and memory efficiency.
    """
    if HAS_POLARS and len(df) > 50_000:  # Use Polars for large datasets
        try:
            logger.info(f"*** POLARS BACKEND: Using Polars for lag computation on {len(df):,} rows")
            print(f"*** POLARS BACKEND: Using Polars for lag computation on {len(df):,} rows")
            
            # Convert to Polars
            df_pl = pl.from_pandas(df)
            
            # Define group columns
            group_cols = [col for col in ["granary_id", "heap_id", "grid_x", "grid_y", "grid_z"] 
                         if col in df.columns]
            
            # Create lag expressions (vectorized in Polars)
            lag_expressions = []
            for lag in lags:
                if group_cols:
                    lag_expr = pl.col(temp_col).shift(lag).over(group_cols).alias(f"lag_temp_{lag}d")
                else:
                    lag_expr = pl.col(temp_col).shift(lag).alias(f"lag_temp_{lag}d")
                lag_expressions.append(lag_expr)
            
            # Apply all lags at once (highly efficient)
            df_pl = df_pl.with_columns(lag_expressions)
            
            # Add delta features
            delta_expressions = []
            for lag in lags:
                lag_col = f"lag_temp_{lag}d"
                delta_col = f"delta_temp_{lag}d"
                delta_expr = (pl.col(temp_col) - pl.col(lag_col)).alias(delta_col)
                delta_expressions.append(delta_expr)
            
            df_pl = df_pl.with_columns(delta_expressions)
            
            # Convert back to pandas
            df_result = df_pl.to_pandas()
            logger.info(f"*** POLARS SUCCESS: Lag computation completed successfully")
            print(f"*** POLARS SUCCESS: Lag computation completed successfully")
            return df_result
            
        except Exception as e:
            logger.warning(f"*** POLARS FALLBACK: Polars lag computation failed, using memory-efficient fallback: {e}")
            print(f"*** POLARS FALLBACK: Polars lag computation failed, using memory-efficient fallback: {e}")
            # Use the memory-efficient pandas version from features.py
            from granarypredict.features import add_multi_lag
            return add_multi_lag(df, lags=tuple(lags), temp_col=temp_col)
    else:
        # Use standard pandas approach for smaller datasets
        backend_reason = f"dataset too small ({len(df):,} rows)" if HAS_POLARS else "Polars not available"
        logger.info(f"*** PANDAS BACKEND: Using pandas for lag computation ({backend_reason})")
        print(f"*** PANDAS BACKEND: Using pandas for lag computation ({backend_reason})")
        from granarypredict.features import add_multi_lag
        return add_multi_lag(df, lags=tuple(lags), temp_col=temp_col)


def create_time_features_optimized(df: pd.DataFrame, timestamp_col: str = "detection_time") -> pd.DataFrame:
    """Polars-optimized time feature creation - 5-10x faster than pandas."""
    if HAS_POLARS and len(df) > 50_000:
        try:
            logger.info(f"*** POLARS BACKEND: Using Polars for time features on {len(df):,} rows")
            print(f"*** POLARS BACKEND: Using Polars for time features on {len(df):,} rows")
            
            df_pl = pl.from_pandas(df)
            df_pl = df_pl.with_columns([
                pl.col(timestamp_col).dt.year().alias("year"),
                pl.col(timestamp_col).dt.month().alias("month"),
                pl.col(timestamp_col).dt.day().alias("day"),
                pl.col(timestamp_col).dt.hour().alias("hour"),
                
                # Cyclical encodings - vectorized in Polars
                (2 * np.pi * pl.col(timestamp_col).dt.month() / 12).sin().alias("month_sin"),
                (2 * np.pi * pl.col(timestamp_col).dt.month() / 12).cos().alias("month_cos"),
                (2 * np.pi * pl.col(timestamp_col).dt.hour() / 24).sin().alias("hour_sin"),
                (2 * np.pi * pl.col(timestamp_col).dt.hour() / 24).cos().alias("hour_cos"),
                
                # Day of year and week features
                pl.col(timestamp_col).dt.ordinal_day().alias("doy"),
                pl.col(timestamp_col).dt.week().alias("weekofyear"),
                
                # More cyclical encodings
                (2 * np.pi * pl.col(timestamp_col).dt.ordinal_day() / 365).sin().alias("doy_sin"),
                (2 * np.pi * pl.col(timestamp_col).dt.ordinal_day() / 365).cos().alias("doy_cos"),
                (2 * np.pi * pl.col(timestamp_col).dt.week() / 52).sin().alias("woy_sin"),
                (2 * np.pi * pl.col(timestamp_col).dt.week() / 52).cos().alias("woy_cos"),
                
                # Weekend indicator
                (pl.col(timestamp_col).dt.weekday() >= 6).alias("is_weekend")
            ])
            
            df_result = df_pl.to_pandas()
            logger.info(f"*** POLARS SUCCESS: Time features completed successfully")
            print(f"*** POLARS SUCCESS: Time features completed successfully")
            return df_result
            
        except Exception as e:
            logger.warning(f"*** POLARS FALLBACK: Polars time features failed, using pandas: {e}")
            print(f"*** POLARS FALLBACK: Polars time features failed, using pandas: {e}")
            from granarypredict.features import create_time_features
            return create_time_features(df, timestamp_col=timestamp_col)
    else:
        backend_reason = f"dataset too small ({len(df):,} rows)" if HAS_POLARS else "Polars not available"
        logger.info(f"*** PANDAS BACKEND: Using pandas for time features ({backend_reason})")
        print(f"*** PANDAS BACKEND: Using pandas for time features ({backend_reason})")
        from granarypredict.features import create_time_features
        return create_time_features(df, timestamp_col=timestamp_col)


def add_rolling_stats_optimized(df: pd.DataFrame, window_days: int = 7, 
                               temp_col: str = "temperature_grain") -> pd.DataFrame:
    """Polars-optimized rolling statistics - 5-20x faster than pandas."""
    if HAS_POLARS and len(df) > 50_000:
        try:
            logger.info(f"*** POLARS BACKEND: Using Polars for rolling statistics on {len(df):,} rows")
            print(f"*** POLARS BACKEND: Using Polars for rolling statistics on {len(df):,} rows")
            
            df_pl = pl.from_pandas(df)
            group_cols = [col for col in ["granary_id", "heap_id", "grid_x", "grid_y", "grid_z"] 
                         if col in df.columns]
            
            if group_cols:
                # Grouped rolling operations
                rolling_expressions = [
                    pl.col(temp_col).rolling_mean(window_days).over(group_cols).alias(f"roll_mean_{window_days}d"),
                    pl.col(temp_col).rolling_std(window_days).over(group_cols).alias(f"roll_std_{window_days}d")
                ]
            else:
                # Non-grouped rolling operations
                rolling_expressions = [
                    pl.col(temp_col).rolling_mean(window_days).alias(f"roll_mean_{window_days}d"),
                    pl.col(temp_col).rolling_std(window_days).alias(f"roll_std_{window_days}d")
                ]
            
            df_pl = df_pl.with_columns(rolling_expressions)
            df_result = df_pl.to_pandas()
            logger.info(f"*** POLARS SUCCESS: Rolling statistics completed successfully")
            print(f"*** POLARS SUCCESS: Rolling statistics completed successfully")
            return df_result
            
        except Exception as e:
            logger.warning(f"*** POLARS FALLBACK: Polars rolling statistics failed, using pandas: {e}")
            print(f"*** POLARS FALLBACK: Polars rolling statistics failed, using pandas: {e}")
            from granarypredict.features import add_rolling_stats
            return add_rolling_stats(df, window_days=window_days, temp_col=temp_col)
    else:
        backend_reason = f"dataset too small ({len(df):,} rows)" if HAS_POLARS else "Polars not available"
        logger.info(f"*** PANDAS BACKEND: Using pandas for rolling statistics ({backend_reason})")
        print(f"*** PANDAS BACKEND: Using pandas for rolling statistics ({backend_reason})")
        from granarypredict.features import add_rolling_stats
        return add_rolling_stats(df, window_days=window_days, temp_col=temp_col)

import argparse
import time
import joblib
from pathlib import Path
from typing import Optional, List, Dict, Tuple, Union
import asyncio
import os

# GPU Detection Function (with multi-GPU support and proper selection)
def detect_gpu_availability(force_enable=False, preferred_gpu_id=None):
    """
    Enhanced GPU detection with multi-GPU support and proper selection.
    Returns tuple: (gpu_available, gpu_info, gpu_config)
    
    Args:
        force_enable: If True, attempt to enable GPU even if auto-detection is disabled
        preferred_gpu_id: Specific GPU ID to use (0, 1, 2, etc.). If None, auto-select best GPU
    """
    if force_enable:
        # User requested GPU - try comprehensive detection
        try:
            import lightgbm as lgb
            import numpy as np
            
            # Detect available GPUs
            gpu_info = detect_available_gpus()
            
            if not gpu_info['gpus']:
                return False, "No GPUs detected on system", {}
            
            # Select which GPU to use
            if preferred_gpu_id is not None:
                if preferred_gpu_id >= len(gpu_info['gpus']):
                    return False, f"GPU ID {preferred_gpu_id} not available. Available GPUs: {len(gpu_info['gpus'])}", {}
                selected_gpu = preferred_gpu_id
                selection_reason = f"user-specified GPU {preferred_gpu_id}"
            else:
                # Auto-select best GPU based on memory and compute capability
                selected_gpu = select_best_gpu(gpu_info['gpus'])
                selection_reason = f"auto-selected best GPU {selected_gpu}"
            
            # Test the selected GPU with LightGBM
            gpu_config = {
                'device': 'gpu',
                'gpu_platform_id': 0,  # Usually 0 for CUDA/OpenCL
                'gpu_device_id': selected_gpu,
                'gpu_use_dp': True,  # Use double precision for stability
                'max_bin': 255,  # Optimized for GPU memory
            }
            
            # Verify GPU works with LightGBM
            test_data = lgb.Dataset(np.random.random((100, 10)), label=np.random.random(100))
            test_params = {
                'objective': 'regression', 
                'verbose': -1,
                **gpu_config
            }
            
            model = lgb.train(
                params=test_params,
                train_set=test_data,
                num_boost_round=5,
                valid_sets=[test_data],
                callbacks=[lgb.early_stopping(3), lgb.log_evaluation(0)]
            )
            
            gpu_details = gpu_info['gpus'][selected_gpu]
            success_msg = (
                f"GPU acceleration enabled: {gpu_details['name']} "
                f"({gpu_details['memory_mb']:.0f}MB, {selection_reason})"
            )
            
            return True, success_msg, gpu_config
            
        except Exception as e:
            return False, f"GPU requested but failed: {e}", {}
    else:
        # Default behavior - disabled for compatibility
        return False, "GPU detection disabled by default - using CPU for compatibility", {}

def detect_available_gpus():
    """
    Detect all available GPUs on the system with detailed information.
    Returns dict with GPU information for proper selection.
    """
    gpu_info = {
        'gpus': [],
        'total_gpus': 0,
        'detection_method': 'none'
    }
    
    # Try NVIDIA GPUs first (most common for ML)
    try:
        import pynvml
        pynvml.nvmlInit()
        gpu_count = pynvml.nvmlDeviceGetCount()
        
        for i in range(gpu_count):
            handle = pynvml.nvmlDeviceGetHandleByIndex(i)
            name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
            memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            
            # Get utilization info
            try:
                utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_util = utilization.gpu
                memory_util = utilization.memory
            except:
                gpu_util = 0
                memory_util = 0
            
            gpu_info['gpus'].append({
                'id': i,
                'name': name,
                'memory_total_mb': memory_info.total / 1024 / 1024,
                'memory_free_mb': memory_info.free / 1024 / 1024,
                'memory_used_mb': memory_info.used / 1024 / 1024,
                'gpu_utilization': gpu_util,
                'memory_utilization': memory_util,
                'vendor': 'NVIDIA'
            })
        
        gpu_info['total_gpus'] = gpu_count
        gpu_info['detection_method'] = 'pynvml'
        return gpu_info
        
    except ImportError:
        pass  # pynvml not available
    except Exception as e:
        pass  # NVIDIA detection failed
    
    # Try OpenCL detection as fallback
    try:
        import pyopencl as cl
        platforms = cl.get_platforms()
        
        gpu_id = 0
        for platform in platforms:
            devices = platform.get_devices(device_type=cl.device_type.GPU)
            for device in devices:
                # Get basic device info
                name = device.name.strip()
                global_mem = device.global_mem_size / 1024 / 1024  # Convert to MB
                
                gpu_info['gpus'].append({
                    'id': gpu_id,
                    'name': name,
                    'memory_total_mb': global_mem,
                    'memory_free_mb': global_mem * 0.9,  # Estimate
                    'memory_used_mb': global_mem * 0.1,  # Estimate
                    'gpu_utilization': 0,  # Cannot get real-time utilization
                    'memory_utilization': 10,  # Estimate
                    'vendor': str(platform.vendor).strip()
                })
                gpu_id += 1
        
        gpu_info['total_gpus'] = len(gpu_info['gpus'])
        gpu_info['detection_method'] = 'opencl'
        return gpu_info
        
    except ImportError:
        pass  # pyopencl not available
    except Exception as e:
        pass  # OpenCL detection failed
    
    # No GPUs detected
    return gpu_info

def select_best_gpu(gpus):
    """
    Select the best GPU for LightGBM training based on multiple factors.
    Returns the GPU ID of the best GPU.
    """
    if not gpus:
        return 0
    
    if len(gpus) == 1:
        return 0
    
    # Scoring function: prioritize free memory, low utilization, and total memory
    best_gpu = 0
    best_score = -1
    
    for gpu in gpus:
        # Calculate score based on:
        # 1. Free memory (40% weight)
        # 2. Low GPU utilization (30% weight) 
        # 3. Total memory (20% weight)
        # 4. Low memory utilization (10% weight)
        
        free_memory_score = gpu['memory_free_mb'] / max(g['memory_total_mb'] for g in gpus)
        gpu_util_score = max(0, (100 - gpu['gpu_utilization']) / 100)
        total_memory_score = gpu['memory_total_mb'] / max(g['memory_total_mb'] for g in gpus)
        memory_util_score = max(0, (100 - gpu['memory_utilization']) / 100)
        
        score = (
            0.4 * free_memory_score +
            0.3 * gpu_util_score +
            0.2 * total_memory_score +
            0.1 * memory_util_score
        )
        
        if score > best_score:
            best_score = score
            best_gpu = gpu['id']
    
    return best_gpu

def get_gpu_config_for_dataset(dataset_size, feature_count, gpu_memory_mb):
    """
    Get optimal GPU configuration based on dataset characteristics and GPU memory.
    """
    # Estimate memory requirements
    # Rough estimate: dataset_size * feature_count * 8 bytes * 3 (for working memory)
    estimated_memory_mb = (dataset_size * feature_count * 8 * 3) / (1024 * 1024)
    
    config = {
        'device': 'gpu',
        'gpu_platform_id': 0,
        'gpu_device_id': 0,  # Will be set by caller
        'gpu_use_dp': True,  # Use double precision for stability
    }
    
    # Adjust max_bin based on available memory and dataset size
    if estimated_memory_mb > gpu_memory_mb * 0.8:  # If using >80% of GPU memory
        config['max_bin'] = 127  # Very conservative
    elif estimated_memory_mb > gpu_memory_mb * 0.6:  # If using >60% of GPU memory
        config['max_bin'] = 255  # Conservative
    elif dataset_size > 500_000:  # Large datasets
        config['max_bin'] = 255  # Conservative for large data
    else:
        config['max_bin'] = 511  # Standard
    
    return config

# Detect GPU availability once at module level with multi-GPU support
GPU_AVAILABLE, GPU_INFO, GPU_CONFIG = detect_gpu_availability()

# Add granarypredict directory to Python path
sys.path.insert(0, str(Path(__file__).parent.parent))

# Import granarypredict modules
from granarypredict import ingestion, cleaning, features
from granarypredict.data_utils import assign_group_id, comprehensive_sort

# Import Polars-optimized data utilities for large dataset processing
try:
    from granarypredict.polars_data_utils import (
        comprehensive_sort_optimized, 
        assign_group_id_optimized,
        optimized_data_pipeline
    )
    HAS_POLARS_DATA_UTILS = True
    print("*** POLARS DATA UTILS: Polars-optimized sorting and grouping available")
except ImportError:
    HAS_POLARS_DATA_UTILS = False
    print("*** POLARS DATA UTILS: Using standard pandas-only sorting and grouping")

# Import GPU-accelerated data utilities for maximum performance
try:
    from granarypredict.gpu_data_utils import (
        gpu_comprehensive_sort_optimized,
        gpu_assign_group_id_optimized,
        gpu_data_pipeline,
        detect_optimal_backend,
        get_gpu_data_backend_info,
        HAS_CUDF
    )
    HAS_GPU_DATA_UTILS = True
    if HAS_CUDF:
        print("*** GPU DATA UTILS: RAPIDS cuDF GPU-accelerated data processing available (10-150x speedup)")
    else:
        print("*** GPU DATA UTILS: GPU utilities available but cuDF not installed - falling back to Polars/pandas")
except ImportError:
    HAS_GPU_DATA_UTILS = False
    print("*** GPU DATA UTILS: Using standard data processing (install cudf for massive speedups)")

# Import cleaning helpers from the correct location
# These functions are defined in the Dashboard.py file, so we'll import them from there
sys.path.insert(0, str(Path(__file__).parent.parent / "app"))
try:
    from Dashboard import insert_calendar_gaps, interpolate_sensor_numeric, split_train_eval_frac
except ImportError:
    # Fallback: define simple versions if Dashboard import fails
    def insert_calendar_gaps(df: pd.DataFrame) -> pd.DataFrame:
        """Simple calendar gap insertion - placeholder"""
        return df
    
    def interpolate_sensor_numeric(df: pd.DataFrame) -> pd.DataFrame:
        """Simple numeric interpolation - placeholder"""
        return df
    
    def split_train_eval_frac(df: pd.DataFrame, test_frac: float = 0.05) -> tuple:
        """Simple train/eval split - placeholder"""
        split_idx = int(len(df) * (1 - test_frac))
        return df.iloc[:split_idx], df.iloc[split_idx:]

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_gpu_config_for_training(dataset_size=None, feature_count=None, force_gpu_id=None):
    """
    Get GPU configuration for model training with proper multi-GPU handling.
    
    Args:
        dataset_size: Number of rows in dataset
        feature_count: Number of features
        force_gpu_id: Specific GPU ID to use (overrides auto-selection)
    
    Returns:
        dict: GPU configuration for LightGBM or empty dict for CPU
    """
    if not GPU_AVAILABLE:
        return {}
    
    # Use module-level config as base
    config = GPU_CONFIG.copy()
    
    # Override GPU ID if specified
    if force_gpu_id is not None:
        gpu_details = detect_available_gpus()
        if force_gpu_id < len(gpu_details['gpus']):
            config['gpu_device_id'] = force_gpu_id
            logger.info(f"ðŸŽ¯ Using user-specified GPU {force_gpu_id}")
        else:
            logger.warning(f"âš ï¸  Requested GPU {force_gpu_id} not available, using auto-selected GPU {config['gpu_device_id']}")
    
    # Optimize config based on dataset characteristics
    if dataset_size and feature_count:
        gpu_details = detect_available_gpus()
        if gpu_details['gpus'] and config.get('gpu_device_id', 0) < len(gpu_details['gpus']):
            gpu_memory = gpu_details['gpus'][config['gpu_device_id']]['memory_free_mb']
            optimized_config = get_gpu_config_for_dataset(dataset_size, feature_count, gpu_memory)
            config.update(optimized_config)
            config['gpu_device_id'] = config.get('gpu_device_id', 0)  # Preserve selected GPU
    
    return config

# Log GPU detection results with enhanced multi-GPU information
if GPU_AVAILABLE:
    logger.info(f"âœ… GPU acceleration detected and enabled: {GPU_INFO}")
    if GPU_CONFIG:
        gpu_id = GPU_CONFIG.get('gpu_device_id', 'unknown')
        max_bin = GPU_CONFIG.get('max_bin', 'default')
        logger.info(f"ðŸŽ¯ GPU Configuration: Device {gpu_id}, max_bin={max_bin}")
else:
    logger.info(f"âš ï¸ GPU acceleration disabled, using CPU: {GPU_INFO}")

# Log detected GPU details if available
gpu_details = detect_available_gpus()
if gpu_details['gpus']:
    logger.info(f"ðŸ” GPU Detection Summary: {gpu_details['total_gpus']} GPU(s) found via {gpu_details['detection_method']}")
    for gpu in gpu_details['gpus']:
        logger.info(f"   GPU {gpu['id']}: {gpu['name']} ({gpu['memory_total_mb']:.0f}MB, "
                   f"{gpu['memory_free_mb']:.0f}MB free, {gpu['gpu_utilization']}% util)")
else:
    logger.info("ðŸ” GPU Detection: No GPUs detected on system")

# Log GPU data processing backend information
if HAS_GPU_DATA_UTILS:
    gpu_backend_info = get_gpu_data_backend_info()
    if gpu_backend_info['cudf_available']:
        logger.info(f"ðŸš€ GPU Data Processing: cuDF available (10-150x speedup for large datasets)")
        if 'gpu_memory_mb' in gpu_backend_info and gpu_backend_info['gpu_memory_mb'] != 'unknown':
            logger.info(f"   GPU Memory: {gpu_backend_info['gpu_memory_mb']:.0f}MB total, {gpu_backend_info['gpu_memory_free_mb']:.0f}MB free")
    else:
        logger.info(f"âš¡ GPU Data Processing: cuDF not available, using Polars/pandas fallback")
else:
    logger.info("ðŸ¼ GPU Data Processing: Standard pandas/Polars processing (install cudf for massive speedups)")

# Constants
TARGET_TEMP_COL = 'temperature_grain'
HORIZON_TUPLE = tuple(range(1, 8))  # 1-7 days

def run_complete_pipeline(
    granary_csv: str,  # Individual granary CSV file
    granary_name: str,
    skip_train: bool = False,
    force_retrain: bool = False,
    changed_silos: Optional[List[str]] = None,  # NEW: Track which silos changed
    max_workers: int = 4  # New parameter for parallel processing
) -> dict:
    # Initialize data paths if not provided
    if data_paths is None:
        from .utils.data_paths import data_paths
    """
    Run complete pipeline: preprocess and train for a specific granary CSV.
    
    Parameters:
    -----------
    changed_silos : List[str], optional
        List of silo IDs that have new data. If provided, preprocessing
        will focus on these silos for efficiency, but training will use
        the full granary data.
    """
    # Skip ingestion step since we're working with individual granary CSV
    # Go directly to preprocessing
    # Then training
    results = {
        'granary': granary_name,
        'input_csv': granary_csv,
        'steps_completed': [],
        'errors': [],
        'model_path': None,
        'success': False,
        'changed_silos': changed_silos
    }
    
    try:
        # Step 1: Load full granary data with optimized loading
        logger.info(f"Loading data for granary: {granary_name}")
        df_full = load_data_optimized(granary_csv)
        
        # Step 2: Efficient preprocessing strategy
        if changed_silos and len(changed_silos) < len(df_full['heap_id'].unique()):
            # Only some silos changed - use efficient preprocessing
            logger.info(f"Efficient preprocessing: focusing on {len(changed_silos)} changed silos")
        
            # Load existing processed data if available (supports both CSV and Parquet)
            from .utils.data_paths import data_paths
            processed_dir = data_paths.get_processed_dir()
            processed_path_csv = processed_dir / f"{granary_name}_processed.csv"
            processed_path_parquet = processed_dir / f"{granary_name}_processed.parquet"
            
            if processed_path_parquet.exists():
                # Use Parquet file if available with optimized loading
                df_existing = load_data_optimized(processed_path_parquet)
                logger.info(f"Loaded existing Parquet data: {processed_path_parquet}")
            elif processed_path_csv.exists():
                # Fallback to CSV file if Parquet doesn't exist with optimized loading
                df_existing = load_data_optimized(processed_path_csv)
                logger.info(f"Loaded existing CSV data: {processed_path_csv}")
            else:
                df_existing = None
            
            if df_existing is not None:
                # Remove data for changed silos from existing data
                df_existing_filtered = df_existing[~df_existing['heap_id'].isin(changed_silos)]
                
                # Preprocess only the changed silos
                df_new_silos = df_full[df_full['heap_id'].isin(changed_silos)].copy()
                df_new_processed = _preprocess_silos(df_new_silos)
                
                # Combine existing and new processed data
                df = pd.concat([df_existing_filtered, df_new_processed], ignore_index=True)
                logger.info(f"Combined existing data with {len(changed_silos)} updated silos")
            else:
                # No existing processed data - process everything
                df = _preprocess_silos(df_full)
        else:
            # All silos or no silo info - process everything
            logger.info(f"Full preprocessing: processing all silos")
            df = _preprocess_silos(df_full)
        
        # Save processed data as Parquet (much more efficient for large datasets)
        from granarypredict.ingestion import save_granary_data
        
        processed_output = data_paths.get_processed_dir() / f"{granary_name}_processed"
        processed_output.parent.mkdir(exist_ok=True, parents=True)
        
        # Save as Parquet with snappy compression (60-80% smaller, 10x faster)
        parquet_path = save_granary_data(
            df=df,
            filepath=processed_output,
            format='parquet',
            compression='snappy'
        )
        
        results['steps_completed'].append('preprocess')
        logger.info(f"Preprocessed data saved as Parquet: {parquet_path}")
        logger.info(f"Preprocessed DataFrame shape: {df.shape}")
        
        # Step 3: Train model on FULL granary data (not just changed silos)
        # This ensures the model learns from all silos in the granary
        model_filename = f"{granary_name}_forecast_model.joblib"
        model_path = data_paths.get_models_dir() / model_filename
        
        should_train = (not skip_train and not model_path.exists()) or force_retrain
        
        if should_train:
            logger.info(f"Training model for granary: {granary_name} (using all silos)")
            
            # Prepare training data using the full processed dataset
            df['detection_time'] = pd.to_datetime(df['detection_time'])
            last_date = df['detection_time'].max()
            logger.info(f"Last date in dataset: {last_date}")
            
            from granarypredict.features import select_feature_target_multi
            
            # Phase 1: Create internal 95/5 split for parameter optimization (like Dashboard)
            logger.info("Phase 1: Creating internal 95/5 split for parameter optimization")
            df_sorted = df.sort_values('detection_time', ignore_index=True)
            total_rows = len(df_sorted)
            split_idx = int(total_rows * 0.95)  # Use 95% for training, 5% for validation
            
            # Training data (95%)
            train_df = df_sorted.iloc[:split_idx].copy()
            
            # Validation data (5% - last portion)
            val_df = df_sorted.iloc[split_idx:].copy()
            
            logger.info(f"Training data: {len(train_df)} rows (95%)")
            logger.info(f"Validation data: {len(val_df)} rows (5%)")
            
            # Prepare training features and targets
            X_train, Y_train = select_feature_target_multi(
                df=train_df,
                target_col=TARGET_TEMP_COL,
                horizons=HORIZON_TUPLE,
                allow_na=False
            )
            logger.info(f"X_train shape: {getattr(X_train, 'shape', None)}, Y_train shape: {getattr(Y_train, 'shape', None)}")
            
            # Prepare validation features and targets
            X_val, Y_val = select_feature_target_multi(
                df=val_df,
                target_col=TARGET_TEMP_COL, 
                horizons=HORIZON_TUPLE,
                allow_na=False
            )
            logger.info(f"X_val shape: {getattr(X_val, 'shape', None)}, Y_val shape: {getattr(Y_val, 'shape', None)}")
            
            # Check for empty data
            if X_train is None or Y_train is None or getattr(X_train, 'empty', False) or getattr(Y_train, 'empty', False):
                logger.error(f"Training data is empty for granary: {granary_name}. X_train shape: {getattr(X_train, 'shape', None)}, Y_train shape: {getattr(Y_train, 'shape', None)}")
                results['success'] = False
                results['errors'].append('Training data is empty. Check preprocessing and input data.')
                return results
            if X_val is None or Y_val is None or getattr(X_val, 'empty', False) or getattr(Y_val, 'empty', False):
                logger.error(f"Validation data is empty for granary: {granary_name}. X_val shape: {getattr(X_val, 'shape', None)}, Y_val shape: {getattr(Y_val, 'shape', None)}")
                results['success'] = False
                results['errors'].append('Validation data is empty. Check preprocessing and input data.')
                return results
            
            # Phase 1: Train finder model to get best iteration with Dashboard-optimized settings
            logger.info("Phase 1: Training finder model to determine best iteration")
            from granarypredict.multi_lgbm import MultiLGBMRegressor
            
            # Base parameters - EXACTLY matching Streamlit Dashboard settings with optimal parameters
            base_params = {
                "learning_rate": 0.07172794499286328,  # Optimal parameter
                "max_depth": 20,                       # Optimal parameter
                "num_leaves": 133,                     # Optimal parameter
                "subsample": 0.8901667731353657,       # Optimal parameter
                "colsample_bytree": 0.7729605909501445, # Optimal parameter
                "min_child_samples": 102,              # Optimal parameter
                "lambda_l1": 1.4182488012070926,       # Optimal parameter
                "lambda_l2": 1.7110926238653472,       # Optimal parameter
                "max_bin": 416,                        # Optimal parameter
                "n_jobs": -1,
                # ðŸ†• COMPRESSION OPTIMIZATIONS (40-70% smaller files, no accuracy loss)
                "compress": True,                      # Enable built-in compression
                "compression_level": 6,                # Compression level (1-9, higher = smaller)
                "save_binary": True,                   # Save in binary format (smaller)
            }
            
            # Apply quantile objective (matching Streamlit)
            base_params.update({
                "objective": "quantile",
                "alpha": 0.5,
            })
            
            # Create internal 95/5 split exactly like Streamlit (when no external validation)
            logger.info("Creating internal 95/5 split for tuning (matching Streamlit)")
            train_df, val_df = split_train_eval_frac(df, test_frac=0.05)
            
            X_train, Y_train = features.select_feature_target_multi(
                train_df, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE
            )
            X_val, Y_val = features.select_feature_target_multi(
                val_df, target_col=TARGET_TEMP_COL, horizons=HORIZON_TUPLE
            )
            
            logger.info(f"Internal split sizes â€“ train={len(train_df)}, val={len(val_df)}")
            
            # Get GPU configuration for this training session
            gpu_config = get_gpu_config_for_training(
                dataset_size=len(X_train), 
                feature_count=X_train.shape[1] if hasattr(X_train, 'shape') else None
            )
            
            # Merge GPU config with base parameters
            if gpu_config:
                # Update max_bin from GPU config (it might be optimized for GPU memory)
                if 'max_bin' in gpu_config:
                    base_params['max_bin'] = gpu_config['max_bin']
                # Add other GPU-specific parameters
                base_params.update({k: v for k, v in gpu_config.items() if k != 'max_bin'})
                logger.info(f"ðŸŽ¯ Using GPU {gpu_config.get('gpu_device_id', 'unknown')} with max_bin={gpu_config.get('max_bin', base_params.get('max_bin', 'default'))}")
            
            finder_model = MultiLGBMRegressor(
                base_params=base_params,
                # ðŸš€ OPTIMIZED: Using granarypredict defaults for speed improvements
                # upper_bound_estimators=1000 (default), early_stopping_rounds=50 (default)
                uncertainty_estimation=True,
                n_bootstrap_samples=25,  # ðŸš€ OPTIMIZED: Reduced from 100 for 75% speed improvement
                directional_feature_boost=2.0,  # 2x boost for directional features (matching Dashboard)
                conservative_mode=True,  # Enable conservative predictions
                stability_feature_boost=3.0,  # 3x boost for stability features (matching Dashboard)
                use_gpu=GPU_AVAILABLE,  # Use auto-detected GPU availability
                gpu_optimization=GPU_AVAILABLE  # Enable GPU optimization only if GPU is available
            )
            
            logger.info("Training finder model with anchor-day early stopping and horizon balancing")
            finder_model.fit(
                X=X_train,
                Y=Y_train,
                eval_set=(X_val, Y_val),
                eval_metric="l1",
                verbose=True,
                anchor_df=val_df,  # Pass validation dataframe for anchor-day methodology
                horizon_tuple=HORIZON_TUPLE,
                use_anchor_early_stopping=True,  # Enable anchor-day early stopping
                balance_horizons=True,  # Apply horizon balancing
                horizon_strategy="increasing",  # Increasing horizon importance
            )
            
            best_iteration = finder_model.best_iteration_
            logger.info(f"Phase 1 complete. Best iteration: {best_iteration}")
            
            # Phase 2: Train final model on 100% of data with fixed n_estimators
            logger.info("Phase 2: Training final model on 100% of data with fixed n_estimators")
            
            # Prepare full dataset for final training
            X_full, Y_full = select_feature_target_multi(
                df=df,
                target_col=TARGET_TEMP_COL,
                horizons=HORIZON_TUPLE,
                allow_na=False
            )
            
            final_model = MultiLGBMRegressor(
                base_params=base_params,  # GPU config already merged in base_params
                upper_bound_estimators=best_iteration,  # Use the best iteration from Phase 1
                early_stopping_rounds=0,  # No early stopping for final model
                uncertainty_estimation=True,
                n_bootstrap_samples=25,  # ðŸš€ OPTIMIZED: Reduced from 100 for 75% speed improvement
                directional_feature_boost=2.0,
                conservative_mode=True,
                stability_feature_boost=3.0,
                use_gpu=GPU_AVAILABLE,  # Use auto-detected GPU availability
                gpu_optimization=GPU_AVAILABLE  # Enable GPU optimization only if GPU is available
            )
            
            logger.info("Training final model on 100% of data")
            final_model.fit(
                X=X_full,
                Y=Y_full,
                verbose=True
            )
            
            # Save the final model with adaptive compression
            model_path.parent.mkdir(exist_ok=True, parents=True)
            from granarypredict.compression_utils import save_compressed_model
            
            # Use adaptive compression based on model characteristics
            compression_stats = save_compressed_model(final_model, model_path)
            results['model_path'] = compression_stats['path']
            results['compression_stats'] = compression_stats
            results['steps_completed'].append('train')
            logger.info(f"Final model saved to: {compression_stats['path']}")
            logger.info(f"Compression: {compression_stats['compression_algorithm']} "
                       f"({compression_stats['compression_ratio']:.2f}x, "
                       f"{compression_stats['compressed_size_mb']:.2f} MB)")
        
        results['success'] = True
        return results
        
    except Exception as e:
        logger.error(f"Error in pipeline: {e}")
        results['errors'].append(str(e))
    return results


def _preprocess_silos(df: pd.DataFrame) -> pd.DataFrame:
    """Preprocess a dataframe containing silo data with massive dataset support"""
    
    # Check if this is a massive dataset that needs streaming processing
    dataset_size = len(df)
    memory_threshold = 500_000  # 500K rows threshold for streaming
    
    if dataset_size > memory_threshold:
        logger.info(f"Large dataset detected ({dataset_size:,} rows), using streaming processing")
        return _preprocess_silos_streaming(df)
    else:
        logger.info(f"Standard dataset size ({dataset_size:,} rows), using in-memory processing")
        return _preprocess_silos_memory(df)


def _preprocess_silos_streaming(df: pd.DataFrame) -> pd.DataFrame:
    """Streaming preprocessing for massive datasets"""
    try:
        from granarypredict.streaming_processor import MassiveDatasetProcessor
        
        # Save input data temporarily
        import tempfile
        temp_dir = Path(tempfile.mkdtemp())
        temp_input = temp_dir / "temp_input.parquet"
        temp_output = temp_dir / "temp_output.parquet"
        
        # Save as Parquet for efficient streaming
        df.to_parquet(temp_input, index=False)
        
        # Process with streaming
        processor = MassiveDatasetProcessor(
            chunk_size=100_000,  # 100K row chunks
            backend="polars" if hasattr(processor, '_select_backend') else "pandas"
        )
        
        # Define streaming feature functions
        def streaming_features(chunk_df):
            # Apply all preprocessing steps to chunk
            chunk_df = _apply_basic_preprocessing(chunk_df)
            return chunk_df
        
        success = processor.process_massive_features(
            file_path=temp_input,
            output_path=temp_output,
            feature_functions=[streaming_features]
        )
        
        if success and temp_output.exists():
            result_df = pd.read_parquet(temp_output)
            
            # Cleanup temp files
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
            
            return result_df
        else:
            logger.warning("Streaming processing failed, falling back to chunked in-memory processing")
            return _preprocess_silos_chunked(df)
            
    except ImportError:
        logger.warning("Streaming processor not available, using chunked processing")
        return _preprocess_silos_chunked(df)
    except Exception as e:
        logger.error(f"Streaming processing failed: {e}")
        return _preprocess_silos_chunked(df)


def _preprocess_silos_chunked(df: pd.DataFrame) -> pd.DataFrame:
    """Chunked preprocessing for large datasets that don't fit in memory"""
    chunk_size = 100_000
    processed_chunks = []
    
    logger.info(f"Processing {len(df):,} rows in chunks of {chunk_size:,}")
    
    for i in range(0, len(df), chunk_size):
        end_idx = min(i + chunk_size, len(df))
        chunk = df.iloc[i:end_idx].copy()
        
        logger.info(f"Processing chunk {i//chunk_size + 1}/{(len(df) + chunk_size - 1)//chunk_size}")
        
        # Process chunk
        processed_chunk = _apply_basic_preprocessing(chunk)
        processed_chunks.append(processed_chunk)
        
        # Memory cleanup
        del chunk
        import gc
        gc.collect()
    
    # Combine results
    logger.info("Combining processed chunks...")
    result_df = pd.concat(processed_chunks, ignore_index=True)
    
    # Final cleanup
    del processed_chunks
    import gc
    gc.collect()
    
    return result_df


def _preprocess_silos_memory(df: pd.DataFrame) -> pd.DataFrame:
    """Standard in-memory preprocessing for smaller datasets"""
    return _apply_basic_preprocessing(df)


def _apply_basic_preprocessing(df: pd.DataFrame) -> pd.DataFrame:
    """Apply basic preprocessing steps to a dataframe (chunk or full)"""
    # Apply all preprocessing steps (same as current implementation)
    # 1. Apply comprehensive column standardization
    from granarypredict.ingestion import standardize_granary_csv
    df = standardize_granary_csv(df)
    logger.info(f"Applied column standardization: {list(df.columns)[:5]}...")
    
    # 2. Basic cleaning
    df = cleaning.basic_clean(df)
    
    # 3. Drop redundant columns
    columns_to_drop = ['locatType', 'line_no', 'layer_no']
    existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]
    if existing_columns_to_drop:
        df.drop(columns=existing_columns_to_drop, inplace=True)
        logger.info(f"Dropped redundant columns: {existing_columns_to_drop}")
    
    # 4. Insert calendar gaps (memory-efficient version)
    df = insert_calendar_gaps(df)
    
    # 5. Interpolate missing data (memory-efficient version)
    df = interpolate_sensor_numeric(df)
    
    # 6. Final cleaning
    df = cleaning.fill_missing(df)
    
    # 7. Feature engineering (optimized for large datasets)
    df = create_time_features_optimized(df)
    df = features.create_spatial_features(df)
    df = features.add_time_since_last_measurement(df)
    
    # Use optimized feature engineering for large datasets
    if len(df) > 50_000:  # Changed from 100_000 to 50_000 to match optimized functions
        # Use optimized feature engineering for large datasets
        logger.info(f"*** LARGE DATASET MODE: {len(df):,} rows detected, using optimized Polars feature engineering")
        print(f"*** LARGE DATASET MODE: {len(df):,} rows detected, using optimized Polars feature engineering")
        
        # Disable parallel processing for very large chunks to avoid memory issues
        original_parallel = os.environ.get("SILOFLOW_DISABLE_PARALLEL", "0")
        os.environ["SILOFLOW_DISABLE_PARALLEL"] = "1"
        
        try:
            df = add_lags_optimized(df, lags=[1,2,3,7])  # Memory-efficient lag computation
            df = add_rolling_stats_optimized(df, window_days=7)  # Polars-optimized rolling stats
            df = features.add_directional_features_lean(df)
            df = features.add_stability_features(df)
            df = features.add_horizon_specific_directional_features(df, max_horizon=7)
            df = features.add_multi_horizon_targets(df, horizons=tuple(range(1,8)))
        finally:
            # Restore original parallel setting
            os.environ["SILOFLOW_DISABLE_PARALLEL"] = original_parallel
    else:
        # Standard feature engineering for smaller datasets (with optimized time features)
        logger.info(f"*** STANDARD DATASET MODE: {len(df):,} rows, using standard feature engineering with optimized time features")
        print(f"*** STANDARD DATASET MODE: {len(df):,} rows, using standard feature engineering with optimized time features")
        df = create_time_features_optimized(df)  # Still faster even for small datasets
        df = features.add_multi_lag_parallel(df, lags=(1,2,3,4,5,6,7,14,30))
        df = features.add_rolling_stats_parallel(df, window_days=7)
        df = features.add_directional_features_lean(df)
        df = features.add_stability_features_parallel(df)
        df = features.add_horizon_specific_directional_features(df, max_horizon=7)
        df = features.add_multi_horizon_targets(df, horizons=tuple(range(1,8)))
    
    # 8. Sorting and grouping (using GPU-accelerated versions for maximum performance)
    if HAS_GPU_DATA_UTILS:
        # Use GPU-accelerated data processing for best performance
        df = gpu_assign_group_id_optimized(df, group_columns=['granary_id', 'heap_id'])
        df = gpu_comprehensive_sort_optimized(df, sort_columns=['detection_time'])
        logger.info(f"ðŸš€ GPU-accelerated data processing completed for {len(df):,} rows")
    elif HAS_POLARS_DATA_UTILS:
        # Fallback to Polars optimization
        df = assign_group_id_optimized(df)
        df = comprehensive_sort_optimized(df)
        logger.info(f"âš¡ Polars-optimized data processing completed for {len(df):,} rows")
    else:
        # Final fallback to standard pandas
        df = assign_group_id(df)
        df = comprehensive_sort(df)
        logger.info(f"ðŸ¼ Standard pandas data processing completed for {len(df):,} rows")
    
    # 9. Column reordering (simplified version)
    desired_order = [
        'granary_id','address_cn','heap_id','detection_time','temperature_grain',
        'grid_x','grid_y','grid_z',
        'avg_grain_temp','max_temp','min_temp','temperature_inside','humidity_warehouse',
        'temperature_outside','humidity_outside','warehouse_type'
    ]
    ordered_cols = [c for c in desired_order if c in df.columns]
    remaining_cols = [c for c in df.columns if c not in ordered_cols]
    final_order = ordered_cols + remaining_cols
    df = df[final_order]
    
    return df

# --- CLI Entrypoint ---
def main():
    parser = argparse.ArgumentParser(description="Granary Data Pipeline CLI")
    # Check environment variable that controls subfolder creation behavior
    # Used by batch processing GUI to disable unnecessary folder creation
    no_subfolder_creation = os.environ.get("SILOFLOW_NO_SUBFOLDER_CREATION", "0") == "1"
    
    subparsers = parser.add_subparsers(dest="command")

    ingest_parser = subparsers.add_parser("ingest", help="Ingest and sort raw CSV data")
    ingest_parser.add_argument("--input", required=True, help="Path to raw CSV file")

    preprocess_parser = subparsers.add_parser("preprocess", help="Preprocess granary CSV data")
    preprocess_parser.add_argument("--input", required=True, help="Path to granary CSV file")
    preprocess_parser.add_argument("--output", required=True, help="Path to save processed CSV")

    train_parser = subparsers.add_parser("train", help="Train model for a granary")
    train_parser.add_argument("--granary", required=True, help="Granary name")
    train_parser.add_argument("--tune", action="store_true", default=True, help="Enable Optuna hyperparameter tuning (default: True)")
    train_parser.add_argument("--no-tune", dest="tune", action="store_false", help="Disable Optuna hyperparameter tuning")
    train_parser.add_argument("--trials", type=int, default=100, help="Number of Optuna trials (default: 100)")
    train_parser.add_argument("--timeout", type=int, default=600, help="Optuna timeout in seconds (default: 600)")
    train_parser.add_argument("--gpu", action="store_true", default=False, help="Force enable GPU acceleration (if available)")
    train_parser.add_argument("--gpu-id", type=int, help="Specific GPU ID to use (0, 1, 2, etc.). If not specified, auto-selects best GPU")

    forecast_parser = subparsers.add_parser("forecast", help="Forecast for a granary")
    forecast_parser.add_argument("--granary", required=True, help="Granary name")
    forecast_parser.add_argument("--horizon", type=int, default=7, help="Forecast horizon (days)")
    forecast_parser.add_argument("--gpu", action="store_true", default=False, help="Force enable GPU acceleration (if available)")
    forecast_parser.add_argument("--gpu-id", type=int, help="Specific GPU ID to use (0, 1, 2, etc.). If not specified, auto-selects best GPU")
    pipeline_parser = subparsers.add_parser("pipeline", help="Run complete pipeline: ingest, preprocess, and train")
    pipeline_parser.add_argument("--input", required=True, help="Path to raw CSV file")
    pipeline_parser.add_argument("--granary", required=True, help="Granary name to process")
    pipeline_parser.add_argument("--skip-train", action="store_true", help="Skip training if model already exists")
    pipeline_parser.add_argument("--force-retrain", action="store_true", help="Force retraining even if model exists")

    args = parser.parse_args()

    if args.command == "ingest":
        logger.info(f"Ingesting raw CSV: {args.input}")
        updated_granaries = ingestion.ingest_and_sort(args.input)
        print(f"Ingested and sorted data for granaries: {updated_granaries}")

    elif args.command == "preprocess":
        logger.info(f"Preprocessing granary CSV: {args.input}")
        df = load_data_optimized(args.input)  # Use optimized loading
        # 1. Apply comprehensive column standardization
        from granarypredict.ingestion import standardize_granary_csv
        df = standardize_granary_csv(df)
        logger.info(f"Applied column standardization: {list(df.columns)[:5]}...")
        # 2. Basic cleaning
        df = cleaning.basic_clean(df)
        # 2.5. Drop redundant columns
        columns_to_drop = ['locatType', 'line_no', 'layer_no']
        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]
        if existing_columns_to_drop:
            df.drop(columns=existing_columns_to_drop, inplace=True)
            logger.info(f"Dropped redundant columns: {existing_columns_to_drop}")
        # 3. Insert calendar gaps per sensor
        df = insert_calendar_gaps(df)
        # 4. Interpolate missing numeric data per sensor
        df = interpolate_sensor_numeric(df)
        # 6. Final cleaning
        df = cleaning.fill_missing(df)
        # 7. Feature engineering (using optimized pipeline)
        df = create_time_features_optimized(df)
        df = features.create_spatial_features(df)
        df = features.add_time_since_last_measurement(df)
        
        # Check dataset size and use appropriate feature engineering strategy
        if len(df) > 50_000:  # Changed from 100_000 to 50_000 to match optimized functions
            # Large dataset: use Polars-optimized functions
            logger.info(f"*** LARGE DATASET MODE: {len(df):,} rows, using Polars-optimized feature engineering")
            print(f"*** LARGE DATASET MODE: {len(df):,} rows, using Polars-optimized feature engineering")
            df = add_lags_optimized(df, lags=[1,2,3,7])  # Memory-efficient lag computation
            df = add_rolling_stats_optimized(df, window_days=7)  # Polars-optimized rolling stats
        else:
            # Standard dataset: use optimized time features + standard processing
            logger.info(f"*** STANDARD DATASET MODE: {len(df):,} rows, using standard feature engineering with optimized time features")
            print(f"*** STANDARD DATASET MODE: {len(df):,} rows, using standard feature engineering with optimized time features")
            df = features.add_multi_lag_parallel(df, lags=(1,2,3,4,5,6,7,14,30))
            df = features.add_rolling_stats_parallel(df, window_days=7)
        
        df = features.add_directional_features_lean(df)
        df = features.add_stability_features_parallel(df)
        df = features.add_horizon_specific_directional_features(df, max_horizon=7)
        df = features.add_multi_horizon_targets(df, horizons=tuple(range(1,8)))
        # 8. Sorting and grouping (using GPU-accelerated versions for maximum performance)
        if HAS_GPU_DATA_UTILS:
            # Use GPU-accelerated data processing for best performance
            df = gpu_assign_group_id_optimized(df, group_columns=['granary_id', 'heap_id'])
            df = gpu_comprehensive_sort_optimized(df, sort_columns=['detection_time'])
            logger.info(f"ðŸš€ GPU-accelerated data processing completed for {len(df):,} rows")
        elif HAS_POLARS_DATA_UTILS:
            # Fallback to Polars optimization
            df = assign_group_id_optimized(df)
            df = comprehensive_sort_optimized(df)
            logger.info(f"âš¡ Polars-optimized data processing completed for {len(df):,} rows")
        else:
            # Final fallback to standard pandas
            df = assign_group_id(df)
            df = comprehensive_sort(df)
            logger.info(f"ðŸ¼ Standard pandas data processing completed for {len(df):,} rows")
        # 9. Remove duplicate columns (_x, _y) and reorder to match dashboard
        def clean_and_reorder(df):
            # Column standardization is already applied at the beginning of preprocessing
            # This function now only handles column reordering
            
            # Handle any remaining duplicate columns that end with _x or _y
            # but are not the essential grid coordinates
            columns_to_drop = []
            for col in df.columns:
                if col.endswith('_x') or col.endswith('_y'):
                    base = col[:-2]
                    # Keep grid_x and grid_y, drop other duplicates
                    if base in df.columns and base not in ['grid_x', 'grid_y']:
                        columns_to_drop.append(col)
                    elif base not in df.columns and col not in ['grid_x', 'grid_y']:
                        # Rename if it's not a duplicate and not grid coordinates
                        df.rename(columns={col: base}, inplace=True)
            
            # Drop the identified duplicate columns
            if columns_to_drop:
                df.drop(columns=columns_to_drop, inplace=True)
            
            # Define the desired column order
            desired_order = [
                'granary_id','address_cn','heap_id','detection_time','temperature_grain',
                'grid_x','grid_y','grid_z',
                'avg_grain_temp','max_temp','min_temp','temperature_inside','humidity_warehouse','temperature_outside','humidity_outside','warehouse_type','year','month','day','hour','month_sin','month_cos','hour_sin','hour_cos','doy','weekofyear','doy_sin','doy_cos','woy_sin','woy_cos','is_weekend','hours_since_last_measurement','lag_temp_1d','lag_temp_2d','lag_temp_3d','lag_temp_4d','lag_temp_5d','lag_temp_6d','lag_temp_7d','lag_temp_14d','lag_temp_30d','delta_temp_1d','delta_temp_2d','delta_temp_3d','delta_temp_4d','delta_temp_5d','delta_temp_6d','delta_temp_7d','delta_temp_14d','delta_temp_30d','roll_mean_7d','roll_std_7d','temp_accel','trend_3d','is_warming','velocity_smooth','trend_consistency','stability_index','thermal_inertia','change_resistance','historical_stability','dampening_factor','equilibrium_temp','temp_deviation_from_equilibrium','mean_reversion_tendency','velocity_1d','velocity_3d','velocity_7d','momentum_strength','momentum_direction','temp_volatility','velocity_volatility','temp_acceleration_3d','trend_reversal_signal','direction_consistency_2d','direction_consistency_3d','direction_consistency_5d','direction_consistency_7d','temp_range_7d','temp_position_in_range','temperature_grain_h1d','temperature_grain_h2d','temperature_grain_h3d','temperature_grain_h4d','temperature_grain_h5d','temperature_grain_h6d','temperature_grain_h7d','_group_id'
            ]
            
            # Reorder columns: first the desired order, then any remaining columns
            ordered_cols = [c for c in desired_order if c in df.columns]
            remaining_cols = [c for c in df.columns if c not in ordered_cols]
            final_order = ordered_cols + remaining_cols
            
            df = df[final_order]
            return df
        df = clean_and_reorder(df)
        
        # Save in the appropriate format based on file extension
        output_path = Path(args.output)
        if output_path.suffix.lower() == '.parquet':
            df.to_parquet(args.output, index=False)
            print(f"Preprocessed Parquet file saved to {args.output}")
        else:
            df.to_csv(args.output, index=False)
            print(f"Preprocessed CSV saved to {args.output}")

    elif args.command == "train":
        logger.info(f"Training model for granary: {args.granary}")
        
        # Handle GPU override if requested by user
        if args.gpu:
            logger.info("ðŸš€ User requested GPU acceleration - attempting to enable...")
            preferred_gpu_id = getattr(args, 'gpu_id', None)
            if preferred_gpu_id is not None:
                logger.info(f"ðŸŽ¯ User specified GPU ID: {preferred_gpu_id}")
            GPU_AVAILABLE_OVERRIDE, GPU_INFO_OVERRIDE, GPU_CONFIG_OVERRIDE = detect_gpu_availability(
                force_enable=True, preferred_gpu_id=preferred_gpu_id
            )
            if GPU_AVAILABLE_OVERRIDE:
                logger.info(f"âœ… {GPU_INFO_OVERRIDE}")
            else:
                logger.warning(f"âš ï¸ {GPU_INFO_OVERRIDE}")
        else:
            # Use the default GPU detection (disabled)
            GPU_AVAILABLE_OVERRIDE, GPU_INFO_OVERRIDE, GPU_CONFIG_OVERRIDE = GPU_AVAILABLE, GPU_INFO, GPU_CONFIG
            logger.info(f"ðŸ’» Using default GPU setting: {GPU_INFO_OVERRIDE}")
        
        # Find the processed file for this granary (supports both CSV and Parquet)
        processed_file = None
        # Use centralized data paths if available
        try:
            from .utils.data_paths import data_paths
            processed_dir = data_paths.get_processed_dir()
        except ImportError:
            processed_dir = pathlib.Path("data/processed")
        if processed_dir.exists():
            # Try Parquet first (preferred format)
            for file in processed_dir.glob(f"*{args.granary}*.parquet"):
                processed_file = file
                break
            # Fallback to CSV
            if not processed_file:
                for file in processed_dir.glob(f"*{args.granary}*.csv"):
                    processed_file = file
                    break
        
        if not processed_file:
            # Try granaries directory as fallback
            granaries_dir = pathlib.Path("data/granaries")
            if granaries_dir.exists():
                for file in granaries_dir.glob(f"*{args.granary}*.csv"):
                    processed_file = file
                    break
        
        if not processed_file:
            print(f"Error: No processed data found for granary '{args.granary}'")
            print("Please run preprocessing first: python granary_pipeline.py preprocess --input <granary.csv> --output <processed.csv>")
            return
        
        logger.info(f"Loading processed data from: {processed_file}")
        df: pd.DataFrame = ingestion.read_granary_csv(processed_file)
        
        # Prepare features and targets for multi-horizon training
        from granarypredict.features import select_feature_target_multi
        
        # Get the last date for evaluation
        df['detection_time'] = pd.to_datetime(df['detection_time'])
        last_date = df['detection_time'].max()
        logger.info(f"Last date in dataset: {last_date}")
        
        # Create anchor dataset using a proper train/test split approach
        # This mimics the Streamlit dashboard approach
        
        # Create a 95/5 split for anchor evaluation (similar to Streamlit)
        df_sorted = df.sort_values('detection_time')
        total_rows = len(df_sorted)
        split_idx = int(total_rows * 0.95)
        
        # Training data (95%)
        train_df = df_sorted.iloc[:split_idx].copy()
        
        # Anchor/validation data (5% - last portion)
        anchor_df = df_sorted.iloc[split_idx:].copy()
        
        print(f"Training data: {len(train_df)} rows (95%)")
        print(f"Anchor data: {len(anchor_df)} rows (5%)")
        
        # Prepare training features and targets
        train_X, train_Y = select_feature_target_multi(
            df=train_df,
            target_col=TARGET_TEMP_COL,
            horizons=HORIZON_TUPLE,
            allow_na=False
        )
        
        # Prepare anchor features and targets
        anchor_X, anchor_Y = select_feature_target_multi(
            df=anchor_df,
            target_col=TARGET_TEMP_COL, 
            horizons=HORIZON_TUPLE,
            allow_na=False
        )
        
        if train_X.empty or train_Y.empty:
            print(f"Error: No valid training data after feature selection")
            print(f"Training dataframe shape: {train_df.shape}")
            return
            
        if anchor_X.empty or anchor_Y.empty:
            print(f"Error: No valid anchor data for evaluation")
            print(f"Anchor dataframe shape: {anchor_df.shape}")
            print(f"Available columns: {list(anchor_df.columns)}")
            return
        
        # Use the training data we prepared above
        X, Y = train_X, train_Y
        
        logger.info(f"Training data shape: X={X.shape}, Y={Y.shape}")
        logger.info(f"Anchor data shape: X={anchor_X.shape}, Y={anchor_Y.shape}")
        
        # Initialize MultiLGBM model with optional Optuna hyperparameter tuning
        from granarypredict.multi_lgbm import MultiLGBMRegressor
        
        if args.tune:
            # âš¡ EXTREMELY LARGE DATASET OPTIMIZATION âš¡
            # Detect dataset size and optimize Optuna accordingly
            dataset_size = len(X)
            feature_count = X.shape[1] if hasattr(X, 'shape') else len(X.columns)
            
            # Adaptive settings based on dataset size
            if dataset_size > 500_000:
                # MASSIVE DATASET (>500K rows): Ultra-fast optimization
                optimized_trials = min(args.trials, 15)  # Max 15 trials for massive datasets
                optimized_timeout = min(args.timeout, 300)  # Max 5 minutes
                early_stopping_rounds = 25  # Very aggressive early stopping
                n_bootstrap_samples = 10   # Minimal bootstrap for speed
                max_estimators = 500       # Fewer estimators per trial
                pruning_patience = 3       # Very aggressive pruning
                logger.info(f"ï¿½ MASSIVE DATASET MODE: {dataset_size:,} rows â†’ Ultra-fast optimization")
                logger.info(f"   â†’ {optimized_trials} trials (reduced from {args.trials})")
                logger.info(f"   â†’ {optimized_timeout}s timeout (reduced from {args.timeout})")
                
            elif dataset_size > 200_000:
                # LARGE DATASET (200K-500K rows): Fast optimization
                optimized_trials = min(args.trials, 25)  # Max 25 trials
                optimized_timeout = min(args.timeout, 600)  # Max 10 minutes
                early_stopping_rounds = 35
                n_bootstrap_samples = 15
                max_estimators = 750
                pruning_patience = 5
                logger.info(f"ðŸš€ LARGE DATASET MODE: {dataset_size:,} rows â†’ Fast optimization")
                logger.info(f"   â†’ {optimized_trials} trials (reduced from {args.trials})")
                logger.info(f"   â†’ {optimized_timeout}s timeout (reduced from {args.timeout})")
                
            elif dataset_size > 100_000:
                # MEDIUM DATASET (100K-200K rows): Balanced optimization
                optimized_trials = min(args.trials, 40)
                optimized_timeout = min(args.timeout, 900)  # Max 15 minutes
                early_stopping_rounds = 50
                n_bootstrap_samples = 25
                max_estimators = 1000
                pruning_patience = 7
                logger.info(f"âš¡ MEDIUM DATASET MODE: {dataset_size:,} rows â†’ Balanced optimization")
                
            else:
                # STANDARD DATASET (<100K rows): Full optimization
                optimized_trials = args.trials
                optimized_timeout = args.timeout
                early_stopping_rounds = 100
                n_bootstrap_samples = 50
                max_estimators = 2000
                pruning_patience = 10
                logger.info(f"ðŸ“Š STANDARD DATASET MODE: {dataset_size:,} rows â†’ Full optimization")
            
            logger.info("ðŸ” Starting OPTIMIZED Optuna hyperparameter tuning...")
            logger.info(f"ðŸŽ¯ Adaptive settings: {optimized_trials} trials, {optimized_timeout}s timeout")
            logger.info(f"ðŸŽ¯ Early stopping: {early_stopping_rounds} rounds, Bootstrap samples: {n_bootstrap_samples}")
            
            # OPTIMIZED PARAMETER SPACE: Focus on high-impact parameters for large datasets
            if dataset_size > 200_000:
                # Reduced parameter space for large datasets - focus on essentials
                optuna_param_space = {
                    'learning_rate': ('float', 0.05, 0.12),      # Narrower range, higher learning rates for speed
                    'max_depth': ('int', 8, 15),                 # Moderate depths to avoid overfitting
                    'num_leaves': ('int', 64, 128),              # Balanced range for large data
                    'subsample': ('float', 0.7, 0.9),            # Higher sampling for stability
                    'colsample_bytree': ('float', 0.7, 0.9),     # Higher feature sampling
                    'min_child_samples': ('int', 50, 100),       # Higher minimums for large data
                    'lambda_l1': ('float', 0.1, 1.0),            # Moderate regularization
                    'lambda_l2': ('float', 0.1, 1.0),            # Moderate regularization
                }
                logger.info("ðŸ“‰ Using REDUCED parameter space optimized for large datasets")
            else:
                # Full parameter space for smaller datasets
                optuna_param_space = {
                    'learning_rate': ('float', 0.01, 0.15),
                    'max_depth': ('int', 6, 25),
                    'num_leaves': ('int', 31, 200),
                    'subsample': ('float', 0.6, 0.95),
                    'colsample_bytree': ('float', 0.6, 0.95),
                    'min_child_samples': ('int', 10, 150),
                    'lambda_l1': ('float', 0.01, 2.0),
                    'lambda_l2': ('float', 0.01, 2.0),
                    'max_bin': ('int', 200, 500),
                }
                logger.info("ðŸ“Š Using FULL parameter space for standard datasets")
            
            # Use Optuna to find optimal parameters for this specific granary
            model = MultiLGBMRegressor(
                # Let Optuna find the best parameters instead of using fixed ones
                base_params={
                    "objective": "quantile",
                    "alpha": 0.5,
                    "n_jobs": -1,
                    # LARGE DATASET OPTIMIZATIONS
                    "max_bin": 255 if dataset_size > 200_000 else 511,  # Reduce bins for large datasets
                    "min_data_in_leaf": 50 if dataset_size > 200_000 else 20,  # Higher minimum for stability
                },
                # ADAPTIVE Optuna tuning settings
                optuna_trials=optimized_trials,
                optuna_timeout=optimized_timeout,
                optuna_sampler="TPE",  # Tree-structured Parzen Estimator (best for continuous parameters)
                optuna_pruner="HyperbandPruner",  # More aggressive pruning than MedianPruner
                optuna_study_name=f"granary_{args.granary}_optimized_tuning",
                optuna_storage=None,  # In-memory storage (can be changed to persistent storage later)
                
                # ADAPTIVE Model settings based on dataset size
                upper_bound_estimators=max_estimators,
                early_stopping_rounds=early_stopping_rounds,
                uncertainty_estimation=True,
                n_bootstrap_samples=n_bootstrap_samples,  # Adaptive bootstrap samples
                directional_feature_boost=2.0,  # 2x boost for directional features (matching Dashboard)
                conservative_mode=True,  # Enable conservative predictions
                stability_feature_boost=3.0,  # 3x boost for stability features (matching Dashboard)
                use_gpu=GPU_AVAILABLE_OVERRIDE,  # Use user-specified or auto-detected GPU availability
                gpu_optimization=GPU_AVAILABLE_OVERRIDE,  # Enable GPU optimization based on user choice
                
                # OPTIMIZED parameter search space
                optuna_param_space=optuna_param_space,
                
                # ADDITIONAL OPTIMIZATIONS for large datasets
                optuna_pruner_patience=pruning_patience,  # How many poor trials before pruning
                optuna_n_startup_trials=5,  # Reduce startup trials for faster convergence
                optuna_n_warmup_steps=3,   # Fewer warmup steps for speed
            )
            
            logger.info("ðŸš€ Training MultiLGBM model with OPTIMIZED Optuna...")
            
            # SMART DATA SAMPLING for extremely large datasets during Optuna tuning
            if dataset_size > 300_000:
                # Use smart sampling to speed up Optuna trials
                logger.info("ðŸŽ¯ LARGE DATASET DETECTED: Using smart sampling for Optuna trials")
                
                # Sample stratified by time to maintain temporal patterns
                sample_fraction = min(0.3, 100_000 / dataset_size)  # Cap at 100K samples or 30%
                sample_size = int(dataset_size * sample_fraction)
                
                # Stratified sampling: keep recent data (more important) + random historical
                recent_fraction = 0.7  # 70% from recent data
                historical_fraction = 0.3  # 30% from historical data
                
                # Sort by time
                if 'detection_time' in train_df.columns:
                    train_df_sorted = train_df.sort_values('detection_time')
                    
                    # Recent data (last 30% of time range)
                    recent_size = int(sample_size * recent_fraction)
                    recent_data = train_df_sorted.tail(int(len(train_df_sorted) * 0.3))
                    if len(recent_data) > recent_size:
                        recent_sample = recent_data.sample(n=recent_size, random_state=42)
                    else:
                        recent_sample = recent_data
                    
                    # Historical data (random from first 70% of time range)
                    historical_size = sample_size - len(recent_sample)
                    historical_data = train_df_sorted.head(int(len(train_df_sorted) * 0.7))
                    if len(historical_data) > historical_size and historical_size > 0:
                        historical_sample = historical_data.sample(n=historical_size, random_state=42)
                    else:
                        historical_sample = historical_data.head(historical_size) if historical_size > 0 else pd.DataFrame()
                    
                    # Combine samples
                    if not historical_sample.empty:
                        optuna_train_df = pd.concat([recent_sample, historical_sample], ignore_index=True)
                    else:
                        optuna_train_df = recent_sample
                        
                    logger.info(f"ðŸ“Š Smart sampling: {len(optuna_train_df):,} samples ({sample_fraction:.1%}) from {dataset_size:,} rows")
                    logger.info(f"   â†’ Recent data: {len(recent_sample):,} samples")
                    logger.info(f"   â†’ Historical data: {len(historical_sample):,} samples")
                else:
                    # Fallback to random sampling if no time column
                    optuna_train_df = train_df.sample(n=sample_size, random_state=42)
                    logger.info(f"ðŸ“Š Random sampling: {sample_size:,} samples ({sample_fraction:.1%}) from {dataset_size:,} rows")
                
                # Prepare sampled features and targets for Optuna
                X_optuna, Y_optuna = select_feature_target_multi(
                    df=optuna_train_df,
                    target_col=TARGET_TEMP_COL,
                    horizons=HORIZON_TUPLE,
                    allow_na=False
                )
                
                logger.info(f"ðŸŽ¯ Optuna training data: X={X_optuna.shape}, Y={Y_optuna.shape}")
                
                # Use sampled data for Optuna hyperparameter search
                optuna_X, optuna_Y = X_optuna, Y_optuna
            else:
                # Use full dataset for smaller datasets
                logger.info("ðŸ“Š Using full dataset for Optuna tuning (dataset not extremely large)")
                optuna_X, optuna_Y = X, Y
        
        else:
            logger.info("âš¡ Using fixed parameters (no Optuna tuning)...")
            
            # Use fixed parameters similar to Dashboard defaults
            model = MultiLGBMRegressor(
                base_params={
                    "objective": "quantile",
                    "alpha": 0.5,
                    "learning_rate": 0.05,
                    "max_depth": 8,
                    "num_leaves": 64,
                    "subsample": 0.8,
                    "colsample_bytree": 0.8,
                    "min_child_samples": 20,
                    "lambda_l1": 0.1,
                    "lambda_l2": 0.1,
                    "max_bin": 255,
                    "n_jobs": -1,
                },
                upper_bound_estimators=2000,
                early_stopping_rounds=100,
                uncertainty_estimation=True,
                n_bootstrap_samples=50,
                directional_feature_boost=2.0,
                conservative_mode=True,
                stability_feature_boost=3.0,
                use_gpu=GPU_AVAILABLE,  # Use auto-detected GPU availability
                gpu_optimization=GPU_AVAILABLE  # Enable GPU optimization only if GPU is available
            )
        
        # Train the model with or without Optuna hyperparameter optimization
        if args.tune:
            # PHASE 1: Optuna hyperparameter search (using sampled data for speed)
            logger.info("ðŸ” PHASE 1: Optuna hyperparameter search on sampled data...")
            model.fit(
                X=optuna_X,  # Use sampled data for Optuna
                Y=optuna_Y,  # Use sampled data for Optuna
                eval_set=(anchor_X, anchor_Y),  # Use anchor data for validation
                eval_metric="l1",
                verbose=True,
                anchor_df=anchor_df,  # Pass anchor dataframe for anchor-day methodology
                horizon_tuple=HORIZON_TUPLE,
                use_anchor_early_stopping=True,  # Enable anchor-day early stopping
                balance_horizons=True,  # Apply horizon balancing
                horizon_strategy="increasing",  # Increasing horizon importance
            )
        else:
            # No tuning: train directly on full data
            logger.info("âš¡ Training with fixed parameters on full dataset...")
            model.fit(
                X=X,
                Y=Y,
                eval_set=(anchor_X, anchor_Y),  # Use anchor data for validation
                eval_metric="l1",
                verbose=True,
                anchor_df=anchor_df,  # Pass anchor dataframe for anchor-day methodology
                horizon_tuple=HORIZON_TUPLE,
                use_anchor_early_stopping=True,  # Enable anchor-day early stopping
                balance_horizons=True,  # Apply horizon balancing
                horizon_strategy="increasing",  # Increasing horizon importance
            )
        
        # Log the results of hyperparameter tuning if enabled
        if args.tune:
            if hasattr(model, 'best_params_'):
                logger.info(f"ðŸŽ¯ Best parameters found by Optuna: {model.best_params_}")
                logger.info(f"ðŸ† Best validation score: {model.best_score_:.4f}")
            
            # Log Optuna study statistics
            if hasattr(model, 'optuna_study_') and model.optuna_study_ is not None:
                study = model.optuna_study_
                completed_trials = len([t for t in study.trials if t.state.name == 'COMPLETE'])
                logger.info(f"ðŸ“Š Optuna completed {completed_trials} successful trials out of {len(study.trials)} total")
                logger.info(f"ðŸ“ˆ Best trial number: {study.best_trial.number}")
                
                # Show parameter importance if available
                try:
                    importance = study.get_param_importances()
                    logger.info("ðŸ” Parameter importance (top 3):")
                    for param, importance_val in sorted(importance.items(), key=lambda x: x[1], reverse=True)[:3]:
                        logger.info(f"   {param}: {importance_val:.3f}")
                except Exception as e:
                    logger.warning(f"Could not get parameter importance: {e}")
            
            # PHASE 2: Train final model on FULL dataset with optimized parameters
            logger.info("ðŸ PHASE 2: Training final model on FULL dataset with optimized parameters...")
            
            # Get the best parameters found by Optuna
            if hasattr(model, 'best_params_') and model.best_params_:
                best_params = model.best_params_.copy()
                best_iteration = model.best_iteration_ if hasattr(model, 'best_iteration_') else max_estimators
                logger.info(f"ðŸŽ¯ Using optimized parameters: {best_params}")
                logger.info(f"ï¿½ Using optimized iteration count: {best_iteration}")
            else:
                # Fallback to reasonable defaults if Optuna failed
                logger.warning("âš ï¸ No optimal parameters found, using fallback defaults")
                best_params = {
                    "learning_rate": 0.08,
                    "max_depth": 10,
                    "num_leaves": 80,
                    "subsample": 0.85,
                    "colsample_bytree": 0.85,
                    "min_child_samples": 30,
                    "lambda_l1": 0.5,
                    "lambda_l2": 0.5,
                }
                best_iteration = 800  # Conservative default
            
            # Create final model with optimized parameters
            final_params = {
                "objective": "quantile",
                "alpha": 0.5,
                "n_jobs": -1,
                "max_bin": 255 if dataset_size > 200_000 else 511,
                "min_data_in_leaf": 50 if dataset_size > 200_000 else 20,
                "n_estimators": best_iteration,
            }
            final_params.update(best_params)
            
            logger.info(f"ðŸŽ¯ Final model parameters: {final_params}")
            
            final_model = MultiLGBMRegressor(
                base_params=final_params,
                upper_bound_estimators=best_iteration,
                early_stopping_rounds=0,  # No early stopping for final model
                uncertainty_estimation=True,
                n_bootstrap_samples=n_bootstrap_samples//2,  # Reduce for final model speed
                directional_feature_boost=2.0,
                conservative_mode=True,
                stability_feature_boost=3.0,
                use_gpu=GPU_AVAILABLE_OVERRIDE,
                gpu_optimization=GPU_AVAILABLE_OVERRIDE
            )
            
            # Train final model on FULL dataset
            logger.info(f"ðŸš€ Training final model on FULL dataset ({len(X):,} rows)...")
            final_model.fit(
                X=X,  # Use FULL dataset
                Y=Y,  # Use FULL dataset
                eval_set=None,  # No validation set for final model
                eval_metric="l1",
                verbose=True,
                balance_horizons=True,
                horizon_strategy="increasing",
            )
            
            # Use final model for predictions and saving
            model = final_model
            logger.info("âœ… Final model training completed on full dataset")
        else:
            logger.info("âš¡ Model trained with fixed parameters on full dataset")
        
        # Prepare full dataset features and targets for evaluation
        full_X, full_Y = select_feature_target_multi(
            df=df.copy(),
            target_col=TARGET_TEMP_COL,
            horizons=HORIZON_TUPLE,
            allow_na=False
        )
        
        # Generate predictions for the anchor data to calculate metrics
        logger.info("Generating forecasts for anchor validation data...")
        predictions = model.predict(anchor_X)
        
        # Calculate metrics on the anchor data
        from sklearn.metrics import mean_absolute_error
        mae_scores = []
        for h in range(7):
            if h < predictions.shape[1] and h < anchor_Y.shape[1]:
                mae = mean_absolute_error(anchor_Y.iloc[:, h], predictions[:, h])
                mae_scores.append(mae)
                logger.info(f"h+{h+1} MAE: {mae:.3f}Â°C")
        
        avg_mae = sum(mae_scores) / len(mae_scores) if mae_scores else 0
        logger.info(f"Average MAE across horizons: {avg_mae:.3f}Â°C")
        
        # Generate predictions for the anchor data to calculate metrics
        logger.info("Generating forecasts for anchor validation data...")
        predictions = model.predict(anchor_X)
        
        # Calculate metrics on the anchor data
        from sklearn.metrics import mean_absolute_error
        mae_scores = []
        for h in range(7):
            if h < predictions.shape[1] and h < anchor_Y.shape[1]:
                mae = mean_absolute_error(anchor_Y.iloc[:, h], predictions[:, h])
                mae_scores.append(mae)
                logger.info(f"h+{h+1} MAE: {mae:.3f}Â°C")
        
        avg_mae = sum(mae_scores) / len(mae_scores) if mae_scores else 0
        logger.info(f"Average MAE across horizons: {avg_mae:.3f}Â°C")
        
        # Save the model with adaptive compression
        from granarypredict.compression_utils import save_compressed_model
        model_filename = f"{args.granary}_forecast_model.joblib"
        
        # Check if subfolder creation is disabled (controlled by batch processing GUI)
        # Always save to the standard models directory when no_subfolder_creation is True
        model_path = pathlib.Path("models") / model_filename  # Standard models directory
        
        compression_stats = save_compressed_model(model, model_path)
        logger.info(f"Model saved to: {compression_stats['path']}")
        logger.info(f"Compression: {compression_stats['compression_algorithm']} "
                   f"({compression_stats['compression_ratio']:.2f}x, "
                   f"{compression_stats['compressed_size_mb']:.2f} MB)")
        
        # Save forecast results
        forecast_results = {
            'granary': args.granary,
            'last_date': last_date.isoformat(),
            'mae_scores': mae_scores,
            'avg_mae': avg_mae,
            'predictions_shape': predictions.shape,
            'model_path': str(model_path)
        }
        
        results_filename = f"{args.granary}_training_results.json"
        results_path = pathlib.Path("model_results") / results_filename  # Changed from pipeline/model_results to model_results
        
        import json
        with open(results_path, 'w') as f:
            json.dump(forecast_results, f, indent=2, default=str)
        
        print(f"âœ… Training completed successfully!")
        print(f"ðŸ“ Model saved to: {model_path}")
        print(f"Results saved to: {results_path}")
        print(f"Average MAE: {avg_mae:.3f}Â°C")
        print(f"ðŸ“… Training data: {len(train_df)} rows, Validation: {len(anchor_df)} rows")
        print(f"ðŸ”® Forecast horizons: h+1 to h+7")

    elif args.command == "forecast":
        logger.info(f"Forecasting for granary: {args.granary}")
        
        # Handle GPU override if requested by user
        if args.gpu:
            logger.info("ðŸš€ User requested GPU acceleration for forecasting - attempting to enable...")
            preferred_gpu_id = getattr(args, 'gpu_id', None)
            if preferred_gpu_id is not None:
                logger.info(f"ðŸŽ¯ User specified GPU ID: {preferred_gpu_id}")
            GPU_AVAILABLE_OVERRIDE, GPU_INFO_OVERRIDE, GPU_CONFIG_OVERRIDE = detect_gpu_availability(
                force_enable=True, preferred_gpu_id=preferred_gpu_id
            )
            if GPU_AVAILABLE_OVERRIDE:
                logger.info(f"âœ… {GPU_INFO_OVERRIDE}")
            else:
                logger.warning(f"âš ï¸ {GPU_INFO_OVERRIDE}")
        else:
            # Use the default GPU detection (disabled)
            GPU_AVAILABLE_OVERRIDE, GPU_INFO_OVERRIDE, GPU_CONFIG_OVERRIDE = GPU_AVAILABLE, GPU_INFO, GPU_CONFIG
            logger.info(f"ðŸ’» Using default GPU setting: {GPU_INFO_OVERRIDE}")
        
        # Find the trained model
        model_filename = f"{args.granary}_forecast_model.joblib"
        model_path = pathlib.Path("models") / model_filename  # Changed from pipeline/models to models
        
        if not model_path.exists():
            print(f"Error: No trained model found for granary '{args.granary}'")
            print(f"Expected model file: {model_path}")
            print("Please run training first: python granary_pipeline.py train --granary <granary_name>")
            return
        
        # Load the trained model with adaptive compression support
        from granarypredict.compression_utils import load_compressed_model
        import joblib
        
        try:
            # Use new adaptive loading system
            model = load_compressed_model(model_path)
            logger.info(f"Loaded model using adaptive compression from: {model_path}")
        except Exception as e:
            logger.warning(f"Adaptive loading failed, trying fallback: {e}")
            # Fallback to regular joblib loading
            model = joblib.load(model_path)
            logger.info(f"Loaded model using fallback from: {model_path}")
        
        # Find the processed data file for this granary (supports both CSV and Parquet)
        processed_file = None
        processed_dir = pathlib.Path("data/processed")
        if processed_dir.exists():
            # Try Parquet first (preferred format)
            parquet_files = list(processed_dir.glob(f"*{args.granary}*.parquet"))
            if parquet_files:
                processed_file = parquet_files[0]
                logger.info(f"Found Parquet file: {processed_file}")
            else:
                # Fallback to CSV files
                csv_files = list(processed_dir.glob(f"*{args.granary}*.csv*"))
                if csv_files:
                    processed_file = csv_files[0]
                    logger.info(f"Found CSV file: {processed_file}")
        
        if not processed_file:
            # Try granaries directory as fallback
            granaries_dir = pathlib.Path("data/granaries")
            if granaries_dir.exists():
                for file in granaries_dir.glob(f"*{args.granary}*.parquet"):
                    processed_file = file
                    break
        
        if not processed_file:
            print(f"Error: No processed data found for granary '{args.granary}'")
            print("Please run preprocessing first: python granary_pipeline.py preprocess --input <granary.csv> --output <processed.csv>")
            return
        
        logger.info(f"Loading processed data from: {processed_file}")
        df_processed: pd.DataFrame = ingestion.read_granary_csv(processed_file)
        
        # Get the last date for forecasting
        df_processed['detection_time'] = pd.to_datetime(df_processed['detection_time'])
        last_date = df_processed['detection_time'].max()
        logger.info(f"Last date in dataset: {last_date}")
        
        # Create future data for forecasting using Dashboard approach
        def make_future_dashboard_style(df: pd.DataFrame, horizon_days: int) -> pd.DataFrame:
            """Create future dataframe for forecasting using Dashboard approach."""
            last_date = pd.to_datetime(df['detection_time'].max())
            logger.info(f"Using only the most recent date for forecasting: {last_date}")
            
            # Get the last known row per physical sensor (like Dashboard)
            sensors_key = [c for c in ["granary_id", "heap_id", "grid_x", "grid_y", "grid_z"] if c in df.columns]
            last_rows = (
                df.sort_values("detection_time")
                .groupby(sensors_key, dropna=False)
                .tail(1)
                .copy()
            )
            logger.info(f"Found {len(last_rows)} sensors at the most recent date")
            
            # Create future dates
            future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=horizon_days, freq='D')
            
            # Build future frames for each horizon (like Dashboard)
            all_future_frames = []
            for h in range(1, horizon_days + 1):
                day_frame = last_rows.copy()
                day_frame["detection_time"] = last_date + pd.Timedelta(days=h)
                day_frame["forecast_day"] = h
                
                # Clear target variables
                for col in df.columns:
                    if col.startswith('temperature_grain_h') or col == 'temperature_grain':
                        day_frame[col] = np.nan
                
                all_future_frames.append(day_frame)
            
            # Create future dataframe with only the future rows
            future_df = pd.concat(all_future_frames, ignore_index=True)
            logger.info(f"Created future dataframe with {len(future_df)} rows for {horizon_days} days")
            
            return future_df
        
        # Create future dataframe with the specified horizon (Dashboard style)
        future_df = make_future_dashboard_style(df_processed, horizon_days=args.horizon)
        logger.info(f"Created future dataframe with shape: {future_df.shape}")
        
        # Prepare features for forecasting
        from granarypredict.features import select_feature_target_multi
        
        # Select features (no targets for future data)
        X_future, _ = select_feature_target_multi(
            df=future_df.copy(),
            target_col="temperature_grain",
            horizons=(1, 2, 3, 4, 5, 6, 7),
            allow_na=True  # Allow missing targets for future data
        )
        
        logger.info(f"Future features shape: {X_future.shape}")
        
        # Generate forecasts using multi-output approach (like Dashboard)
        logger.info(f"Generating forecasts for h+1 to h+{args.horizon}...")
        predictions = model.predict(X_future)
        
        # Get uncertainty estimates if available
        uncertainties = None
        if hasattr(model, 'get_uncertainty_estimates'):
            uncertainties = model.get_uncertainty_estimates()
        
        # Get prediction intervals if available
        prediction_intervals = None
        if hasattr(model, 'get_prediction_intervals'):
            prediction_intervals = model.get_prediction_intervals(confidence_level=0.95)
        
        # Create forecast results
        forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=args.horizon, freq='D')
        
        forecast_results = {
            'granary': args.granary,
            'last_historical_date': last_date.isoformat(),
            'forecast_dates': [d.isoformat() for d in forecast_dates],
            'horizon_days': args.horizon,
            'predictions': predictions.tolist() if predictions is not None else None,
            'uncertainties': uncertainties.tolist() if uncertainties is not None else None,
            'prediction_intervals': {
                'lower': prediction_intervals[0].tolist() if prediction_intervals else None,
                'upper': prediction_intervals[1].tolist() if prediction_intervals else None
            } if prediction_intervals else None
        }
        
        # Create comprehensive CSV forecast with sensor coordinates and all metadata (Dashboard style)
        if predictions is not None:
            # Create comprehensive forecast CSV with individual sensor predictions
            forecast_rows = []
            
            # Get the last known row per physical sensor (same as above)
            sensors_key = [c for c in ["granary_id", "heap_id", "grid_x", "grid_y", "grid_z"] if c in df_processed.columns]
            last_rows = (
                df_processed.sort_values("detection_time")
                .groupby(sensors_key, dropna=False)
                .tail(1)
                .copy()
            )
            
            for day_idx, forecast_date in enumerate(forecast_dates):
                for sensor_idx, (_, sensor_row) in enumerate(last_rows.iterrows()):
                    if sensor_idx < len(predictions):
                        sensor_predictions = predictions[sensor_idx]
                        
                        # Get prediction for this specific day
                        if day_idx < len(sensor_predictions):
                            temperature = float(sensor_predictions[day_idx])
                            
                            # Get uncertainty if available
                            uncertainty = None
                            if uncertainties is not None and sensor_idx < len(uncertainties) and day_idx < len(uncertainties[sensor_idx]):
                                uncertainty = float(uncertainties[sensor_idx][day_idx])
                            
                            # Get prediction intervals if available
                            lower_bound = None
                            upper_bound = None
                            if prediction_intervals is not None:
                                if (sensor_idx < len(prediction_intervals[0]) and 
                                    day_idx < len(prediction_intervals[0][sensor_idx])):
                                    lower_bound = float(prediction_intervals[0][sensor_idx][day_idx])
                                    upper_bound = float(prediction_intervals[1][sensor_idx][day_idx])
                            
                            # Create comprehensive row (individual sensor level)
                            forecast_row = {
                                'granary_id': args.granary,
                                'forecast_date': forecast_date.strftime('%Y-%m-%d'),
                                'forecast_day': day_idx + 1,
                                'grid_x': int(sensor_row.get('grid_x', sensor_row.get('x', 0)) or 0),
                                'grid_y': int(sensor_row.get('grid_y', sensor_row.get('y', 0)) or 0),
                                'grid_z': int(sensor_row.get('grid_z', sensor_row.get('z', 0)) or 0),
                                'heap_id': sensor_row.get('heap_id', ''),
                                'predicted_temperature_celsius': round(temperature, 2),
                                'uncertainty': round(uncertainty, 2) if uncertainty is not None else None,
                                'lower_bound_95ci': round(lower_bound, 2) if lower_bound is not None else None,
                                'upper_bound_95ci': round(upper_bound, 2) if upper_bound is not None else None,
                                'last_historical_date': last_date.strftime('%Y-%m-%d'),
                                'model_confidence': 'high' if uncertainty and uncertainty < 0.5 else 'medium' if uncertainty and uncertainty < 1.0 else 'low'
                            }
                            
                            # Add any additional metadata from sensor row
                            for col in ['address_cn', 'warehouse_type', 'avg_grain_temp', 'max_temp', 'min_temp']:
                                if col in sensor_row and pd.notna(sensor_row[col]):
                                    forecast_row[col] = sensor_row[col]
                            
                            forecast_rows.append(forecast_row)
            
            # Create DataFrame and save comprehensive CSV
            forecast_df = pd.DataFrame(forecast_rows)
            
            # Create daily summary data for the same CSV
            summary_rows = []
            for day_idx, forecast_date in enumerate(forecast_dates):
                day_data = forecast_df[forecast_df['forecast_day'] == day_idx + 1]
                if not day_data.empty:
                    # Calculate daily statistics (like Dashboard)
                    daily_stats = {
                        'granary_id': args.granary,
                        'forecast_date': forecast_date.strftime('%Y-%m-%d'),
                        'forecast_day': day_idx + 1,
                        'total_sensors': len(day_data),
                        'avg_temperature': round(day_data['predicted_temperature_celsius'].mean(), 2),
                        'min_temperature': round(day_data['predicted_temperature_celsius'].min(), 2),
                        'max_temperature': round(day_data['predicted_temperature_celsius'].max(), 2),
                        'temperature_range': round(day_data['predicted_temperature_celsius'].max() - day_data['predicted_temperature_celsius'].min(), 2),
                        'std_temperature': round(day_data['predicted_temperature_celsius'].std(), 2),
                        'avg_uncertainty': round(day_data['uncertainty'].mean(), 2) if 'uncertainty' in day_data.columns and not day_data['uncertainty'].isna().all() else None,
                        'last_historical_date': last_date.strftime('%Y-%m-%d')
                    }
                    
                    # Add extremes tracking (like Dashboard)
                    max_temp_idx = day_data['predicted_temperature_celsius'].idxmax()
                    min_temp_idx = day_data['predicted_temperature_celsius'].idxmin()
                    max_temp_row = day_data.loc[max_temp_idx]
                    min_temp_row = day_data.loc[min_temp_idx]
                    
                    daily_stats.update({
                        'max_temp_location': f"({max_temp_row['grid_x']},{max_temp_row['grid_y']},{max_temp_row['grid_z']})",
                        'min_temp_location': f"({min_temp_row['grid_x']},{min_temp_row['grid_y']},{min_temp_row['grid_z']})",
                        'max_temp_heap_id': max_temp_row.get('heap_id', ''),
                        'min_temp_heap_id': min_temp_row.get('heap_id', '')
                    })
                    
                    summary_rows.append(daily_stats)
            
            summary_df = pd.DataFrame(summary_rows)
            
            # Save comprehensive forecast data as Parquet (much more efficient)
            timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
            forecast_filename = f"{args.granary}_forecast_{timestamp}"
            forecast_path = Path("forecasts") / forecast_filename  # Save to forecasts directory
            forecast_path.parent.mkdir(exist_ok=True, parents=True)
            
            # Save as Parquet with snappy compression (60-80% smaller, 10x faster)
            from granarypredict.ingestion import save_granary_data
            parquet_path = save_granary_data(
                df=forecast_df,
                filepath=forecast_path,
                format='parquet',
                compression='snappy'
            )
            

            
            print(f"âœ… Forecasting completed successfully!")
            print(f"Forecast Parquet saved to: {parquet_path}")
            print(f"ðŸ“… Based on historical data from: {last_date.strftime('%Y-%m-%d')}")
            print(f"ðŸ”® Forecast dates: {forecast_dates[0].strftime('%Y-%m-%d')} to {forecast_dates[-1].strftime('%Y-%m-%d')}")
            print(f"ðŸ”® Forecast horizons: h+1 to h+{args.horizon}")
            print(f"ðŸ“ Total forecast records: {len(forecast_df)} (sensors Ã— days)")
            print(f"ðŸ“ Sensors used: {len(last_rows)} (from most recent date only)")
            
            # Show sample predictions and daily stats
            if predictions is not None and len(predictions) > 0:
                print(f"ðŸ“ˆ Sample prediction (first sensor, h+1): {predictions[0, 0]:.2f}Â°C")
                if uncertainties is not None:
                    print(f"Sample uncertainty (first sensor, h+1): Â±{uncertainties[0, 0]:.2f}Â°C")
                
                # Show daily summary stats
                if not summary_df.empty:
                    first_day = summary_df.iloc[0]
                    print(f"Day 1 summary: Avg={first_day['avg_temperature']}Â°C, Min={first_day['min_temperature']}Â°C, Max={first_day['max_temperature']}Â°C")
                    print(f"ðŸ“ Day 1 extremes: Max at {first_day['max_temp_location']}, Min at {first_day['min_temp_location']}")
                
                print(f"ðŸ“‹ Parquet format: granary_id, forecast_date, forecast_day, grid_x, grid_y, grid_z, heap_id, predicted_temperature_celsius, uncertainty, lower_bound_95ci, upper_bound_95ci, model_confidence")
        else:
            print(f"âŒ Error: No predictions generated")

    elif args.command == "pipeline":
        logger.info(f"Running complete pipeline for granary: {args.granary}")
        
        # Use the standalone function
        results = run_complete_pipeline(
            granary_csv=args.input,  # Pass the raw input file for ingestion
            granary_name=args.granary,
            skip_train=args.skip_train,
            force_retrain=args.force_retrain
        )
        
        # Display results
        if results['success']:
            print(f"âœ… Pipeline completed successfully for {args.granary}")
            print(f"ðŸ“‹ Steps completed: {', '.join(results['steps_completed'])}")
            if results['model_path']:
                print(f"ðŸ“ Model path: {results['model_path']}")
        else:
            print(f"âŒ Pipeline failed for {args.granary}")
            print(f"ðŸš¨ Errors: {', '.join(results['errors'])}")

    else:
        parser.print_help()

if __name__ == "__main__":
    main()

